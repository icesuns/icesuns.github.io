<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">






  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark,RDD,">










<meta name="description" content="这里，从源码的角度总结一下Spark RDD算子的用法。">
<meta name="keywords" content="Spark,RDD">
<meta property="og:type" content="article">
<meta property="og:title" content="学习之路——Spark(6)&lt;br&gt;Spark RDD算子">
<meta property="og:url" content="http://www.zicesun.com/2019/04/11/201904110935-spark-learning-6/index.html">
<meta property="og:site_name" content="ZIcesun">
<meta property="og:description" content="这里，从源码的角度总结一下Spark RDD算子的用法。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-06-09T16:35:04.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="学习之路——Spark(6)&lt;br&gt;Spark RDD算子">
<meta name="twitter:description" content="这里，从源码的角度总结一下Spark RDD算子的用法。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.zicesun.com/2019/04/11/201904110935-spark-learning-6/">


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116658050-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-116658050-2');
</script>



  <title>学习之路——Spark(6)<br>Spark RDD算子 | ZIcesun</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-116658050-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">ZIcesun</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">Live free or die</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-talking">
          <a href="/talking" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-plane"></i> <br>
            
            说说
          </a>
        </li>
      
        
        <li class="menu-item menu-item-essay">
          <a href="/categories/essay/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-book"></i> <br>
            
            随笔
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.zicesun.com/2019/04/11/201904110935-spark-learning-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="ZIcesun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ZIcesun">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">学习之路——Spark(6)<br>Spark RDD算子</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-11T09:35:18+08:00">
                2019-04-11
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2019-06-10T00:35:04+08:00">
                2019-06-10
              </time>
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          


          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  7.5k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  42
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>这里，从源码的角度总结一下Spark RDD算子的用法。<a id="more"></a></p>
<h1 id="单值型Transformation算子"><a href="#单值型Transformation算子" class="headerlink" title="单值型Transformation算子"></a>单值型Transformation算子</h1><h2 id="map"><a href="#map" class="headerlink" title="map"></a>map</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD by applying a function to all elements of this RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>源码中有一个 <code>sc.clean()</code> 函数，它的所用是去除闭包中不能序列话的外部引用变量。Scala支持闭包，闭包会把它对外的引用(闭包里面引用了闭包外面的对像)保存到自己内部，这个闭包就可以被单独使用了，而不用担心它脱离了当前的作用域；但是在spark这种分布式环境里，这种作法会带来问题，如果对外部的引用是不可serializable的，它就不能正确被发送到worker节点上去了；还有一些引用，可能根本没有用到，这些没有使用到的引用是不需要被发到worker上的； 实际上sc.clean函数调用的是ClosureCleaner.clean()；ClosureCleaner.clean()通过递归遍历闭包里面的引用，检查不能serializable的, 去除unused的引用；</p>
<p>map函数是一个粗粒度的操作，对于一个RDD来说，会使用迭代器对分区进行遍历，然后针对一个分区使用你想要执行的操作f, 然后返回一个新的RDD。其实可以理解为rdd的每一个元素都会执行同样的操作。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val array = Array(1,2,3,4,5,6)</span></span><br><span class="line">array: Array[Int] = Array(1, 2, 3, 4, 5, 6)</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd = sc.app</span></span><br><span class="line">appName   applicationAttemptId   applicationId</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd = sc.parallelize(array, 2)</span></span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val mapRdd = rdd.map(x =&gt; x * 2)  </span></span><br><span class="line">mapRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> mapRdd.collect().foreach(println)</span></span><br><span class="line">2</span><br><span class="line">4</span><br><span class="line">6</span><br><span class="line">8</span><br><span class="line">10</span><br><span class="line">12</span><br></pre></td></tr></table></figure></p>
<h2 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h2><p>flatMap方法与map方法类似，但是允许一次map方法中输出多个对象，而不是map中的一个对象经过函数转换成另一个对象。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  Return a new RDD by first applying a function to all elements of this</span></span><br><span class="line"><span class="comment"> *  RDD, and then flattening the results.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 10, 5)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.flatMap(num =&gt; 1 to num).collect</span></span><br><span class="line">res1: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</span><br></pre></td></tr></table></figure>
<h2 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h2><p>mapPartitions是map的另一个实现，map的输入函数应用与RDD的每个元素，但是mapPartitions的输入函数作用于每个分区，也就是每个分区的内容作为整体。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Return a new RDD by applying a function to each partition of this RDD.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span></span><br><span class="line"><span class="comment">  * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">     f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">     preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">   <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">   <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">     <span class="keyword">this</span>,</span><br><span class="line">     (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">     preservesPartitioning)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> def myfunc[T](iter: Iterator[T]):Iterator[(T,T)]=&#123; </span></span><br><span class="line">     | var res = List[(T,T)]()</span><br><span class="line">     | var pre = iter.next</span><br><span class="line">     | while(iter.hasNext)&#123;</span><br><span class="line">     |   var cur = iter.next</span><br><span class="line">     |   res .::= (pre, cur)</span><br><span class="line">     |   pre = cur</span><br><span class="line">     | &#125;</span><br><span class="line">     | res.iterator</span><br><span class="line">     | &#125;</span><br><span class="line">myfunc: [T](iter: Iterator[T])Iterator[(T, T)]</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 9,3)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.mapPartitions</span></span><br><span class="line">mapPartitions   mapPartitionsWithIndex</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.mapPartitions(myfunc).collect</span></span><br><span class="line">res0: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8))</span><br></pre></td></tr></table></figure>
<h2 id="mapPartitionWithIndex"><a href="#mapPartitionWithIndex" class="headerlink" title="mapPartitionWithIndex"></a>mapPartitionWithIndex</h2><p>mapPartitionWithIndex方法与mapPartitions方法类似，不同的是mapPartitionWithIndex会对原始分区的索引进行追踪，这样就可以知道分区所对应的元素，方法的参数为一个函数，函数的输入为整型索引和迭代器。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD by applying a function to each partition of this RDD, while tracking the index</span></span><br><span class="line"><span class="comment"> * of the original partition.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * `preservesPartitioning` indicates whether the input function preserves the partitioner, which</span></span><br><span class="line"><span class="comment"> * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndex</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(index, iter),</span><br><span class="line">    preservesPartitioning)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val x = sc.parallelize(1 to 10, 3)</span></span><br><span class="line">x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> def myFunc(index:Int, iter:Iterator[Int]):Iterator[String]=&#123;</span></span><br><span class="line">     |   iter.toList.map(x =&gt; index + "," + x).iterator</span><br><span class="line">     | &#125;</span><br><span class="line">myFunc: (index: Int, iter: Iterator[Int])Iterator[String]</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> x.mapPartitions</span></span><br><span class="line">mapPartitions   mapPartitionsWithIndex</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> x.mapPartitionsWithIndex(myFunc).collect</span></span><br><span class="line">res1: Array[String] = Array(0,1, 0,2, 0,3, 1,4, 1,5, 1,6, 2,7, 2,8, 2,9, 2,10)</span><br></pre></td></tr></table></figure>
<h2 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h2><p>foreach主要对每一个输入的数据对象执行循环操作，可以用来执行对RDD元素的输出操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Applies a function f to all elements of this RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.foreach(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> var x = sc.parallelize(List(1 to 9), 3)</span></span><br><span class="line">x: org.apache.spark.rdd.RDD[scala.collection.immutable.Range.Inclusive] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> x.foreach(<span class="built_in">print</span>)</span></span><br><span class="line">Range(1, 2, 3, 4, 5, 6, 7, 8, 9)</span><br></pre></td></tr></table></figure>
<h2 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h2><p>foreachPartition方法和mapPartition的作用一样，通过迭代器参数对RDD中每一个分区的数据对象应用函数，区别在于使用的参数是否有返回值。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Applies a function f to each partition of this RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreachPartition</span></span>(f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanF(iter))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = sc.parallelize(List(1,2,3,4,5,6), 3)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> b.foreachPartition(x =&gt; println(x.reduce((a,b) =&gt; a +b)))</span></span><br><span class="line">7</span><br><span class="line">3</span><br><span class="line">11</span><br></pre></td></tr></table></figure>
<h2 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h2><p>glom的作用与collec类似，collect是将RDD直接转化为数组的形式，而glom则是将RDD分区数据组装到数组类型的RDD中，每一个返回的数组包含一个分区的所有元素，按分区转化为数组，有几个分区就返回几个数组类型的RDD。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return an RDD created by coalescing all elements within each partition into an array.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]] = withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">Array</span>[<span class="type">T</span>], <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; <span class="type">Iterator</span>(iter.toArray))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>下面的例子中，RDD a有三个分区，glom将a转化为由三个数组构成的RDD。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 9, 3)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.glom.collect</span></span><br><span class="line">res5: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9))</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.glom</span></span><br><span class="line">res6: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[4] at glom at &lt;console&gt;:26</span><br></pre></td></tr></table></figure></p>
<h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><p>union方法与++方法是等价的，将两个RDD去并集，取并集的过程中不会去重。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return the union of this RDD and another one. Any identical elements will appear multiple</span></span><br><span class="line"><span class="comment"> * times (use `.distinct()` to eliminate them).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  sc.union(<span class="keyword">this</span>, other)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return the union of this RDD and another one. Any identical elements will appear multiple</span></span><br><span class="line"><span class="comment"> * times (use `.distinct()` to eliminate them).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">++</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">this</span>.union(other)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 4,2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = sc.parallelize(2 to 5,1)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.un</span></span><br><span class="line">union   unpersist</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.union(b).collect</span></span><br><span class="line">res7: Array[Int] = Array(1, 2, 3, 4, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure>
<h2 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h2><p>计算两个RDD中每个对象的笛卡尔积<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of</span></span><br><span class="line"><span class="comment"> * elements (a, b) where a is in `this` and b is in `other`.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cartesian</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](other: <span class="type">RDD</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)] = withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">CartesianRDD</span>(sc, <span class="keyword">this</span>, other)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">cala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 4,2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = sc.parallelize(2 to 5,1)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.cartesian(b).collect</span></span><br><span class="line">res8: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (2,2), (2,3), (2,4), (2,5), (3,2), (3,3), (3,4), (3,5), (4,2), (4,3), (4,4), (4,5))</span><br></pre></td></tr></table></figure>
<h2 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h2><p>groupBy方法有三个重载方法，功能是讲元素通过map函数生成Key-Value格式，然后使用groupByKey方法对Key-Value进行聚合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return an RDD of grouped items. Each group consists of a key and a sequence of elements</span></span><br><span class="line"><span class="comment">   * mapping to that key. The ordering of elements within each group is not guaranteed, and</span></span><br><span class="line"><span class="comment">   * may even differ each time the resulting RDD is evaluated.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @note This operation may be very expensive. If you are grouping in order to perform an</span></span><br><span class="line"><span class="comment">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span></span><br><span class="line"><span class="comment">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](f: <span class="type">T</span> =&gt; <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])] = withScope &#123;</span><br><span class="line">    groupBy[<span class="type">K</span>](f, defaultPartitioner(<span class="keyword">this</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements</span></span><br><span class="line"><span class="comment">   * mapping to that key. The ordering of elements within each group is not guaranteed, and</span></span><br><span class="line"><span class="comment">   * may even differ each time the resulting RDD is evaluated.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @note This operation may be very expensive. If you are grouping in order to perform an</span></span><br><span class="line"><span class="comment">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span></span><br><span class="line"><span class="comment">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](</span><br><span class="line">      f: <span class="type">T</span> =&gt; <span class="type">K</span>,</span><br><span class="line">      numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])] = withScope &#123;</span><br><span class="line">    groupBy(f, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Return an RDD of grouped items. Each group consists of a key and a sequence of elements</span></span><br><span class="line"><span class="comment">   * mapping to that key. The ordering of elements within each group is not guaranteed, and</span></span><br><span class="line"><span class="comment">   * may even differ each time the resulting RDD is evaluated.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @note This operation may be very expensive. If you are grouping in order to perform an</span></span><br><span class="line"><span class="comment">   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span></span><br><span class="line"><span class="comment">   * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](f: <span class="type">T</span> =&gt; <span class="type">K</span>, p: <span class="type">Partitioner</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>)</span><br><span class="line">      : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">this</span>.map(t =&gt; (cleanF(t), t)).groupByKey(p)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 9,2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.groupBy(x =&gt; &#123;<span class="keyword">if</span>(x % 2 == 0) <span class="string">"even"</span> <span class="keyword">else</span> <span class="string">"odd"</span>&#125;).collect</span></span><br><span class="line">res9: Array[(String, Iterable[Int])] = Array((even,CompactBuffer(2, 4, 6, 8)), (odd,CompactBuffer(1, 3, 5, 7, 9)))</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> def myfunc(a: Int):Int=&#123;</span></span><br><span class="line">     | a % 2</span><br><span class="line">     | &#125;</span><br><span class="line">myfunc: (a: Int)Int</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.groupBy(myfunc).collect</span></span><br><span class="line">res10: Array[(Int, Iterable[Int])] = Array((0,CompactBuffer(2, 4, 6, 8)), (1,CompactBuffer(1, 3, 5, 7, 9)))</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.groupBy(myfunc(_), 1).collect</span></span><br><span class="line">res11: Array[(Int, Iterable[Int])] = Array((0,CompactBuffer(2, 4, 6, 8)), (1,CompactBuffer(1, 3, 5, 7, 9)))</span><br></pre></td></tr></table></figure>
<h2 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h2><p>filter方法对输入元素进行过滤，参数是一个返回值为boolean的函数，如果函数对元素的运算结果为true，则通过元素，否则就将该元素过滤，不进入结果集。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD containing only the elements that satisfy a predicate.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: <span class="type">T</span> =&gt; <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">T</span>, <span class="type">T</span>](</span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (context, pid, iter) =&gt; iter.filter(cleanF),</span><br><span class="line">    preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(List(<span class="string">"we"</span>, <span class="string">"are"</span>, <span class="string">"from"</span>, <span class="string">"China"</span>, <span class="string">"not"</span>, <span class="string">"from"</span>, <span class="string">"America"</span>))</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = a.filter(x =&gt; x.length &gt;= 4)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at filter at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> b.collect.foreach(println)</span></span><br><span class="line">from</span><br><span class="line">China</span><br><span class="line">from</span><br><span class="line">America</span><br></pre></td></tr></table></figure>
<h2 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h2><p>distinct方法将RDD中重复的元素去掉，只留下唯一的RDD元素。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD containing the distinct elements in this RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  map(x =&gt; (x, <span class="literal">null</span>)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(List(<span class="string">"we"</span>, <span class="string">"are"</span>, <span class="string">"from"</span>, <span class="string">"China"</span>, <span class="string">"not"</span>, <span class="string">"from"</span>, <span class="string">"America"</span>))</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = a.map(x =&gt; x.length)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[19] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val c = b.distinct</span></span><br><span class="line">c: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[22] at distinct at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> c.foreach(println)</span></span><br><span class="line">5</span><br><span class="line">4</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">7</span><br></pre></td></tr></table></figure>
<h2 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h2><p>subtract方法就是求集合A-B，即把集合A中包含集合B的元素都删除，返回剩下的元素。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Return an RDD with the elements from `this` that are not in `other`.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting</span></span><br><span class="line"><span class="comment">  * RDD will be &amp;lt;= us.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">   subtract(other, partitioner.getOrElse(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(partitions.length)))</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Return an RDD with the elements from `this` that are not in `other`.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">   subtract(other, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions))</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Return an RDD with the elements from `this` that are not in `other`.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(</span><br><span class="line">     other: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">     p: <span class="type">Partitioner</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">   <span class="keyword">if</span> (partitioner == <span class="type">Some</span>(p)) &#123;</span><br><span class="line">     <span class="comment">// Our partitioner knows how to handle T (which, since we have a partitioner, is</span></span><br><span class="line">     <span class="comment">// really (K, V)) so make a new Partitioner that will de-tuple our fake tuples</span></span><br><span class="line">     <span class="keyword">val</span> p2 = <span class="keyword">new</span> <span class="type">Partitioner</span>() &#123;</span><br><span class="line">       <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = p.numPartitions</span><br><span class="line">       <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(k: <span class="type">Any</span>): <span class="type">Int</span> = p.getPartition(k.asInstanceOf[(<span class="type">Any</span>, _)]._1)</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// Unfortunately, since we're making a new p2, we'll get ShuffleDependencies</span></span><br><span class="line">     <span class="comment">// anyway, and when calling .keys, will not have a partitioner set, even though</span></span><br><span class="line">     <span class="comment">// the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be</span></span><br><span class="line">     <span class="comment">// partitioned by the right/real keys (e.g. p).</span></span><br><span class="line">     <span class="keyword">this</span>.map(x =&gt; (x, <span class="literal">null</span>)).subtractByKey(other.map((_, <span class="literal">null</span>)), p2).keys</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">this</span>.map(x =&gt; (x, <span class="literal">null</span>)).subtractByKey(other.map((_, <span class="literal">null</span>)), p).keys</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 9, 2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = sc.parallelize(2 to 5, 4)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val c = a.subtract(b)</span></span><br><span class="line">c: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[28] at subtract at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> c.collect</span></span><br><span class="line">res14: Array[Int] = Array(6, 8, 1, 7, 9)</span><br></pre></td></tr></table></figure>
<h2 id="persist与cache"><a href="#persist与cache" class="headerlink" title="persist与cache"></a>persist与cache</h2><p>cache,缓存数据，把RDD缓存到内存中，以便下次计算式再次被调用。persist是把RDD根据不同的级别进行持久化，通过参数指定持久化级别，如果不带参数则为默认持久化级别，即只保存到内存中，与Cache等价。</p>
<h2 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h2><p>sample方法的作用是随即对RDD中的元素进行采样，或得一个新的子RDD。根据参数制定是否放回采样，子集占总数的百分比和随机种子。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Return a sampled subset of this RDD.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param withReplacement can elements be sampled multiple times (replaced when sampled out)</span></span><br><span class="line"><span class="comment">  * @param fraction expected size of the sample as a fraction of this RDD's size</span></span><br><span class="line"><span class="comment">  *  without replacement: probability that each element is chosen; fraction must be [0, 1]</span></span><br><span class="line"><span class="comment">  *  with replacement: expected number of times each element is chosen; fraction must be greater</span></span><br><span class="line"><span class="comment">  *  than or equal to 0</span></span><br><span class="line"><span class="comment">  * @param seed seed for the random number generator</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @note This is NOT guaranteed to provide exactly the fraction of the count</span></span><br><span class="line"><span class="comment">  * of the given [[RDD]].</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">     withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">     fraction: <span class="type">Double</span>,</span><br><span class="line">     seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">   require(fraction &gt;= <span class="number">0</span>,</span><br><span class="line">     <span class="string">s"Fraction must be nonnegative, but got <span class="subst">$&#123;fraction&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">   withScope &#123;</span><br><span class="line">     require(fraction &gt;= <span class="number">0.0</span>, <span class="string">"Negative fraction value: "</span> + fraction)</span><br><span class="line">     <span class="keyword">if</span> (withReplacement) &#123;</span><br><span class="line">       <span class="keyword">new</span> <span class="type">PartitionwiseSampledRDD</span>[<span class="type">T</span>, <span class="type">T</span>](<span class="keyword">this</span>, <span class="keyword">new</span> <span class="type">PoissonSampler</span>[<span class="type">T</span>](fraction), <span class="literal">true</span>, seed)</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="keyword">new</span> <span class="type">PartitionwiseSampledRDD</span>[<span class="type">T</span>, <span class="type">T</span>](<span class="keyword">this</span>, <span class="keyword">new</span> <span class="type">BernoulliSampler</span>[<span class="type">T</span>](fraction), <span class="literal">true</span>, seed)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 100, 2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = a.sample(<span class="literal">false</span>, 0.2, 0)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[32] at sample at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> b.foreach(println)</span></span><br><span class="line">5</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">57</span><br><span class="line">40</span><br><span class="line">61</span><br><span class="line">45</span><br><span class="line">68</span><br><span class="line">73</span><br><span class="line">50</span><br><span class="line">75</span><br><span class="line">79</span><br><span class="line">81</span><br><span class="line">85</span><br><span class="line">89</span><br><span class="line">99</span><br></pre></td></tr></table></figure>
<h1 id="键值对型transformation算子"><a href="#键值对型transformation算子" class="headerlink" title="键值对型transformation算子"></a>键值对型transformation算子</h1><h2 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h2><p>类似于groupBy，将每一个相同的Key的Value聚集起来形成序列，可以使用默认的分区器和自定义的分区器。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Group the values for each key in the RDD into a single sequence. Allows controlling the</span></span><br><span class="line"><span class="comment"> * partitioning of the resulting key-value pair RDD by passing a Partitioner.</span></span><br><span class="line"><span class="comment"> * The ordering of elements within each group is not guaranteed, and may even differ</span></span><br><span class="line"><span class="comment"> * each time the resulting RDD is evaluated.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This operation may be very expensive. If you are grouping in order to perform an</span></span><br><span class="line"><span class="comment"> * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span></span><br><span class="line"><span class="comment"> * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note As currently implemented, groupByKey must be able to hold all the key-value pairs for any</span></span><br><span class="line"><span class="comment"> * key in memory. If a key has too many values, it can result in an `OutOfMemoryError`.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])] = self.withScope &#123;</span><br><span class="line">  <span class="comment">// groupByKey shouldn't use map side combine because map side combine does not</span></span><br><span class="line">  <span class="comment">// reduce the amount of data shuffled and requires all map side data be inserted</span></span><br><span class="line">  <span class="comment">// into a hash table, leading to more objects in the old gen.</span></span><br><span class="line">  <span class="keyword">val</span> createCombiner = (v: <span class="type">V</span>) =&gt; <span class="type">CompactBuffer</span>(v)</span><br><span class="line">  <span class="keyword">val</span> mergeValue = (buf: <span class="type">CompactBuffer</span>[<span class="type">V</span>], v: <span class="type">V</span>) =&gt; buf += v</span><br><span class="line">  <span class="keyword">val</span> mergeCombiners = (c1: <span class="type">CompactBuffer</span>[<span class="type">V</span>], c2: <span class="type">CompactBuffer</span>[<span class="type">V</span>]) =&gt; c1 ++= c2</span><br><span class="line">  <span class="keyword">val</span> bufs = combineByKeyWithClassTag[<span class="type">CompactBuffer</span>[<span class="type">V</span>]](</span><br><span class="line">    createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = <span class="literal">false</span>)</span><br><span class="line">  bufs.asInstanceOf[<span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Group the values for each key in the RDD into a single sequence. Hash-partitions the</span></span><br><span class="line"><span class="comment"> * resulting RDD with into `numPartitions` partitions. The ordering of elements within</span></span><br><span class="line"><span class="comment"> * each group is not guaranteed, and may even differ each time the resulting RDD is evaluated.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This operation may be very expensive. If you are grouping in order to perform an</span></span><br><span class="line"><span class="comment"> * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`</span></span><br><span class="line"><span class="comment"> * or `PairRDDFunctions.reduceByKey` will provide much better performance.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note As currently implemented, groupByKey must be able to hold all the key-value pairs for any</span></span><br><span class="line"><span class="comment"> * key in memory. If a key has too many values, it can result in an `OutOfMemoryError`.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])] = self.withScope &#123;</span><br><span class="line">  groupByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(List(<span class="string">"mk"</span>, <span class="string">"zq"</span>, <span class="string">"xwc"</span>, <span class="string">"fig"</span>, <span class="string">"dcp"</span>, <span class="string">"snn"</span>), 2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = a.keyBy(x =&gt; x.length)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[34] at keyBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> b.groupByKey.collect</span></span><br><span class="line">res17: Array[(Int, Iterable[String])] = Array((2,CompactBuffer(mk, zq)), (3,CompactBuffer(xwc, fig, dcp, snn)))</span><br></pre></td></tr></table></figure>
<h2 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h2><p>comineByKey方法能够有效地讲键值对形式的RDD相同的Key的Value合并成序列形式，用户能自定义RDD的分区器和是否在Map端进行聚合操作。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Generic function to combine the elements for each key using a custom set of aggregation</span></span><br><span class="line"><span class="comment"> * functions. This method is here for backward compatibility. It does not provide combiner</span></span><br><span class="line"><span class="comment"> * classtag information to the shuffle.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @see `combineByKeyWithClassTag`</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">  combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners,</span><br><span class="line">    partitioner, mapSideCombine, serializer)(<span class="literal">null</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Simplified version of combineByKeyWithClassTag that hash-partitions the output RDD.</span></span><br><span class="line"><span class="comment"> * This method is here for backward compatibility. It does not provide combiner</span></span><br><span class="line"><span class="comment"> * classtag information to the shuffle.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @see `combineByKeyWithClassTag`</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">  combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners, numPartitions)(<span class="literal">null</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(List(<span class="string">"xwc"</span>, <span class="string">"fig"</span>,<span class="string">"wc"</span>, <span class="string">"dcp"</span>, <span class="string">"zq"</span>, <span class="string">"znn"</span>, <span class="string">"mk"</span>, <span class="string">"zl"</span>, <span class="string">"hk"</span>, <span class="string">"lp"</span>), 2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[36] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = sc.parallelize(List(1,2,2,3,2,1,2,2,2,3),2)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val c = b.zip(a)</span></span><br><span class="line">c: org.apache.spark.rdd.RDD[(Int, String)] = ZippedPartitionsRDD2[38] at zip at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val d = c.combineByKey(List(_), (x:List[String], y:String)=&gt;y::x, (x:List[String], y:List[String])=&gt;x::: y)</span></span><br><span class="line">d: org.apache.spark.rdd.RDD[(Int, List[String])] = ShuffledRDD[39] at combineByKey at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> d.collect</span></span><br><span class="line">res18: Array[(Int, List[String])] = Array((2,List(zq, wc, fig, hk, zl, mk)), (1,List(xwc, znn)), (3,List(dcp, lp)))</span><br></pre></td></tr></table></figure>
<p>上面的例子使用三个参数重载的方法，该方法的第一个参数createCombiner把元素V转换成另一类元素C，该例子中使用的参数是List(_),表示将输入元素放在List集合中；第二个参数mergeValue的含义是吧元素V合并到元素C中，该例子中使用的是(x:List[String],y:String)=&gt;y::x,表示将y字符合并到x链表集合中；第三个参数的含义是讲两个C元素合并，该例子中使用的是（x:List[String], y:List[String]）=&gt;x:::y, 表示把x链表集合中的内容合并到y链表中。</p>
<h2 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h2><p>使用一个reduce函数来实现对想要的Key的value的聚合操作，发送给reduce前会在map端本地merge操作，该方法的底层实现是调用combineByKey方法的一个重载方法。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Merge the values for each key using an associative and commutative reduce function. This will</span></span><br><span class="line"><span class="comment">  * also perform the merging locally on each mapper before sending results to a reducer, similarly</span></span><br><span class="line"><span class="comment">  * to a "combiner" in MapReduce.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">   combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Merge the values for each key using an associative and commutative reduce function. This will</span></span><br><span class="line"><span class="comment">  * also perform the merging locally on each mapper before sending results to a reducer, similarly</span></span><br><span class="line"><span class="comment">  * to a "combiner" in MapReduce. Output will be hash-partitioned with numPartitions partitions.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">   reduceByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions), func)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Merge the values for each key using an associative and commutative reduce function. This will</span></span><br><span class="line"><span class="comment">  * also perform the merging locally on each mapper before sending results to a reducer, similarly</span></span><br><span class="line"><span class="comment">  * to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/</span></span><br><span class="line"><span class="comment">  * parallelism level.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">   reduceByKey(defaultPartitioner(self), func)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(List(<span class="string">"dcp"</span>,<span class="string">"fjg"</span>,<span class="string">"snn"</span>,<span class="string">"wc"</span>, <span class="string">"za"</span>), 2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = a.map(x =&gt; (x.length,x))</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[2] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> b.reduceByKey((a, b) =&gt; a + b ).collect</span></span><br><span class="line">res1: Array[(Int, String)] = Array((2,wcza), (3,dcpfjgsnn))</span><br></pre></td></tr></table></figure>
<h2 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h2><p>根据Key值对键值对进行排序，如果是字符，则按照字典顺序排序，如果是数组则按照数字大小排序，可通过参数指定升序还是降序。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(List(<span class="string">"dcp"</span>,<span class="string">"fjg"</span>,<span class="string">"snn"</span>,<span class="string">"wc"</span>, <span class="string">"za"</span>), 2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val b = sc.parallelize(1 to a.count.toInt,2)</span></span><br><span class="line">b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val c = a.zip(b)</span></span><br><span class="line">c: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[6] at zip at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> c.sortByKey(<span class="literal">true</span>).collect</span></span><br><span class="line">res2: Array[(String, Int)] = Array((dcp,1), (fjg,2), (snn,3), (wc,4), (za,5))</span><br></pre></td></tr></table></figure></p>
<h2 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(List(1,2,2,3,1,3),2)</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val b = a.map(x =&gt; (x, &quot;b&quot;))</span><br><span class="line">b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[11] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; val c = a.map(x =&gt; (x, &quot;c&quot;))</span><br><span class="line">c: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[12] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; b.cogroup(c).collect</span><br><span class="line">res3: Array[(Int, (Iterable[String], Iterable[String]))] = Array((2,(CompactBuffer(b, b),CompactBuffer(c, c))), (1,(CompactBuffer(b, b),CompactBuffer(c, c))), (3,(CompactBuffer(b, b),CompactBuffer(c, c))))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; val a = sc.parallelize(List(1,2,2,2,1,3),1)</span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val b = a.map(x =&gt; (x, &quot;b&quot;))</span><br><span class="line">b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[16] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; val c = a.map(x =&gt; (x, &quot;c&quot;))</span><br><span class="line">c: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[17] at map at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; b.cogroup(c).collect</span><br><span class="line">res4: Array[(Int, (Iterable[String], Iterable[String]))] = Array((1,(CompactBuffer(b, b),CompactBuffer(c, c))), (3,(CompactBuffer(b),CompactBuffer(c))), (2,(CompactBuffer(b, b, b),CompactBuffer(c, c, c))))</span><br></pre></td></tr></table></figure>
<h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>首先对RDD进行cogroup操作，然后对每个新的RDD下Key的值进行笛卡尔积操作，再返回结果使用flatmapValue方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a= sc.parallelize(List(&quot;fjg&quot;,&quot;wc&quot;,&quot;xwc&quot;),2)</span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24 </span><br><span class="line"></span><br><span class="line">scala&gt; val c = sc.parallelize(List(&quot;fig&quot;, &quot;wc&quot;, &quot;sbb&quot;, &quot;zq&quot;,&quot;xwc&quot;,&quot;dcp&quot;), 2)</span><br><span class="line">c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[22] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val d = c.keyBy(_.length)</span><br><span class="line">d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[23] at keyBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; val b = a.keyBy(_.length)</span><br><span class="line">b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[24] at keyBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; b.join(d).collect</span><br><span class="line">res6: Array[(Int, (String, String))] = Array((2,(wc,wc)), (2,(wc,zq)), (3,(fjg,fig)), (3,(fjg,sbb)), (3,(fjg,xwc)), (3,(fjg,dcp)), (3,(xwc,fig)), (3,(xwc,sbb)), (3,(xwc,xwc)), (3,(xwc,dcp)))</span><br></pre></td></tr></table></figure></p>
<h1 id="Action算子"><a href="#Action算子" class="headerlink" title="Action算子"></a>Action算子</h1><h2 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h2><p>把RDD中的元素以数组的形式返回。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return an array that contains all of the elements in this RDD.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment"> * all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> results = sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.toArray)</span><br><span class="line">  <span class="type">Array</span>.concat(results: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(List(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>),2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.collect</span></span><br><span class="line">res7: Array[String] = Array(a, b, c)</span><br></pre></td></tr></table></figure>
<h2 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h2><p>使用一个带两个参数的函数把元素进行聚集，返回一个元素的结果。该函数中的二元操作应该满足交换律和结合律，这样才能在并行计算中得到正确的计算结果。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Reduces the elements of this RDD using the specified commutative and</span></span><br><span class="line"><span class="comment"> * associative binary operator.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span> = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">val</span> reducePartition: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Option</span>[<span class="type">T</span>] = iter =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (iter.hasNext) &#123;</span><br><span class="line">      <span class="type">Some</span>(iter.reduceLeft(cleanF))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">var</span> jobResult: <span class="type">Option</span>[<span class="type">T</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">val</span> mergeResult = (index: <span class="type">Int</span>, taskResult: <span class="type">Option</span>[<span class="type">T</span>]) =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (taskResult.isDefined) &#123;</span><br><span class="line">      jobResult = jobResult <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(value) =&gt; <span class="type">Some</span>(f(value, taskResult.get))</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; taskResult</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  sc.runJob(<span class="keyword">this</span>, reducePartition, mergeResult)</span><br><span class="line">  <span class="comment">// Get the final result out of our Option, or throw an exception if the RDD was empty</span></span><br><span class="line">  jobResult.getOrElse(<span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">UnsupportedOperationException</span>(<span class="string">"empty collection"</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 10, 2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[29] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.reduce((a, b) =&gt; a + b)</span></span><br><span class="line">res8: Int = 55</span><br></pre></td></tr></table></figure>
<h2 id="take"><a href="#take" class="headerlink" title="take"></a>take</h2><p>take方法会从RDD中取出前n个元素。先扫描一个分区，之后从分区中得到结果，然后评估该分区的元素是否满足n，若果不满足则继续从其他分区中扫描获取。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Take the first num elements of the RDD. It works by first scanning one partition, and use the</span></span><br><span class="line"><span class="comment">   * results from that partition to estimate the number of additional partitions needed to satisfy</span></span><br><span class="line"><span class="comment">   * the limit.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @note This method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment">   * all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @note Due to complications in the internal implementation, this method will raise</span></span><br><span class="line"><span class="comment">   * an exception if called on an RDD of `Nothing` or `Null`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> scaleUpFactor = <span class="type">Math</span>.max(conf.getInt(<span class="string">"spark.rdd.limit.scaleUpFactor"</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> (num == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](<span class="number">0</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> buf = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">T</span>]</span><br><span class="line">      <span class="keyword">val</span> totalParts = <span class="keyword">this</span>.partitions.length</span><br><span class="line">      <span class="keyword">var</span> partsScanned = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span> (buf.size &lt; num &amp;&amp; partsScanned &lt; totalParts) &#123;</span><br><span class="line">        <span class="comment">// The number of partitions to try in this iteration. It is ok for this number to be</span></span><br><span class="line">        <span class="comment">// greater than totalParts because we actually cap it at totalParts in runJob.</span></span><br><span class="line">        <span class="keyword">var</span> numPartsToTry = <span class="number">1</span>L</span><br><span class="line">        <span class="keyword">val</span> left = num - buf.size</span><br><span class="line">        <span class="keyword">if</span> (partsScanned &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="comment">// If we didn't find any rows after the previous iteration, quadruple and retry.</span></span><br><span class="line">          <span class="comment">// Otherwise, interpolate the number of partitions we need to try, but overestimate</span></span><br><span class="line">          <span class="comment">// it by 50%. We also cap the estimation in the end.</span></span><br><span class="line">          <span class="keyword">if</span> (buf.isEmpty) &#123;</span><br><span class="line">            numPartsToTry = partsScanned * scaleUpFactor</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// As left &gt; 0, numPartsToTry is always &gt;= 1</span></span><br><span class="line">            numPartsToTry = <span class="type">Math</span>.ceil(<span class="number">1.5</span> * left * partsScanned / buf.size).toInt</span><br><span class="line">            numPartsToTry = <span class="type">Math</span>.min(numPartsToTry, partsScanned * scaleUpFactor)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)</span><br><span class="line">        <span class="keyword">val</span> res = sc.runJob(<span class="keyword">this</span>, (it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; it.take(left).toArray, p)</span><br><span class="line"></span><br><span class="line">        res.foreach(buf ++= _.take(num - buf.size))</span><br><span class="line">        partsScanned += p.size</span><br><span class="line">      &#125;</span><br><span class="line">      buf.toArray</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val a = sc.parallelize(1 to 10, 2)</span></span><br><span class="line">a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[30] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> a.take(5)</span></span><br><span class="line">res9: Array[Int] = Array(1, 2, 3, 4, 5)</span><br></pre></td></tr></table></figure>
<h2 id="top"><a href="#top" class="headerlink" title="top"></a>top</h2><p>top会采用隐式排序转换来获取最大的前n个元素。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the top k (largest) elements from this RDD as defined by the specified</span></span><br><span class="line"><span class="comment"> * implicit Ordering[T] and maintains the ordering. This does the opposite of</span></span><br><span class="line"><span class="comment"> * [[takeOrdered]]. For example:</span></span><br><span class="line"><span class="comment"> * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment"> *   sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)</span></span><br><span class="line"><span class="comment"> *   // returns Array(12)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)</span></span><br><span class="line"><span class="comment"> *   // returns Array(6, 5)</span></span><br><span class="line"><span class="comment"> * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment"> * all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param num k, the number of top elements to return</span></span><br><span class="line"><span class="comment"> * @param ord the implicit ordering for T</span></span><br><span class="line"><span class="comment"> * @return an array of top elements</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  takeOrdered(num)(ord.reverse)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Returns the first k (smallest) elements from this RDD as defined by the specified</span></span><br><span class="line"><span class="comment"> * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]].</span></span><br><span class="line"><span class="comment"> * For example:</span></span><br><span class="line"><span class="comment"> * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment"> *   sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1)</span></span><br><span class="line"><span class="comment"> *   // returns Array(2)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2)</span></span><br><span class="line"><span class="comment"> *   // returns Array(2, 3)</span></span><br><span class="line"><span class="comment"> * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment"> * all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param num k, the number of elements to return</span></span><br><span class="line"><span class="comment"> * @param ord the implicit ordering for T</span></span><br><span class="line"><span class="comment"> * @return an array of top elements</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>]): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">if</span> (num == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="type">Array</span>.empty</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> mapRDDs = mapPartitions &#123; items =&gt;</span><br><span class="line">      <span class="comment">// Priority keeps the largest elements, so let's reverse the ordering.</span></span><br><span class="line">      <span class="keyword">val</span> queue = <span class="keyword">new</span> <span class="type">BoundedPriorityQueue</span>[<span class="type">T</span>](num)(ord.reverse)</span><br><span class="line">      queue ++= collectionUtils.takeOrdered(items, num)(ord)</span><br><span class="line">      <span class="type">Iterator</span>.single(queue)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mapRDDs.partitions.length == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="type">Array</span>.empty</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      mapRDDs.reduce &#123; (queue1, queue2) =&gt;</span><br><span class="line">        queue1 ++= queue2</span><br><span class="line">        queue1</span><br><span class="line">      &#125;.toArray.sorted(ord)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val c = sc.parallelize(Array(1,2,3,5,3,8,7,97,32),2)</span></span><br><span class="line">c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> c.top(3)</span></span><br><span class="line">res10: Array[Int] = Array(97, 32, 8)</span><br></pre></td></tr></table></figure>
<h2 id="count"><a href="#count" class="headerlink" title="count"></a>count</h2><p>count方法计算返回RDD中元素的个数。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return the number of elements in the RDD.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val c = sc.parallelize(Array(1,2,3,5,3,8,7,97,32),2)</span></span><br><span class="line">c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> c.count</span></span><br><span class="line">res11: Long = 9</span><br></pre></td></tr></table></figure>
<h2 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h2><p>返回一个固定大小的数组形式的采样子集，此外还会把返回元素的顺序随机打乱。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Return a fixed-size sampled subset of this RDD in an array</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @param withReplacement whether sampling is done with replacement</span></span><br><span class="line"><span class="comment">  * @param num size of the returned sample</span></span><br><span class="line"><span class="comment">  * @param seed seed for the random number generator</span></span><br><span class="line"><span class="comment">  * @return sample of specified size in an array</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * @note this method should only be used if the resulting array is expected to be small, as</span></span><br><span class="line"><span class="comment">  * all the data is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">takeSample</span></span>(</span><br><span class="line">     withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">     num: <span class="type">Int</span>,</span><br><span class="line">     seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">   <span class="keyword">val</span> numStDev = <span class="number">10.0</span></span><br><span class="line"></span><br><span class="line">   require(num &gt;= <span class="number">0</span>, <span class="string">"Negative number of elements requested"</span>)</span><br><span class="line">   require(num &lt;= (<span class="type">Int</span>.<span class="type">MaxValue</span> - (numStDev * math.sqrt(<span class="type">Int</span>.<span class="type">MaxValue</span>)).toInt),</span><br><span class="line">     <span class="string">"Cannot support a sample size &gt; Int.MaxValue - "</span> +</span><br><span class="line">     <span class="string">s"<span class="subst">$numStDev</span> * math.sqrt(Int.MaxValue)"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (num == <span class="number">0</span>) &#123;</span><br><span class="line">     <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](<span class="number">0</span>)</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">val</span> initialCount = <span class="keyword">this</span>.count()</span><br><span class="line">     <span class="keyword">if</span> (initialCount == <span class="number">0</span>) &#123;</span><br><span class="line">       <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">T</span>](<span class="number">0</span>)</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="keyword">val</span> rand = <span class="keyword">new</span> <span class="type">Random</span>(seed)</span><br><span class="line">       <span class="keyword">if</span> (!withReplacement &amp;&amp; num &gt;= initialCount) &#123;</span><br><span class="line">         <span class="type">Utils</span>.randomizeInPlace(<span class="keyword">this</span>.collect(), rand)</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">         <span class="keyword">val</span> fraction = <span class="type">SamplingUtils</span>.computeFractionForSampleSize(num, initialCount,</span><br><span class="line">           withReplacement)</span><br><span class="line">         <span class="keyword">var</span> samples = <span class="keyword">this</span>.sample(withReplacement, fraction, rand.nextInt()).collect()</span><br><span class="line"></span><br><span class="line">         <span class="comment">// If the first sample didn't turn out large enough, keep trying to take samples;</span></span><br><span class="line">         <span class="comment">// this shouldn't happen often because we use a big multiplier for the initial size</span></span><br><span class="line">         <span class="keyword">var</span> numIters = <span class="number">0</span></span><br><span class="line">         <span class="keyword">while</span> (samples.length &lt; num) &#123;</span><br><span class="line">           logWarning(<span class="string">s"Needed to re-sample due to insufficient sample size. Repeat #<span class="subst">$numIters</span>"</span>)</span><br><span class="line">           samples = <span class="keyword">this</span>.sample(withReplacement, fraction, rand.nextInt()).collect()</span><br><span class="line">           numIters += <span class="number">1</span></span><br><span class="line">         &#125;</span><br><span class="line">         <span class="type">Utils</span>.randomizeInPlace(samples, rand).take(num)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val c = sc.parallelize(Array(1,2,3,5,3,8,7,97,32),2)</span></span><br><span class="line">c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> c.takeSample(<span class="literal">true</span>,3, 1)</span></span><br><span class="line">res14: Array[Int] = Array(1, 3, 7)</span><br></pre></td></tr></table></figure>
<h2 id="saveAsTextFile"><a href="#saveAsTextFile" class="headerlink" title="saveAsTextFile"></a>saveAsTextFile</h2><p>将RDD存储为文本文件，一次存一行</p>
<h2 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h2><p>类似count，但是countByKey会根据Key计算对应的Value个数，返回Map类型的结果。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Count the number of elements for each key, collecting the results to a local Map.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This method should only be used if the resulting map is expected to be small, as</span></span><br><span class="line"><span class="comment"> * the whole thing is loaded into the driver's memory.</span></span><br><span class="line"><span class="comment"> * To handle very large results, consider using rdd.mapValues(_ =&gt; 1L).reduceByKey(_ + _), which</span></span><br><span class="line"><span class="comment"> * returns an RDD[T, Long] instead of a map.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countByKey</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">Long</span>] = self.withScope &#123;</span><br><span class="line">  self.mapValues(_ =&gt; <span class="number">1</span>L).reduceByKey(_ + _).collect().toMap</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val c = sc.parallelize(List(<span class="string">"fig"</span>, <span class="string">"wc"</span>, <span class="string">"sbb"</span>, <span class="string">"zq"</span>,<span class="string">"xwc"</span>,<span class="string">"dcp"</span>), 2)</span></span><br><span class="line">c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[36] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val d = c.keyBy(_.length)</span></span><br><span class="line">d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[37] at keyBy at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> d.countByKey</span></span><br><span class="line">res15: scala.collection.Map[Int,Long] = Map(2 -&gt; 2, 3 -&gt; 4)</span><br></pre></td></tr></table></figure>
<h2 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Aggregate the elements of each partition, and then the results for all the partitions, using</span></span><br><span class="line"><span class="comment"> * given combine functions and a neutral "zero value". This function can return a different result</span></span><br><span class="line"><span class="comment"> * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U</span></span><br><span class="line"><span class="comment"> * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are</span></span><br><span class="line"><span class="comment"> * allowed to modify and return their first argument instead of creating a new U to avoid memory</span></span><br><span class="line"><span class="comment"> * allocation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param zeroValue the initial value for the accumulated result of each partition for the</span></span><br><span class="line"><span class="comment"> *                  `seqOp` operator, and also the initial value for the combine results from</span></span><br><span class="line"><span class="comment"> *                  different partitions for the `combOp` operator - this will typically be the</span></span><br><span class="line"><span class="comment"> *                  neutral element (e.g. `Nil` for list concatenation or `0` for summation)</span></span><br><span class="line"><span class="comment"> * @param seqOp an operator used to accumulate results within a partition</span></span><br><span class="line"><span class="comment"> * @param combOp an associative operator used to combine results from different partitions</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span> = withScope &#123;</span><br><span class="line">  <span class="comment">// Clone the zero value since we will also be serializing it as part of tasks</span></span><br><span class="line">  <span class="keyword">var</span> jobResult = <span class="type">Utils</span>.clone(zeroValue, sc.env.serializer.newInstance())</span><br><span class="line">  <span class="keyword">val</span> cleanSeqOp = sc.clean(seqOp)</span><br><span class="line">  <span class="keyword">val</span> cleanCombOp = sc.clean(combOp)</span><br><span class="line">  <span class="keyword">val</span> aggregatePartition = (it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)</span><br><span class="line">  <span class="keyword">val</span> mergeResult = (index: <span class="type">Int</span>, taskResult: <span class="type">U</span>) =&gt; jobResult = combOp(jobResult, taskResult)</span><br><span class="line">  sc.runJob(<span class="keyword">this</span>, aggregatePartition, mergeResult)</span><br><span class="line">  jobResult</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Aggregate the elements of each partition, and then the results for all the partitions, using a</span></span><br><span class="line"><span class="comment">   * given associative function and a neutral "zero value". The function</span></span><br><span class="line"><span class="comment">   * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object</span></span><br><span class="line"><span class="comment">   * allocation; however, it should not modify t2.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * This behaves somewhat differently from fold operations implemented for non-distributed</span></span><br><span class="line"><span class="comment">   * collections in functional languages like Scala. This fold operation may be applied to</span></span><br><span class="line"><span class="comment">   * partitions individually, and then fold those results into the final result, rather than</span></span><br><span class="line"><span class="comment">   * apply the fold to each element sequentially in some defined ordering. For functions</span></span><br><span class="line"><span class="comment">   * that are not commutative, the result may differ from that of a fold applied to a</span></span><br><span class="line"><span class="comment">   * non-distributed collection.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param zeroValue the initial value for the accumulated result of each partition for the `op`</span></span><br><span class="line"><span class="comment">   *                  operator, and also the initial value for the combine results from different</span></span><br><span class="line"><span class="comment">   *                  partitions for the `op` operator - this will typically be the neutral</span></span><br><span class="line"><span class="comment">   *                  element (e.g. `Nil` for list concatenation or `0` for summation)</span></span><br><span class="line"><span class="comment">   * @param op an operator used to both accumulate results within a partition and combine results</span></span><br><span class="line"><span class="comment">   *                  from different partitions</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fold</span></span>(zeroValue: <span class="type">T</span>)(op: (<span class="type">T</span>, <span class="type">T</span>) =&gt; <span class="type">T</span>): <span class="type">T</span> = withScope &#123;</span><br><span class="line">    <span class="comment">// Clone the zero value since we will also be serializing it as part of tasks</span></span><br><span class="line">    <span class="keyword">var</span> jobResult = <span class="type">Utils</span>.clone(zeroValue, sc.env.closureSerializer.newInstance())</span><br><span class="line">    <span class="keyword">val</span> cleanOp = sc.clean(op)</span><br><span class="line">    <span class="keyword">val</span> foldPartition = (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.fold(zeroValue)(cleanOp)</span><br><span class="line">    <span class="keyword">val</span> mergeResult = (index: <span class="type">Int</span>, taskResult: <span class="type">T</span>) =&gt; jobResult = op(jobResult, taskResult)</span><br><span class="line">    sc.runJob(<span class="keyword">this</span>, foldPartition, mergeResult)</span><br><span class="line">    jobResult</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

      

              <div>
      
      <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">------ 本文结束------</div>
    
</div>

      
      </div>

            
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"><i class="fa fa-tag"></i> Spark</a>
          
            <a href="/tags/RDD/" rel="tag"><i class="fa fa-tag"></i> RDD</a>
          
        </div>
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Donate comment here</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/WeChat_pay.jpg" alt="ZIcesun 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/ali_pay.jpg" alt="ZIcesun 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    


    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    ZIcesun
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://www.zicesun.com/2019/04/11/201904110935-spark-learning-6/" title="学习之路——Spark(6)<br>Spark RDD算子">http://www.zicesun.com/2019/04/11/201904110935-spark-learning-6/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">


      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/10/201904101128-spark-learning-5/" rel="next" title="学习之路——Spark(5)<br> Spark 集群资源调度">
                <i class="fa fa-chevron-left"></i> 学习之路——Spark(5)<br> Spark 集群资源调度
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/09/201906092129-spark-java-1/" rel="prev" title="java实现spark（1）">
                java实现spark（1） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      

    </footer>
  </div>
  
  
  



  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
 	<div id="gitalk-container"></div>
 	
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="ZIcesun">
            
              <p class="site-author-name" itemprop="name">ZIcesun</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">38</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/icesuns" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:icesuns@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/3902621880" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-weibo"></i>微博</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://www.zhihu.com/people/icesuns" target="_blank" title="知乎">
                      
                        <i class="fa fa-fw fa-book"></i>知乎</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/Icesuns" title="icesuns" target="_blank">icesuns</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.cnblogs.com/qingyunzong" title="扎心了，老铁" target="_blank">扎心了，老铁</a>
                  </li>
                
              </ul>
            </div>
          

          
        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#单值型Transformation算子"><span class="nav-number">1.</span> <span class="nav-text">单值型Transformation算子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#map"><span class="nav-number">1.1.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#flatMap"><span class="nav-number">1.2.</span> <span class="nav-text">flatMap</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mapPartitions"><span class="nav-number">1.3.</span> <span class="nav-text">mapPartitions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mapPartitionWithIndex"><span class="nav-number">1.4.</span> <span class="nav-text">mapPartitionWithIndex</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#foreach"><span class="nav-number">1.5.</span> <span class="nav-text">foreach</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#foreachPartition"><span class="nav-number">1.6.</span> <span class="nav-text">foreachPartition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#glom"><span class="nav-number">1.7.</span> <span class="nav-text">glom</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#union"><span class="nav-number">1.8.</span> <span class="nav-text">union</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cartesian"><span class="nav-number">1.9.</span> <span class="nav-text">cartesian</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#groupBy"><span class="nav-number">1.10.</span> <span class="nav-text">groupBy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#filter"><span class="nav-number">1.11.</span> <span class="nav-text">filter</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#distinct"><span class="nav-number">1.12.</span> <span class="nav-text">distinct</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#subtract"><span class="nav-number">1.13.</span> <span class="nav-text">subtract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#persist与cache"><span class="nav-number">1.14.</span> <span class="nav-text">persist与cache</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sample"><span class="nav-number">1.15.</span> <span class="nav-text">sample</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#键值对型transformation算子"><span class="nav-number">2.</span> <span class="nav-text">键值对型transformation算子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#groupByKey"><span class="nav-number">2.1.</span> <span class="nav-text">groupByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#combineByKey"><span class="nav-number">2.2.</span> <span class="nav-text">combineByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reduceByKey"><span class="nav-number">2.3.</span> <span class="nav-text">reduceByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sortByKey"><span class="nav-number">2.4.</span> <span class="nav-text">sortByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cogroup"><span class="nav-number">2.5.</span> <span class="nav-text">cogroup</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#join"><span class="nav-number">2.6.</span> <span class="nav-text">join</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Action算子"><span class="nav-number">3.</span> <span class="nav-text">Action算子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#collect"><span class="nav-number">3.1.</span> <span class="nav-text">collect</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reduce"><span class="nav-number">3.2.</span> <span class="nav-text">reduce</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#take"><span class="nav-number">3.3.</span> <span class="nav-text">take</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#top"><span class="nav-number">3.4.</span> <span class="nav-text">top</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#count"><span class="nav-number">3.5.</span> <span class="nav-text">count</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#takeSample"><span class="nav-number">3.6.</span> <span class="nav-text">takeSample</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#saveAsTextFile"><span class="nav-number">3.7.</span> <span class="nav-text">saveAsTextFile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#countByKey"><span class="nav-number">3.8.</span> <span class="nav-text">countByKey</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#aggregate"><span class="nav-number">3.9.</span> <span class="nav-text">aggregate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fold"><span class="nav-number">3.10.</span> <span class="nav-text">fold</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZIcesun</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">34.7k</span>
  
</div>






  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>



  
  
  |  本页点击 <span id="busuanzi_value_page_pv"></span> 次
  |  本站总点击 <span id="busuanzi_value_site_pv"></span> 次
  |  您是第 <span id="busuanzi_value_site_uv"></span> 位访客
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '0a25b655a2577489330f',
          clientSecret: '63507e39d6d6c35ed2a33d6961ce748a0d64796d',
          repo: 'icesuns.github.io',
          owner: 'icesuns',
          admin: ['icesuns'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
