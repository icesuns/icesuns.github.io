<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2020年]]></title>
    <url>%2F2021%2F01%2F06%2F20210106-1%2F</url>
    <content type="text"><![CDATA[一年转眼即逝，新的一年又展开画卷。2020年，很平凡，但是又有着不同的意义。 自六岁开始，到二十六岁，整整二十载的求学之路，读书是一件很平凡的事情。这二十年承载着父辈们望子成龙的希冀，背负着老一辈跨越阶级的梦想。其实能够读到硕士学历，很大程度取决于爸妈对于读书的执念与对知识的敬畏。在他们俩看来，读书是改变人生、改变命运的唯一出路。因为他们也就读了初中和小学，所以也就一直以不好好读书，就会跟他们一样，只有外出打工挣钱为生。相较于在相同环境的他们的同辈人而言，他们对于教育的重视是前所未有的。我觉得这跟他们上世纪90年代就去上海打工，见过更大更广的世界有关。而我跟我姐，在一定程度上就承载着他们的梦想。大人们对于教育的观念，在一定程度上决定了我和我同辈孩子在读书路上能走多远。对于我们这些在农村的孩子而言，能够成功读到大学，不仅取决于家庭经济条件、家庭教育，还取决于自己有多努力。中考与高考，就是在分数的优胜劣汰。这种分数高低的简单比较，反而是最纯净的竞争。读研是推免的，走的是捷径，通过学习成绩以及各项比赛加分，最终拿到了推免资格，是沾了素质教育的光的。2020年1月3号，研究生毕业，给20年的求学之路画上了句号。一年过去，特别是工作了大半年之后，才体会到在学校的时光是多么快乐、轻松、纯粹。尽管求学的时候充满了竞争，经历了大大小小决定人生的考试，但是积累的知识和方法，都为现在的工作和生活打下了基础。怀念读书时代，甚至想要辞职申请PhD了。毕业工作一年方知过去的20年里，无忧无虑的生活是建立在爸妈的辛苦操作之上的，感谢爸妈的付出。毕业了，也该承担起支持一家的担子了。毕业，是告别一个时代，也是另一个时代的开始。对于未来的生活，不退缩，不气馁。 2020年，新冠肺炎疫情也是在毕业之后爆发。（未完待续。。。。死也要更新完。）]]></content>
      <categories>
        <category>essay</category>
      </categories>
      <tags>
        <tag>感</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark scheduler 内部原理剖析(转载)]]></title>
    <url>%2F2019%2F04%2F02%2F201904021108-spark-scheduler%2F</url>
    <content type="text"><![CDATA[大鲨鱼的博客《Spark Scheduler内部原理剖析》中，对spark scheduler的内部原理的分析还是很到位的。现在把这篇文章转载，详细请点击链接。http://sharkdtu.com/posts/spark-scheduler.html]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——Spark(3)Spark RDD内部结构]]></title>
    <url>%2F2019%2F03%2F28%2F1903281452-spark-learning-3%2F</url>
    <content type="text"><![CDATA[Spark是一个基于分布式内存的大数据计算框架，RDD (Resilient Distributed Dataset)是Spark最重要的一个数据抽象。这篇文章记录了我对RDD的一些理解，有不足和错误的地方，请留言指正。 什么是RDDRDD (Resilient Distributed Dataset)，弹性分布式数据集，是数据集合的抽象。它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。 RDD的计算具有惰性，只有在RDD需要讲计算结果提交到Driver或者需要把数据写到文件的时候，计算才真正开始。Spark为RDD的计算提供了两类接口，也就是转换(transformation)操作和动作（action）操作。其中转换操作，是将RDD转换成另一个RDD，这一步计算是具有惰性延迟的，不会立即执行计算。RDD不能改变，在转换的过程中只会生成一个新的RDD。动作操作，会激发计算。 RDD的转换操作和动作操作会在后续的博客中详细介绍。 RDD的接口在介绍RDD的接口之前，先带着几个思考： RDD的结构是什么样子的，怎么表示并行计算单元。 RDD是分布式数据集合的抽象，但是在分布式条件下，RDD怎么保证数据分布均匀？ RDD的计算具有惰性，那么它怎么保证计算的准确性，计算流程是什么样的？ 大数据计算，怎么容错？当Spark数据丢失之后，怎么保证数据的可靠性和完整性。 RDD通过实现一些接口来完成上述的一些问题。 分区(Partition)RDD内部如何去表示并行计算的一个单元呢？其实是采用分区，也就是RDD的内部数据集合在逻辑上被划分为多个分片，这样每一个分片叫做分区（Partition）。分区的个数将决定并行计算的粒度，而每一个分区的数据的计算都是在一个单独的任务中进行，因此任务的个数是由RDD的分区的个数来决定的（准确来说应该是一个作业的最后一个RDD的分区数决定）。在下图中，每一个RDD里面的单元就是一个分区。 1234trait Partition extends Serializable&#123; def index:Int override def hashCode():Int=index&#125; 上面的代码可以看出，分区在源码级别的实现是Partition类，其实是分区的一个标识，index表示这个分区在RDD中的编号。我们通过RDD的编号和Partition编号就可定位到具体的分区，通过接口，可以层存储介质中提取分区对应的数据。 RDD的分区的原则就是尽可能让分区的个数等于起核心的数目。当然用户也可以自指定分区的数据，也可以使用系统默认配置，也就是机器的CPU的核心综述。RDD可以通过创建操作和转换操作生成，在转换操作中，分区的个数根据转换操作中对应的多个RDD之间的以来关系得到。窄依赖的子RDD由父RDD分区个数决定，而宽依赖或shuffle依赖由子RDD的分区器（Partitioner）决定。 分区内部数据的分配原则是尽可能让不同分区内的记录数量保持一直，也就是保证数据分布均衡。正如上面所属，窄以来的子RDD依赖于父RDD，转换之间不会发生shuffle操作，因此在父RDD分区的数据分配方式决定了窄依赖链的分配方式。但是在宽依赖中，父RDD到子RDD之间会进行shuffle操作，因此分区数据的分配由子RDD的分区器决定。哈希分区器不能够保证数据被平均分配到各个分区上，但是范围分区器能够做到这一点。 依赖(Dependency)在上面也提及到了宽依赖和窄以来。在Spark中，以一个真正的计算标志着一个Job，而一个Job则是由多个转换操作构成。由于Spark的计算具有惰性，只有当遇到一个动作操作的时候，计算任务才被激活。因此经过不同变换之间的RDD形成了一个具有以来关系的链，或者说是一个有向无环图（DAG）。在这个有向无环图中，节点表示RDD，子RDD与父RDD分区之间的边则表示数据的依赖关系。 窄依赖(Narrow Dependency)窄依赖如上图所示，父RDD中的一个分区最多只会被子RDD中的一个分区使用，也就是父RDD中一个分区内的数据不能够被再次分割，必须完整地交给子RDD的一个分区。窄依赖又可以分成 一对一依赖:一对一依赖很好理解，就是父RDD的分区编号与子RDD的分区编号完全一致，如上图map操作。 范围依赖: 范围依赖只被应用到UnionRDD与父RDD之间的依赖关系之中，如上图union操作。 宽依赖（Shuffle Dependency）shuffle依赖中，父RDD中的一个分区可能会被子RDD中的多个分区使用。这时候父RDD的数据会被再次分割，发送给子RDD的一些分区，也就是Shuffle依赖意味着父RDD和子RDD之间存在这shuffle操作。 如上图，每一个子RDD的分区会接受来自多个父RDD分区的数据。Shuffle依赖也是后面会讲到的Stage的划分，Spark以是否发生shuffle操作，将一个Job划分为多个Stage进行调度。 抽象类Dependency源码链接1234@DeveloperApiabstract class Dependency[T] extends Serializable &#123; def rdd: RDD[T]&#125; 依赖在Spark源码中对应实现是Dependency，每个Dependency子类内部都会存储一个RDD对象，其实就是对应的父RDD。如果一次转换操作对应多个父RDD，就会产生多个Dependency对象，所有的Dependency对象存储在子RDD内部。只要遍历子RDD内部的Dependency对象，就能获取该RDD所有的依赖关系。窄依赖和shuffle依赖的源码参见上面的链接。 依赖与容错机制我们之前也提到了RDD之间会形成依赖关系，这些依赖关系会形成一个有向无环图。子RDD记录着它依赖着的父RDD。现在我们假设某一个分区的在计算的时候数据丢失了，我们只要遍历该分区的依赖关系，找到它的父RDD信息，然后沿着这个依赖关系往回追溯，就可以找到该分区数据的来源，然后再沿着依赖关系往后计算，就可以计算出这个分区丢失的数据是什么了。 计算(Computing)RDD的计算是惰性的，一系列的转化只有遇到动作操作的时候才回去计算数据，而分区是数据计算的基本单位。 RDD的计算会用到一个compute方法，RDD的抽象类要求所有的子类都要实现这个方法，该方法的参数之一是一个Partition对象，目的是计算该分区中的数据。看一下MappedRDD源码12override def compute(split:Partition, context:TaskContext)= firstParent[T].iterator(split, context).map(f) MappedRDD类的compute方法调用当前RDD内的第一个父RDD的iterator方法，该方法拉取父RDD对应的分区内的数据。iterator方法会返回一个迭代对象，迭代器内的每个元素就是父RDD对应分区内的数据记录。RDD的粗粒度转换体现在调用iterator的map方法上，f函数是map转换操作的函数参数，RDD会对没一个分区（而不是一条一条数据记录）内的数据执行单个f操作，最终返回包含所有经过转换过的数据记录的新迭代器，也就是新的分区。换句话说，compute函数就是负责父RDD分区数据到子RDD分区数据的变换逻辑。 iterator在执行的时候会根据RDD的存储级别，如果存储级别不是None，则说明分区的数据可能存储在了文件系统，也可能是当前RDD执行过cache或者persist等持久化操作，因此执行getOrCompute方法。12345678910111213/** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but * is available for implementors of custom * subclasses of RDD. */final def iterator(split: Partition, context: TaskContext): Iterator[T] = &#123; if (storageLevel != StorageLevel.NONE) &#123; getOrCompute(split, context) &#125; else &#123; computeOrReadCheckpoint(split, context) &#125;&#125; getOrCompute方法会根据RDD的编号和分区编号计算得到当前分区在存储层对应的块编号，通过存储层提供的数据读取借口提取出块的数据。对于getOrCompute方法，会出现两种情况。第一种，数据之前存储在存储截介质中，可能是数据本身就在存储介质，也可能是RDD经过持久化操作并且经历过一次计算过程，这时候数据能够成功提取并返回。第二中情况，数据不存储在介质中，可能是丢失，或者RDD经过持久化操作，但是当前分区的数据是第一次被计算，因此会出现拉取到的数据为none的情况，这意味着需要计算分区数据，则会继续调用RDD类中的computeOrReadCheckpoint方法，并将计算得到的数据缓存到存储介质中，下次就不用再重新计算。 computeOrReadCheckpoint方法方法会检查当前RDD是否已经被标记为检查点，如果没有被标记为检查点，则执行自身的compute方法来计算分区的数据，否则直接拉去RDD分区内的数据。对于标记成检查点的情况，当前RDD的父RDD不再是原先转换操作中提供的父RDD，而是被Spark替换成一个Checkpoint对象，该对象中的数据存放在文件系统中，因此最终该对象会从文件系统中读取数据并返回给computeOrReadCheckpoint方法。 分区器(Partitioner)Spark内置了两类分区器，哈希分区器(Hash Partitioner)和范围分区器(Range Partitioner)分区器的主要作用有一下三点： 决定RDD的分区数量。 决定shuffle过程中reducer的个数（实际上也是子RDD的分区个数）以及map端的一条数据应该分配给哪一个分区。 决定依赖类型，如果父RDD和子RDD都有分区器并且分区器相同，则表示两个RDD之间是窄依赖，否则是Shuffle依赖。 哈希分区器1234567891011121314151617181920212223242526/** * A [[org.apache.spark.Partitioner]] that implements hash-based partitioning using * Java's `Object.hashCode`. * * Java arrays have hashCodes that are based on the arrays' identities rather than their contents, * so attempting to partition an RDD[Array[_]] or RDD[(Array[_], _)] using a HashPartitioner will * produce an unexpected or incorrect result. */class HashPartitioner(partitions: Int) extends Partitioner &#123; require(partitions &gt;= 0, s"Number of partitions ($partitions) cannot be negative.") def numPartitions: Int = partitions def getPartition(key: Any): Int = key match &#123; case null =&gt; 0 case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions) &#125; override def equals(other: Any): Boolean = other match &#123; case h: HashPartitioner =&gt; h.numPartitions == numPartitions case _ =&gt; false &#125; override def hashCode: Int = numPartitions&#125; 哈希分区器的实现在HashPartitioner中，其getPartition方法实现很简单，取键值的hashCoder，除上子RDD分区个数，取余即可。尽管哈希分区器实现很简单，运行速度很快，但是它不关心键值的分布情况，所以散列到不同发哪去的概率会因数据而异，所以会出现数据倾斜的现象，也就是一部分分区分配的数据很多，一部分很少。 范围分区器对于范围分区器，之后会单独写篇博客，这里附上一片博客给大家作为参考 分区器针对哈希分区器的缺点，范围分区器则在一定程度上避免了这个问题，范围分区器尽量使得所有分区的数据均匀，并且分区内的数据的上界是有序的。 持久化(Persistence)持久化方法有两个，分别是cache和persist。Cache等价于StorageLevel.Memory_ONLY的persist方法，而persist方法也仅仅是修改当前RDD的存储级别而已。SparkContext中维护了一张哈希表persistentRdds,用语等级所有被持久化的RDD，执行persist操作是，会将RDD的编号作为键值，把RDD记录到persistentRdds表中，unpersist函数会调用SparkContext对象的unpersistRDD方法，除了将RDD从哈希表中移除之外，该方法还会降RDD中的分区对应的所有块从存储介质中删除。 检查点(CheckPoint)检查点机制的实现与持久化是的实现有着较大的区别。检查点并非第一次计算就将计算结果进行存储，而是等到第一个作业结束之后再启动专门的一个作业去完成存储的过程。详见链接 [检查点] (https://www.cnblogs.com/tongxupeng/p/10439889.html) 引用1. 检查点2. 范围分区]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——Spark(2)Spark 集群搭建]]></title>
    <url>%2F2019%2F03%2F27%2F1903272048-spark-learning-2%2F</url>
    <content type="text"><![CDATA[本文讲介绍如何搭建spark集群。搭建spark集群需要进行一下几件事情： 集群配置ssh无秘登录 java jdk1.8 scala-2.11.12 spark-2.4.0-bin-hadoop2.7 hadoop-2.7.6 上述所有的文件都安装在/home/zhuyb/opt文件夹中。 服务器服务器是实验室的，选用了一台master和三台slave机器.IP和机器名在hosts文件中做了映射，因此可以通过hostname直接访问机器。 ip addr hostname 219.216.64.144 master 219.216.64.200 hadoop0 219.216.65.202 hadoop1 219.216.65.243 hadoop2 配置shh免密登录详情参考集群环境ssh免密码登录设置shh免密登录配置其实很容易，我们现在有四台机器，首先我们master上生成新的公钥和私钥文件。12ssh-keygen -t rsa #.ssh文件夹将出现id_rsa，id_rsa.pubcat id_rsa.pub &gt;&gt; authorized_keys #将公钥拷贝到authorized_keys文件中 然后按照同样的方式在三台slave机器上生成公钥和私钥，并将各自的authorized_keys文件拷贝到master上，并将文件中的公钥追加到master的authorized_keys文件中。1ssh-copy-id -i master #将公钥拷贝到master的authorized_keys中 最后讲master的authorized_keys文件拷贝到三台slave机器上。至此，四台机器即可实现ssh免密登录。 安装JDK和ScalaJDK版本是1.8, Scala版本是2.11。Scala 2.12与spark 2.4版本有些不兼容，在后续编程的时候会出现一些问题，之后应该会解决。讲jdk和scala文件解压之后，可在~/.bashrc 文件中配置环境变量。12345678export JAVA_HOME=/home/zhuyb/opt/jdk1.8.0_201export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib:$CLASSPATHexport JAVA_PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JRE_HOME&#125;/binexport PATH=$PATH:$&#123;JAVA_PATH&#125;export SCALA_HOME=/home/zhuyb/opt/scala-2.11.12export PATH=$&#123;SCALA_HOME&#125;/bin:$PATH 按照上述方式配置好之后，然后执行命令 source ~/.bashrc。所有的操作需要在每台机器上进行配置。 配置Hadoop 讲Hadoop文件解压到~/opt/文件夹下。 12tar -zxvf hadoop-2.7.3.tar.gzmv hadoop-2.7.3 ~/opt 在~/.bashrc文件中配置环境变量，并执行source ~/.bashrc生效 1234export HADOOP_HOME=/home/zhuyb/opt/hadoopp-2.7.6export PATH=.:$HADOOP_HOME/bin:$HADOOP_HOME/sbin/:$PATHexport CLASSPATH=.:$HADOOP_HOME/lib:$CLASSPATHexport HADOOP_PREFIX=/home/zhuyb/opt/hadoop-2.7.6 修改相应的配置文件 a. 修改$HADOOP_HOME/etc/hadoop/slaves，将原来的localhost删除，改成如下内容： 123hadoop0hadoop1hadoop2 b. 修改$HADOOP_HOME/etc/hadoop/core-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/zhuyb/opt/hadoop-2.7.6/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; c. 修改$HADOOP_HOME/etc/hadoop/hdfs-site.xml 1234567891011121314151617181920212223242526 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.address&lt;/name&gt; &lt;value&gt;0.0.0.0:50010&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- 备份数：默认为3--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/zhuyb/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;!-- datanode--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/zhuyb/tmp/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; d. 复制template，生成xml, cp mapred-site.xml.template mapred-site.xml,修改$HADOOP_HOME/etc/hadoop/mapred-site.xml 12345678910111213141516 &lt;configuration&gt;&lt;!-- mapreduce任务执行框架为yarn--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;!-- mapreduce任务记录访问地址--&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; e. 修改$HADOOP_HOME/etc/hadoop/yarn-site.xml 123456789101112 &lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; f. 修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh，修改JAVA_HOME 1export JAVA_HOME=~/opt/jdk1.8.0_121 将master Hadoop文件夹拷贝到Hadoop0，hadoop1，hadoop2三台机器上1scp -r ~/opt/hadoop-2.7.3 zhuyb@hadoop0:~/opt 配置Spark 将spark文件解压到~/opt下，然后在~/.bashrc配置环境变量，并执行source ~/.bashrc 12export SPARK_HOME=/home/zhuyb/opt/spark-2.4.0-bin-hadoop2.7export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME 复制spark-env.sh.template成spark-env.sh,cp spark-env.sh.template spark-env.sh修改$SPARK_HOME/conf/spark-env.sh，添加如下内容: 123export JAVA_HOME=/home/zhuyb/opt/jdk1.8.0_201export SPARK_MASTER_IP=masterexport SPARK_MASTER_PORT=7077 复制slaves.template成slaves,cp slaves.template slaves,修改$SPARK_HOME/conf/slaves，添加如下内容: 123hadoop0hadoop1hadoop2 修改hadoop0, hadoop1, hadoop2.修改hadoop1, hadoop2,hadoop0, $SPARK_HOME/conf/spark-env.sh，将export SPARK_LOCAL_IP改成hadoop1, hadoop2,hadoop0的IP 引用Hadoop2.7.3+Spark2.1.0完全分布式集群搭建过程]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>Spark</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——spark(1)Spark简介]]></title>
    <url>%2F2019%2F03%2F27%2F1903271913-spark-learning-1%2F</url>
    <content type="text"><![CDATA[之前看了一些Spark的相关内容，我觉得很有必要进行总结一下，不然都搭不起自己的知识框架。 Apache SparkApache Spark™是用于大规模数据处理的统一分析引擎，是一个实现快速通用的集群计算平台。它是由加州大学伯克利分校AMP实验室 开发的通用内存并行计算框架，用来构建大型的、低延迟的数据分析应用程序。它扩展了广泛使用的MapReduce计算模型。高效的支撑更多计算模式，包括交互式查询和流处理。spark的一个主要特点是能够在内存中进行计算，及时依赖磁盘进行复杂的运算，Spark依然比MapReduce更加高效。官网地址:http://spark.apache.org/ Spark的主要特点 提供Cache机制来支撑需要反复迭代计算或者多次数据共享，减少数据的I/O开销； 提供支持DAG图的分布式并行计算变成框架，减少多次计算之间中间结果写到HDFS的开销； 使用多线程池模型来减少Task启动开销，Shuffle操作中避免不必要的sort操作，并减少磁盘操作。Spark与Hadoop的比较 Hadoop的局限 Spark的改进 抽象层次低， 代码编写难易上手 通过使用RDD的统一抽象，实现数据处理逻辑的代码非常简洁。 只提供map和reduce两个操作，表达能力有限 通过RDD提供了很多transformation和action操作，实现了很多基本的操作，比如Sort，Join等。 一个job只有Map和Reduce两个阶段，复杂的程序需要大量的Job来完成，比起给Job之间的以来关系需要应用户自行管理 一个Job可以包含多个RDD转换操作，只需要在调度时生成多个Stage。一个Stage中可以包含多个Map操作，只需要Map操作使用的RDD分区保持不变。 处理逻辑隐藏在代码细节中，缺少整体逻辑视图 RDD的转换支持流式API，提供处理逻辑的整体视图 对迭代式的数据处理性能比较差，Reduce与下一步Map之间的中间结果只能保存在HDFS文件系统中 通过内存缓存数据，可大大提高迭代式计算的性能，内存不足是可以溢出到磁盘上。 ReduceTask需等待所有MapTask都完成后才开始执行 分区相同的转换操作可以在一个Task中以流水线的形式执行，只有分区不同的转换需要Shuffle操作。 时延高，只适用于批数据处理，对交互式数据处理和实时数据处理的支持不够 将流拆成小的Batch，提供Discretized Stream处理流数据 Spark主要组件 名称 功能 SparkCore 将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。 SparkSQL Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容。 SparkStreaming 是Spark提供的实时数据进行流式计算的组件。 MLlib 提供常用机器学习算法的实现库。 GraphX 提供一个分布式图计算框架，能高效进行图计算。 BlinkDB 用于在海量数据上进行交互式SQL的近似查询引擎。 Tachyon 以内存为中心高容错的的分布式文件系统。 Spark Cluster Managerscluster manager值集群上获取资源的外部服务器 Standalone: Spark原生的资源管理器，有manager负责资源的分配 Apache Messo: 与Hadoop MR兼容性良好的资源调度框架 Hadoop Yarn： 指yarn中的resourceManager]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯云作为图床]]></title>
    <url>%2F2019%2F03%2F27%2F1903271019-tencent-picdb%2F</url>
    <content type="text"><![CDATA[之前都是使用新浪微博图床，但是最近这个图床一直在维护，其他的图床使用都比较麻烦，因此在这里我打算使用腾讯云的对象存储作为图床。 怎么在腾讯云上创建对象存储，其实很简单。完全可以按照链接进行配置。 在这里我们测试一下防盗链设置。设置防盗链之后，即使别人能够获得图片的链接，但是也不能访问图片。]]></content>
      <categories>
        <category>blogs</category>
      </categories>
      <tags>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流浪——Hive(1)Hive的基本组件及简单的执行流程]]></title>
    <url>%2F2019%2F03%2F26%2F1903262337-hiv-introduction-1%2F</url>
    <content type="text"><![CDATA[本文简单介绍hive的基本组件和简单的执行流程，是笔记本上的照片。更为详细的内容参考文后的引用。 Hive组件 Hive执行流程 引用1 Hive学习之路(一)Hive初识2 Hive技术原理3 Hive学习系列(一)什么是Hive及Hive的架构]]></content>
      <categories>
        <category>大数据</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git提交文件]]></title>
    <url>%2F2019%2F03%2F26%2F1903261608-git-upload-files%2F</url>
    <content type="text"><![CDATA[使用git命令上传文件到github中 git init用 git init 在目录中创建新的 Git 仓库。 你可以在任何时候、任何目录中这么做，完全是本地化的。 1git init git add12git add . #(.表示添加所有文件到缓存中)git add filename #(添加某一文件) git commit使用 git add 命令将想要快照的内容写入缓存区， 而执行 git commit 将缓存区内容添加到仓库中。使用 -m 选项以在命令行中提供提交注释。1git commit -m &apos;上传文件&apos; git remote add origin将本地仓库关联到GitHub上的仓库里去 1git remote add origin https://github.com/icesuns/icesuns.git.io.git git pull首次提交要git pull 一下12git pull origin mastergit pull --rebase origin master git push将文件提交到GitHub上1git push -u origin master]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark核心概念RDD(转载)]]></title>
    <url>%2F2019%2F03%2F23%2FSpark%E2%80%94%E2%80%94RDD%2F</url>
    <content type="text"><![CDATA[本文是转载守护之鲨关于Spark RDD的文章。 RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)，它是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，该DAG描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)一气呵成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系统，简单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。 RDD特点RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。 分区如下图所示，RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。 只读如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。 RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。 依赖RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。 通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。 缓存如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。 checkpoint虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。 小结总结起来，给定一个RDD我们至少可以知道如下几点信息： 1、分区数以及分区方式； 2、由父RDDs衍生而来的相关依赖信息； 3、计算每个分区的数据，计算步骤为： 1）如果被缓存，则从缓存中取的分区的数据； 2）如果被checkpoint，则从checkpoint处恢复数据； 3）根据血缘关系计算分区的数据。 编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。 要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。 应用举例下面介绍一个简单的spark应用程序实例WordCount，统计一个数据集中每个单词出现的次数，首先将从hdfs中加载数据得到原始RDD-0，其中每条记录为数据中的一行句子，经过一个flatMap操作，将一行句子切分为多个独立的词，得到RDD-1，再通过map操作将每个词映射为key-value形式，其中key为词本身，value为初始计数值1，得到RDD-2，将RDD-2中的所有记录归并，统计每个词的计数，得到RDD-3，最后将其保存到hdfs。123456789101112131415161718import org.apache.spark._import SparkContext._object WordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println("Usage: WordCount &lt;inputfile&gt; &lt;outputfile&gt;"); System.exit(1); &#125; val conf = new SparkConf().setAppName("WordCount") val sc = new SparkContext(conf) val result = sc.textFile(args(0)) .flatMap(line =&gt; line.split(" ")) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) result.saveAsTextFile(args(1)) &#125;&#125; 小结基于RDD实现的Spark相比于传统的Hadoop MapReduce有什么优势呢？总结起来应该至少有三点： 1）RDD提供了丰富的操作算子，不再是只有map和reduce两个操作了，对于描述应用程序来说更加方便； 2）通过RDDs之间的转换构建DAG，中间结果不用落地； 3）RDD支持缓存，可以在内存中快速完成计算。 引用1.守护之鲨——Spark核心概念RDD]]></content>
      <categories>
        <category>大数据</category>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Processing vs Stream Processing 批处理与流处理]]></title>
    <url>%2F2019%2F03%2F22%2FBatch-Processing-vs-Stream-Processing%2F</url>
    <content type="text"><![CDATA[最近在学习关于大数据处理方面的东西，因此想总结一下批处理和流处理之间的异同之处。 数字社会中数据到处都在产生，数据规模也在不断增长。处理较小的数据集可能跟简单，但是处理TB，PB级别的数据量将会是很困难的事情。大数据处理框架，比如MapReduce、Hadoop以及Spark等分布式处理框架出现，一定程度上解决了大数据处理的问题。 我们将讨论大数据出路中的一些的方法，批处理（Batch Processing）和流处理（Streaming Processing）。在实际的业务中，可根据自己的实际情况决定采用哪种处理模式。 批处理Batch processing is where the processing happens of blocks of data that have already been stored over a period of time. 批处理强调处理一个时间窗口的批数据，缺乏实时性。如下图，计算一天为单位产生的输入数据，当天产生的数据将会在第二天进行计算。也就是说我们对数据，有一个预先收集存储的过程。 在批处理方式中，数据首先被存储，随后被分析。MapReduce是非常重要的批处理模型。MapReduce的核心思想是，数据首先被分为若干小数据块chunks，随后这些数据块被并行处理并以分布的方式产生中间结果，最后这些中间结果被合并产生最终结果。MapReduce分配与数据存储位置距离较近的计算资源，以避免数据传输的通信开销。 流处理Stream Processing is the act of continuously incorporating new data to compute a result， and is a golden key if you want ananlytics results in real time。 而在流处理中，由于流数据的无边界性，无法确定数据何时产生、何时到达。而流处理强调的是计算的实时性。如下图，当天的数据计算当天需要被及时计算，数据一旦到达就要计算出结果。 流式处理假设数据的潜在价值是数据的新鲜度(freshness)，因此流式处理方式应尽可能快地处理数据并得到结果。在这种方式下，数据以流的方式到达。在数据连续到达的过程中，由于流携带了大量数据，只有小部分的流数据被保存在有限的内存中。流处理理论和技术已研究多年，代表性的开源系统包括Storm，S4和Kafka。流处理方式用于在线应用，通常工作在秒或毫秒级别。 1.大数据技术学习：大数据的两种处理方式的“流”和“批”2.Big Data Battle : Batch Processing vs Stream Processing]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>流处理</tag>
        <tag>批处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A survey of the Usages of Deep Learning in NLP]]></title>
    <url>%2F2019%2F01%2F13%2Fa-survey-of-the-usages-of-deep-learning-in-NLP%2F</url>
    <content type="text"><![CDATA[这是一篇关于深度学习在自然语言处理方向的综述. 点击下载原文 简介NLPNLP是一个交叉学科,涉及计算语言学,认知科学,统计学等等学科.自然语言处理又包含多方面的任务,包括认知,理解, 生成.自然语言认知和理解是让计算机把输入的语言变成meaningful的符号和关系,然后在根据任务目的进行在处理.自然语言生成是计算机数据转化成自然语言. NLP的工作可以分为两个子任务: core areas: Language modeling: morphological processing dealing with segmentation of meaningful component of words parts of speech of words syntatic processing or parsing semantic parsing applications: information extraction translation summarization question answering classification or clustering 接下来对以上每一个小点进行介绍. Neural Networks由于算力巨大提升和大量可用的NLP数据集的公开,神经网络(neural networks)在NLP的使用今年来发展迅速,比如机器翻的质量也愈来愈好.现在NLP任务中采用的神经网络有一下几种: Feedward Networks Convolutional Neural Networks (CNN) Recursive Neural Networks Recurrent Neural Networks (RNN) Long Short-Term Memory Networks (LSTM) Gated Recurrent Unit (GRU) 除此之外,还有一些其他的技术: Attention Mechanisms 注意力机制 Residual Connections 残差连接 Dropout Encoder-Decoder frameworks 对于这些神经网络的具体细节,这里不做介绍.想了解的,可以查找相关的技术资料,然后对这些知识点进行学习. Core AreasLanguage Modeling 语言模型,即是判断一句话是否为正常人的话.语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率$P(w_1, w_2, …, w_t)$ .$w_1$ 到$w_t$依次表示这句话中的各个词。有个很简单的推论是：$P(w_1, w_2, …, w_t) = P(w_1) \times P(w_2 | w_1) \times P(w_3 | w_1, w_2) \times … \times P(w_t | w_1, w_2, …, w_{t-1})$常用的语言模型都是在近似地求$P(w_t | w_1, w_2, …, w_{t-1})$。比如 n-gram 模型就是用$P(w_t | w_{t-n+1}, …, w_{t-1})$近似表示前者。 神经网络在处理语言模型的时候，有几点问题需要考虑分别是： 同义词（synonyms) 未登录词 （OOV) 词向量 (word embedding) 是语言模型的一个产物，能够获取词语之间的语义关系。在词语的表示有离散表示，如： one-hot形式，这种表示不能表达词语之间的联系，而且稀疏，当词语数量很大的时候，one-hot就更加稀疏了。word embedding是一种稠密的连续向量表示，在连续的向量空间里，含义相似的词语位置也比较近。甚至词语之间的组合关系，也能够通过向量的表示，比如：$W_(queen) = W_(king) + W_(woman)$ 深度学习在语言模型的应用，主要有以下几点： Attention Mechanism的应用 Residual Memory Networks Convolutional Neural Networks. 卷积神经网络应用到语言模型中，一般将pooling替换成full connection.一些特征会在pooling中丢失，而全连接能够在一定程度上保留这些特征。 Character Aware Neural Language Models. 之前的语言模型是以word为基本单元，这种方法是以character为基本的单元进行处理。 MorphologyMorphology 指的是词法，形态学。主要的研究内容是对词根(roots)、词干(stems)，前缀(preffixes)、后缀(suffixes)、中缀(infixes)。在NPL任务中，来自同一个词根的词语，会表示不同的含义，但是彼此语义仍然有联系。 ParsingParsing 研究的是不同的Word或者phrase如何在一个句子中组织起来，也就是研究语法。Parsing至少可以分成两中模式： Constituency Parsing, 以分层的方式从句子中提取除phrases，在确定短语的语法特征之后，反过来phrases又会组合成一个完整的句子。(In constituency parsing, phrasal constituents are extracted form a sentence in a hierarchical fashion. Phrasal are identified, which in turn form larger phrases, eventually culminating in conplete sentences.) Dependency Parsing, 仅关注pairs of individual words之间的关系。（Dependency parsing on the other hand looks solely at the relationships between pairs of individual words） 深度学习在dependency parsing中应用蛮广泛的，特别是基于Graph-based parsing，将句子解析成parsing trees，然后找出正确的parsing tree。 graph-based parsing用formal grammar去解析句子。最近的grahp-based方法用到了trainsition-based方法，这类方法只会生成一颗parsing tree，句子中word之间的联系用弧表示，弧的方向表示words之间的依赖关系。 SemanticsSemantics的关注点是如何理解words、phrases、sentences甚至是documents的meaning。Distributed Representation是表示Meaning的一种方式。Word Embeddings利用能够获得词语之间的语义联系。semantics的任务有： Semantic Comparison。两个实体的语义是否一直，其衡量指标是将模型和人主观判断的结果进行比较。 Sentence Modeling。用向量表示句子的语义。 ApplicationsInformation Extraction从文本中获取显式或者隐式的信息。 Named Entity Recognition命名实体识别，是指识别出文本中的专有名词，比如日期，时间，价格，产品IDs。一般常用的方法有支持向量机、条件随机场、LSTM网络等。 Event Extractionevent extraction涉及用来知道事件发生的words或者phrases，这些词表示事件的参与者(participant),比如agent、事件的对象(object)、事件的接受者(recipient)以及事件发生的时间等。也就是说挖掘文本中关于事件的相关信息，比如时间，地点，人物，事件等关于event的描述。 EventExtraction有四个子任务： identifying event mentions or phrases that describe events. 识别除描述事件的描述或者短语 identitying event triggers, which are the main words, usually verbs or gerunds，sometimes infinitives,that specify the occurence of the events.识别除事件的触发词,表示时间的发生，一般是动词或者动名词，有时候是动词不定时。 identitying arguments of events. identifying arguments’ roles in events Relationship ExtractionText ClassificationSummarizationQuestion AnsweringMachine TranslationImage and Vidoe Captioning 引用 [1] A Survey of the Usages of Deep Learning in Natural Language Processing 原文[2] 语言模型[3] 词向量 (word embedding)[4] Named Entity Recognition[5] Relationship Extraction[6] Text Classification[7] Summarization[8] Question Answering[9] Image and Vidoe Captioning]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于RNN的英文姓名的性别预测]]></title>
    <url>%2F2019%2F01%2F12%2FRNN-based-name-gender-prediction%2F</url>
    <content type="text"><![CDATA[根据人的名字判断任务的性别是一个很有趣的工作. 预测英文姓名的性别,对first name进行预测,有很多种方法,比如朴素贝叶斯法,SVM,神经网络等.预测的时候能够利用的特征也有多种:名字的最后一个字母,两个字母(2-gram),最后一个字母是否是元音或者辅音. 我们讲介绍如何利用RNN预测英文姓名的性别.点击dataset即可下载数据集 数据处理本项目采用的数据集是一个英文数据集.男性和女性的数据比为$ 3:5$,在一般情况下,数据比例较为合适. - female 5001条 - male 2943条 在处理数据的时候,对数据进行规范化处理: 所有字符小写化; 剔除所有的非字母数据; 字母数值化,每个字母对应在字符标的顺序; 模型的特征是姓名的单个字母; 测试数据和训练数据的划分按照$1:9$的比例进行划分. RNN模型模型有三层,分别是RNN层, 全连接层,softmax层.RNN层采用LSTM网络结构. 12345678910111213141516171819202122232425class Classifier(nn.Module): def __init__(self, input_size, hidden_size, embedding_size): super(Classifier, self).__init__() self.input_size = input_size self.hidden_size = hidden_size self.embedding_size = embedding_size self.embeddings = nn.Embedding(self.input_size, self.embedding_size) nn.init.xavier_normal(self.embeddings.weight.data) self.drop = nn.Dropout(p=0.1) self.rnn = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, batch_first=True) self.out = nn.Linear(self.hidden_size, 2) self.log_softmax = nn.LogSoftmax(dim=-1) def forward(self, input, length): input = self.embeddings(input) input = self.drop(input) input_packed = nn.utils.rnn.pack_padded_sequence(input, length, batch_first=True) # out_packed, (ht, ct) = self.rnn(input_packed, None) # out = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True) _, (ht, _) = self.rnn(input_packed, None) out = self.out(ht) out = self.log_softmax(out) return out RNN网络参数batch_first=True时,需要注意输入序列.用RNN（包括LSTM\GRU等）做NLP任务时，对于同一个batch内的短句子一般需要padding补齐，这些padding的字符一般不应该拿去算output、hidden state、loss. 在pytorch中,在处理NLP任务的时候,首先要讲数据的序列进行padding,让一个batch的数据长度一样.然后根据序列原有的长度进行逆序排序.在我们这个任务里,需要处理names序列,以及labels.这时候有两种方案: names序列按顺序排列之后,labels也按照names排列顺序进行调整. names序列按照长短顺序排列之后,进行出列,然后将结果序列调整为原来的顺序.在这里,采用第一种方法.1234567891011length = np.array([len(name) for name in names_list])sort_idx = np.argsort(-length)max_len = max(length)name_tensors = torch.zeros(len(names), max_len).to(torch.long)for i, idx in enumerate(sort_idx): for j, e in enumerate(names_list[idx]): name_tensors[i][j] = enames_lengths = torch.from_numpy(length[sort_idx]).to(torch.long)labels = labels[sort_idx].view(1, -1).squeeze(0) 训练1234567891011121314151617181920212223242526272829303132333435classifier = Classifier(len(chars), 128, 128)optimizer = optim.RMSprop(classifier.parameters(),lr=0.001)loss_func = nn.NLLLoss()total_loss = 0total_step = 0while True: for data in train_loader: total_step += 1 names = data[0] labels =data[1] name_tensors, labels, names_lengths = name_to_tensor(names, labels) out = classifier(name_tensors, names_lengths).view(-1, 2) # print(out) # print(labels) loss = loss_func(out, labels) total_loss += loss.item() loss.backward() optimizer.step() if total_step % 50 == 0: print('%dth step, avg_loss: %0.4f'%(total_step, total_loss/total_step)) with torch.no_grad(): for data in test_loader: names = data[0] labels = data[1] name_tensors, labels, names_lengths = name_to_tensor(names, labels) out = classifier(name_tensors, names_lengths).view(-1, 2) result = torch.argmax(out,dim=-1 ) result = (result == labels).to(torch.float) print(torch.mean(result)) break 完整代码和实验结果代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145# coding=utf-8import randomfrom torch.utils.data import DataLoaderfrom torch.utils.data import Datasetimport torch.nn as nnimport numpy as npimport torchimport torch.optim as optim################################load dataset ###################################################################################def load_data(path, label): names = [] with open(path) as f: lines = f.readlines() for l in lines: names.append((l.strip(&apos;\n&apos;), label)) return namesfemale_names = load_data(&apos;../datasets/names_gender/eng/female.txt&apos;, 0)male_names = load_data(&apos;../datasets/names_gender/eng/male.txt&apos;, 1)names = female_names + male_namesrandom.shuffle(names)# 将数据划分为训练集和测试集train_dataset = names[: int(len(names)*0.9)]test_dataset = names[int(len(names)*0.9):]# padding的字符为0,chars = [0] + [chr(i) for i in range(97,123)]# print(chars)class NameDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): # 这里可以对数据进行处理,比如讲字符数值化 data = self.data[index] name = data[0] label = data[1] return name, label def __len__(self): return len(self.data)train_dataset = NameDataset(train_dataset)test_dataset = NameDataset(test_dataset)train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)test_loader = DataLoader(test_dataset, batch_size=500, shuffle=True)# 序列按照长短顺序逆序排列def name_to_tensor(names, labels): names_list = [] for name in names: char_list = [] for ch in name.lower(): try: char_list.append(chars.index(ch)) except: char_list.append(0) # name_tensor = torch.from_numpy(np.array(char_list)) names_list.append(char_list) length = np.array([len(name) for name in names_list]) sort_idx = np.argsort(-length) max_len = max(length) name_tensors = torch.zeros(len(names), max_len).to(torch.long) for i, idx in enumerate(sort_idx): for j, e in enumerate(names_list[idx]): name_tensors[i][j] = e names_lengths = torch.from_numpy(length[sort_idx]).to(torch.long) labels = labels[sort_idx].view(1, -1).squeeze(0) return name_tensors, labels, names_lengths######################### model #################### #################################################################class Classifier(nn.Module): def __init__(self, input_size, hidden_size, embedding_size): super(Classifier, self).__init__() self.input_size = input_size self.hidden_size = hidden_size self.embedding_size = embedding_size self.embeddings = nn.Embedding(self.input_size, self.embedding_size) nn.init.xavier_normal(self.embeddings.weight.data) self.drop = nn.Dropout(p=0.1) self.rnn = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, batch_first=True) self.out = nn.Linear(self.hidden_size, 2) self.log_softmax = nn.LogSoftmax(dim=-1) def forward(self, input, length): input = self.embeddings(input) input = self.drop(input) input_packed = nn.utils.rnn.pack_padded_sequence(input, length, batch_first=True) # out_packed, (ht, ct) = self.rnn(input_packed, None) # out = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True) _, (ht, _) = self.rnn(input_packed, None) out = self.out(ht) out = self.log_softmax(out) return outclassifier = Classifier(len(chars), 128, 128)optimizer = optim.RMSprop(classifier.parameters(),lr=0.001)loss_func = nn.NLLLoss()total_loss = 0total_step = 0while True: for data in train_loader: total_step += 1 names = data[0] labels =data[1] name_tensors, labels, names_lengths = name_to_tensor(names, labels) out = classifier(name_tensors, names_lengths).view(-1, 2) # print(out) # print(labels) loss = loss_func(out, labels) total_loss += loss.item() loss.backward() optimizer.step() if total_step % 50 == 0: print(&apos;%dth step, avg_loss: %0.4f&apos;%(total_step, total_loss/total_step)) with torch.no_grad(): for data in test_loader: names = data[0] labels = data[1] name_tensors, labels, names_lengths = name_to_tensor(names, labels) out = classifier(name_tensors, names_lengths).view(-1, 2) result = torch.argmax(out,dim=-1 ) result = (result == labels).to(torch.float) print(torch.mean(result)) break 实验结果123456789101112131415161718192021222324252627282930313233343550th step, avg_loss: 0.4905100th step, avg_loss: 0.4662tensor(0.8200)150th step, avg_loss: 0.4535200th step, avg_loss: 0.4467tensor(0.8020)250th step, avg_loss: 0.4396300th step, avg_loss: 0.4320tensor(0.8260)350th step, avg_loss: 0.4270400th step, avg_loss: 0.4217tensor(0.8300)450th step, avg_loss: 0.4178500th step, avg_loss: 0.4117550th step, avg_loss: 0.4073tensor(0.8140)600th step, avg_loss: 0.4027650th step, avg_loss: 0.3994tensor(0.8260)700th step, avg_loss: 0.3971750th step, avg_loss: 0.3934tensor(0.8100)800th step, avg_loss: 0.3908850th step, avg_loss: 0.3884tensor(0.8160)900th step, avg_loss: 0.3861950th step, avg_loss: 0.38321000th step, avg_loss: 0.3822tensor(0.8140)1050th step, avg_loss: 0.38011100th step, avg_loss: 0.3789tensor(0.8060)1150th step, avg_loss: 0.37691200th step, avg_loss: 0.3757tensor(0.8420) 在训练的时候,验证的结果正确率最高能够达到86%.这个模型还比较粗糙,仅仅用名字的单个字母作为特征,模型还不能够完全分析出字母之间的联系.采用2-gram的方式,结果可能要好一点.这个待以后再去实现. [1] pytorch RNN处理变长的序列]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>pytorch</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch实现数据数据读取]]></title>
    <url>%2F2019%2F01%2F11%2Fpytorch-dataLoder-datasets%2F</url>
    <content type="text"><![CDATA[在学习的过程中,遇到很多的问题.在训练模型的时候,需要对数据进行读取操作.本篇文章,介绍pytorch如何自定义数据dataset和dataloader.在pytorch中,提供了Dataset这个类,负责对数据进行抽象,一次调用只返回第一个同样本.而Dataloader提供了对一个对一个batch的数据操作,还有对数据进行shuffle等功能. DatasetDataset是一个抽象类,表示一个数据集.所有的数据集类都是Dataset的子类.在自定义数据集的时候,需要对函数__getitem()__, __len()__重写. 1234567891011121314151617class Dataset(object): """An abstract class representing a Dataset. All other datasets should subclass it. All subclasses should override ``__len__``, that provides the size of the dataset, and ``__getitem__``, supporting integer indexing in range from 0 to len(self) exclusive. """ #每调用一次,返回一个数据样本, index表示返回样本的索引位置 #在数据处理中，有时会出现某个样本无法读取等问题，比如某张图片损坏。这时在__getitem__函数中将出现异常，此时最好的解决方案即是将出错的样本剔除。然后在数据集中,随机加入一条数据 def __getitem__(self, index): raise NotImplementedError #__len__ 数据集的长度 def __len__(self): raise NotImplementedError #增加数据,对数据集进行拼接 def __add__(self, other): return ConcatDataset([self, other]) DataLoaderDataLoader是一个可迭代的对象,意味着我们可以像使用迭代器一样使用它或者 batch_datas, batch_labels in dataloader:123456789101112DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False)'''dataset：加载的数据集(Dataset对象) batch_size：batch size shuffle:：是否将数据打乱 sampler： 样本抽样，后续会详细介绍 num_workers：使用多进程加载的进程数，0代表不使用多进程 collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可 pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些 drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃''' 例子在本次的例子中,我们利用Dataset和DataLoader处理预测姓名的性别的原始数据.下面是完整的例子.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# coding=utf-8import randomfrom torch.utils.data import DataLoaderfrom torch.utils.data import Datasetimport torch.nn as nn################################load dataset ###################################################################################def load_data(path, label): names = [] with open(path) as f: lines = f.readlines() for l in lines: names.append((l.strip('\n'), label)) return namesfemale_names = load_data('../datasets/names_gender/eng/female.txt', 0)male_names = load_data('../datasets/names_gender/eng/male.txt', 1)names = female_names + male_namesrandom.shuffle(names)# 将数据划分为训练集和测试集train_dataset = names[: int(len(names)*0.9)]test_dataset = names[int(len(names)*0.9)]class NameDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): # 这里可以对数据进行处理,比如讲字符数值化 data = self.data[index] name = data[0] label = data[1] return name, label def __len__(self): return len(self.data)train_dataset = NameDataset(train_dataset)test_dataset = NameDataset(test_dataset)train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True)for data in train_loader: print(data[0]) break 123#结果(&apos;Danelle&apos;, &apos;Ransell&apos;, &apos;Tanya&apos;, &apos;Rutter&apos;, &apos;Herschel&apos;)tensor([0, 1, 0, 1, 1])]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>数据读取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning(2)——MDPs]]></title>
    <url>%2F2018%2F06%2F10%2F20180610-reinforcement-learning-MDP%2F</url>
    <content type="text"><![CDATA[上一篇文章强化学习——简介简单介绍了一下强化学习的相关概念。这篇博客将引入 马尔科夫决策过程(Markov Decision Processes, MDPs)对强化学习进行建模。这篇文章，将对马尔科夫决策过程以及Q-leaning进行介绍。 马尔科夫过程 定义: 若随机过程 $ \left \lbrace X_n, n \in T \right \rbrace$对于任意非负正整数$n \in T$和任意的状态$i_0, i_1, …, i_n \in I$, 其概率满足$$P \left\lbrace X_{n+1}=i_{n+1} \mid X_0=i_0, X_1=i_1,…,X_n=i_n \right\rbrace =P \left\lbrace X_{n+1}=i_{n+1} \mid X_n=i_n \right\rbrace$$则称$ \left \lbrace X_n, n \in T \right \rbrace$是马尔科夫过程。 马尔科夫过程的一大特点是 无后效性 ,当一个随机过程在给定现在状态和过去状态情况下，其未来状态的条件概率仅依赖于当前状态。在给定现在状态是，随机过程与过去状态是条件独立的。也就是说，系统的下个状态只与当前状态有关，与更早之前的状态无关，一旦知道当前状态之后，过去的状态信息就可以抛弃。 马尔科夫过程还有一个重要的元素，那就是 转移概率矩阵。$P_{ij}(n) =P \left\lbrace X_{n+1}=j \mid X_n=i \right\rbrace$，指的时刻$n$，从状态$i$转移到状态$j$的一步转移概率。当$P_{ij}(n)$y与$n$无关是，则称马尔科夫链$ \left \lbrace X_n, n \in T \right \rbrace$是齐次的。 $P=\begin {bmatrix}{p_{11}}&amp;{p_{12}}&amp;{\cdots}&amp;{p_{1n}}\\{p_{21}}&amp;{p_{22}}&amp;{\cdots}&amp;{p_{2n}}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{p_{n1}}&amp;{p_{n2}}&amp;{\cdots}&amp;{p_{nn}}\end {bmatrix}$；其中，${p_{ij}\geq 0}$，$\sum_{j \in I} p_{ij} = 1$因此，定义一个马尔科夫过程，需要给出有限状态集合$\{ x_i \in X, i \in N \}$以及转移概率矩阵$P$。下图是一个马尔科夫链转移矩阵的例子。 马尔科夫决策过程上一篇文章介绍了强化学习是一个与不断进行环境交互学习只能agent的学习过程。而MDPs正是立足于agent与环境的直接交互，只考虑离散时间，假设agent与环境的交互可分解为一系列阶段，每个阶段由“感知——决策——行动”构成。如下图：MDP模型是一个四元组$(S,A,T,R)$ [1] Hexo编辑数学公式： http://bennychen.me/mathjax.html]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>马尔科夫决策过程</tag>
        <tag>Q-learning</tag>
        <tag>值迭代</tag>
        <tag>策略迭代</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning(1)]]></title>
    <url>%2F2018%2F06%2F03%2Freinforcement-learning-1%2F</url>
    <content type="text"><![CDATA[开始写强化学习这个系列的文章，主要是为了记录学习过程中学习到的知识点，分享自己的理解。 强化学习简介强化学习（英语：Reinforcement learning，简称RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。 强化学习涉及很多学科，其中包括计算机、神经科学、工程学科等，详细可见下图。机器学习大致可以分为强化学习、监督学习以及非监督学习这三种，这三者之间的关系如下图所示。强化学习属于机器学习中的一种，与我们熟悉的深度学习有一些区别。强化学习最大的特点就是能够与环境进行交互，通过交互学习到最优的方法解决问题。强化学习和标准的监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。 最直观的方式去区分这三种学习方法的特征就是数据的形式： 监督学习：数据带有明确的标签，这些标签可以看做是一个supervisor，指导学习数据之间的关系。常见应用有的分类、回归等 非监督学习：数据没有标签，算法学习数据特征之间的内部关系。常见有聚类。 强化学习： 数据没有标签，在强化学习系统中，agent感知环境变化，接受每一次行为产生的奖励(reward)，决策下一步的行为。 强化学习的特点： 正如上面所说的，强化学习没有supervisor，只有一个每次行为之后产生的奖励reward 环境对行为的反馈存在延迟，不是即时的 时间的影响很大，系统是一个连续的变化过程 agent的行为会影响下一次接受的数据。 强化学习系统上图为强化学习(RL)系统的原理图。用马尔科夫决策过程(Markov Decision Process, MDP)对RL问题进行建模。通常将MDP定义为一个四元组(S,A,ρ,f) 强化学习四个元素 策略（Policy）policy是agent的一系列行为，也是状态到行为的映射。Policy可以分为两类，一个是确定性策略（deterministic policy）a = π(s)即状态与策略之间的关系是确定的，当系统处于某种状态，必然有一个策略与之对应。另一列是随机策略（Stochastic policy）π(a∣s) = Ρ[At =a ∣St = s],这是在一个状态下，对应于不同策略的概率 值函数（value function）值函数是对未来奖励预测的函数，定义的是在状态s下，采取策略π的长期奖励 奖励信号（a reward signal）Reward就是一个标量值，是每个time step中环境根据agent的行为返回给agent的信号，reward定义了在该情景下执行该行为的好坏，agent可以根据reward来调整自己的policy。常用R来表示。 环境模型（a model of the environment），预测environment下一步会做出什么样的改变，从而预测agent接收到的状态或者reward是什么。 agent与environment上图是agent与environment之间的交互。现在来介绍一个关于agent和environment之间的一些概念。 历史（History） 与 状态（State） history是observations、actions以及rewards的序列。下一步要发生的事件要依赖于history。 state是决策下一步行动的信息，通常是一个history的函数。 environment state、 agent state、 information state enviroment state是环境的私有状态，通常对agent是不可见的 agent state是agent的内部表现状态，比如agent决策下一步行动所选用的信息等。是历史的一个函数。 information state包含来自于历史的所有有用信息。 fully observable environments 和 partially obervable environments full observability, agent能够直接观察到环境的变化状态，是一个马尔科夫决策过程 partial observability, agent只能简洁观察环境，这是一个部分马尔科夫决策过程（partially observable Markov decision process，POMDP）。代理必须根据其他的信息，建模自己的状态模型。 RL问题中agent的分类]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>blogs</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客上线啦！]]></title>
    <url>%2F2018%2F04%2F03%2Fpublish-first-blog%2F</url>
    <content type="text"><![CDATA[搞了一下午和一晚上，博客终于弄好了，现在来测试一下。献上一首很喜欢的诗《致橡树》 致橡树我如果爱你绝不像攀援的凌霄花，借你的高枝炫耀自己；我如果爱你绝不学痴情的鸟儿，为绿荫重复单调的歌曲；也不止像泉源，常年送来清凉的慰藉；也不止像险峰，增加你的高度，衬托你的威仪。甚至日光。甚至春雨。不，这些都还不够！我必须是你近旁的一株木棉，做为树的形象和你站在一起。根，紧握在地下，叶，相触在云里。每一阵风过，我们都互相致意，但没有人，听懂我们的言语。你有你的铜枝铁干，像刀，像剑，也像戟，我有我的红硕花朵，像沉重的叹息，又像英勇的火炬，我们分担寒潮、风雷、霹雳；我们共享雾霭、流岚、虹霓，仿佛永远分离，却又终身相依，这才是伟大的爱情，坚贞就在这里：爱不仅爱你伟岸的身躯，也爱你坚持的位置，脚下的土地。 ——舒婷《致橡树》]]></content>
      <categories>
        <category>blogs</category>
      </categories>
      <tags>
        <tag>诗</tag>
      </tags>
  </entry>
</search>
