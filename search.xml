<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2020]]></title>
    <url>%2F2021%2F01%2F08%2F20210106-2020%2F</url>
    <content type="text"><![CDATA[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 输入密码，查看文章。]]></content>
      <categories>
        <category>essay</category>
      </categories>
      <tags>
        <tag>感</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五种IO模型]]></title>
    <url>%2F2019%2F07%2F07%2F201907071452-IO-model-1%2F</url>
    <content type="text"><![CDATA[之前看了一点关于IO的模型，在看JAVA的时候，IO也是很重要的部分，其中最关键的NIO也是重中之重。在Java中，有三种IO模型，分别是阻塞IO（Block IO），非阻塞IO（NIO）和异步IO（AIO）。本篇笔记中的IO模型指的是操作系统中的IO模型，上面JAVA中的IO模型是只是Java API中的几种IO实现。 I/O 控制方式在正式介绍IO模型之前，介绍一下计算机是如何管理外围设备和内存之间的I/O的。 图1 三种IO控制方式 程序直接控制计算机从外部设备读取数据到存储器，每次读取一个字的数据。对于读入的每个字，CPU需要对外围设备状态进行循环检查，知道确定该字已经在I/O控制器的数据寄存器中。在直接程序控制方式中，由于CPU的高塑性和I/O 设备的低速性，导致CPU的绝大部分分时间都处于等待I/O设备完成数据I/O的循环测试中，造成了CPU资源的浪费。在该方式中，CPU之所以要不断测试I/O设备的状态，就是因为在CPU中没有采用中断机构，使I/O设备无法向CPU报告它已经完成了一个字符的输入操作。程序直接控制方式实现简单，但是CPU和I/O设备只能串行工作，导致CPU的利用率相当低。 中断驱动方式中断驱动方式的思想是，允许I/O设备主动打断CPU的运行并请求服务，从而“解放”CPU，使得其向I/O控制器发送读命令后可以继续做其他有用的工作。如图5-1(b)所示，我们从I/O控制器和CPU两个角度分别来看中断驱动方式的工作过程：从I/O控制器的角度来看，I/O控制器从CPU接收一个读命令，然后从外围设备读数据。一旦数据读入到该I/O控制器的数据寄存器，便通过控制线给CPU发出一个中断信号，表示数据已准备好，然后等待CPU请求该数据。I/O控制器收到CPU发出的取数据请求后，将数据放到数据总线上，传到CPU的寄存器中。至此，本次I/O操作完成，I/O控制器又可幵始下一次I/O操作。 从CPU的角度来看，CPU发出读命令，然后保存当前运行程序的上下文（现场，包括程序计数器及处理机寄存器），转去执行其他程序。在每个指令周期的末尾，CPU检查中断。当有来自I/O控制器的中断时，CPU保存当前正在运行程序的上下文，转去执行中断处理程序处理该中断。这时，CPU从I/O控制器读一个字的数据传送到寄存器，并存入主存。接着， CPU恢复发出I/O命令的程序（或其他程序）的上下文，然后继续运行。 中断驱动方式比程序直接控制方式有效，但由于数据中的每个字在存储器与I/O控制器之间的传输都必须经过CPU,这就导致了中断驱动方式仍然会消耗较多的CPU时间。 DMA(直接内存存取)在中断驱动方式中，I/O设备与内存之间的数据交换必须要经过CPU中的寄存器，所以速度还是受限，而DMA（直接存储器存取）方式的基本思想是在I/O设备和内存之间开辟直接的数据交换通路，彻底“解放” CPU。DMA方式的特点是： 基本单位是数据块。 所传送的数据，是从设备直接送入内存的，或者相反。 仅在传送一个或多个数据块的开始和结束时，才需CPU干预，整块数据的传送是在DMA控制器的控制下完成的。 图2 DMA控制器结构图 为了实现在主机与控制器之间成块数据的直接交换，必须在DMA控制器中设置如下四类寄存器(如上图所示)： 命令/状态寄存器(CR)：用于接收从CPU发来的I/O命令或有关控制信息，或设备的状态。 内存地址寄存器(MAR)：在输入时，它存放把数据从设备传送到内存的起始目标地址；在输出时，它存放由内存到设备的内存源地址。 数据寄存器(DR)：用于暂存从设备到内存，或从内存到设备的数据。 数据计数器(DC)：存放本次CPU要读或写的字（节）数。 如图1(c)所示，DMA方式的工作过程是：CPU读写数据时，它给I/O控制器发出一条命令，启动DMA控制器，然后继续其他工作。之后CPU就把控制操作委托给DMA控制器，由该控制器负责处理。DMA控制器直接与存储器交互，传送整个数据块，每次传送一个字，这个过程不需要CPU参与。当传送完成后，DMA控制器发送一个中断信号给处理器。因此只有在传送开始和结束时才需要CPU的参与。 DMA控制方式与中断驱动方式的主要区别是中断驱动方式在每个数据需要传输时中断CPU，而DMA控制方式则是在所要求传送的一批数据全部传送结束时才中断CPU；此外，中断驱动方式数据传送是在中断处理时由CPU控制完成的，而DMA控制方式则是在DMA 控制器的控制下完成的。 通道控制方式I/O通道是指专门负责输入/输出的处理机。I/O通道方式是DMA方式的发展，它可以进一步减少CPU的干预，即把对一个数据块的读（或写）为单位的干预，减少为对一组数据块的读（或写）及有关的控制和管理为单位的干预。同时，又可以实现CPU、通道和I/O设备三者的并行操作，从而更有效地提高整个系统的资源利用率。 例如，当CPU要完成一组相关的读（或写）操作及有关控制时，只需向I/O通道发送一条I/O指令，以给出其所要执行的通道程序的首地址和要访问的I/O设备，通道接到该指令后，通过执行通道程序便可完成CPU指定的I/O任务，数据传送结束时向CPU发中断请求。I/O通道与一般处理机的区别是：通道指令的类型单一，没有自己的内存，通道所执行的通道程序是放在主机的内存中的，也就是说通道与CPU共享内存。 I/O通道与DMA方式的区别是：DMA方式需要CPU来控制传输的数据块大小、传输的内存位置，而通道方式中这些信息是由通道控制的。另外，每个DMA控制器对应一台设备与内存传递数据，而一个通道可以控制多台设备与内存的数据交换。 I/O模型介绍完I/O设备的控制的四种方式之后，再来介绍操作系统层面上的IO模型，在Linux中一共有五种，分别是阻塞IO、非阻塞IO、IO复用、信号驱动IO以及异步IO。 阻塞IO在Linux中，最简单的IO模型就是阻塞IO模型，一般表现为进程或者线程等待某个条件，如果条件不足，则一直等待下去。条件满足，则进行下一步操作。其实这个很像是上面讲的程序直接控制，进程会一直运行下去，直到IO完成。 非阻塞IO模型对于阻塞IO来说，执行IO的进程或者线程会一直处于等待IO完成的状态中，这段等待时间，线程资源一直被占用，不能执行其他的工作。而非阻塞IO模型，在进行提交完IO指令之后，不再需要进程一直等待，可以去执行一些其他的工作。这样的话，可以利用原来等待而一直被占用的资源。非阻塞模型，还是需要主动了解IO操作的状态，比如询问内核数据是否准备好。询问采用轮询的方式。 信号驱动IO模型上面的非阻塞IO在处理的时候，还需要定时轮询了解内核的状态。还有有一些费事。而信号驱动IO模型呢，则是应用程序预先向内核注册一个信号函数，然后用户返回，并且不阻塞。当内核数据准备就绪的时候发送一个信号给进行，用户进行便在信号处理函数中开始把数据拷贝到用户空间中。这种方式是将事件信息的汇报交给内核，而不用用户去轮询。 IO复用模型多个进程的IO可以注册到同一个管道中，这个管道会统一和IO进行交互，当管道中的某一个请求需要的数据准备好之后，进程再把对应的数据拷贝到用户空间中。IO多路转接是多了一个select函数，多个进程的IO可以注册到同一个select上，当用户进程调用该select，select会监听所有注册好的IO，如果所有被监听的IO需要的数据都没有准备好时，select调用进程会阻塞。当任意一个IO所需的数据准备好之后，select调用就会返回，然后进程在通过recvfrom来进行数据拷贝。 这里的IO复用模型，并没有向内核注册信号处理函数，所以，他并不是非阻塞的。进程在发出select后，要等到select监听的所有IO操作中至少有一个需要的数据准备好，才会有返回，并且也需要再次发送请求去进行文件的拷贝。 以上四种模型都是同步模型。 异步IO模型应用进程把IO请求传给内核后，完全由内核去操作文件拷贝。内核完成相关操作后，会发信号告诉应用进程本次IO已经完成。 用户进程发起aio_read操作之后，给内核传递描述符、缓冲区指针、缓冲区大小等，告诉内核当整个操作完成时，如何通知进程，然后就立刻去做其他事情了。当内核收到aio_read后，会立刻返回，然后内核开始等待数据准备，数据准备好以后，直接把数据拷贝到用户控件，然后再通知进程本次IO已经完成。用户不用关心数据的拷贝等工作，只要提交IO操作命令，其他的事情由内核自己完后。 引用 五种IO 《操作系统》总结五：I/O 管理]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HiveQL 数据操作]]></title>
    <url>%2F2019%2F06%2F28%2F201906281616-hive-hiveQL-dataOperation%2F</url>
    <content type="text"><![CDATA[HiveSQL对Hive的数据操作包括对内部表、外部表、分区表以及桶表的数据的导入以及导出。 [TOC] HiveSQL 数据加载和导出装载数据到表中Hive不提供行级数据的插入，更新以及删除操作，因此当创建一个新的表，往表中装载数据是以大量数据的装载实现的，也就是把数据文件装载到HDFS文件系统表对应的目录下。1Load data local inpath &quot;root/file.txt&quot; into table test_table; Load 命令将文件装载到Hive表对应的目录中。 Local 指本地文件系统路径 Load data local… 表示把文本文件系统的文件装载到Hive在HDFS文件系统的目录 OVERWRITE关键字表示覆盖之前的文件内容，不加这个关键字，只会在原来的内容上进行追加 通过查询语句向表中插入数据Insert语句允许用户通过查询语句向目标表中插入数据。1234INSERT OVERWRITE TABLE employeesPARTITION(country=&apos;US&apos;, state=&apos;OR&apos;)SELECT * from employees_tmp etWHERE et.cnty=&apos;US&apos; AND et.st=&apos;OR&apos; 对于以上具有分区表的插入，加入数据很大，对于每一个分区表都要写一条插入语句，这样会显得很繁琐。比如下方，12345678FROM empleyees_tmo seINSERT OVERWRITE TABLE employeesPARTITION(country=&apos;US&apos;, state=&apos;OR&apos;)SELECT * WHERE et.cnty=&apos;US&apos; AND et.st=&apos;OR&apos;PARTITION(country=&apos;US&apos;, state=&apos;CA&apos;)SELECT * WHERE et.cnty=&apos;US&apos; AND et.st=&apos;CA&apos;PARTITION(country=&apos;US&apos;, state=&apos;IL&apos;)SELECT * WHERE et.cnty=&apos;US&apos; AND et.st=&apos;IL&apos; 对于上面分区很多的情况，可以在插入的时候通过动态分区的方法自动分区，将上面的命令改下为：1234INSERT OVERWRITE TABLE employeesPARTITION(country, state)SELECT ..., et.cnty, et.stFROM employees_tmp et; 单个查询语句中创建并加载数据1234CREATE TABLE employee_newAS SELECT name, salary, addressFROM employee emWHERE em.state = &apos;US&apos;; 这个功能不能用到外部表。 导出数据导出数据分成两种情况： 将表目录下的数据文件导出即可hadoop fs -cp source_path target_path 对于一些查询结果需要保存的时候，可以使用命令：1234INSERT OVERWRITE LOCAL DIRECTORY &apos;/tmp/employee&apos;SELECT name,salary, addressFROM employee emWHERE em.state= &apos;US&apos;; 无论数据在数据源如何存储，HIVE都会将所有的字符段列化成字符串写到文件中。Hive使用和Hive内部存储的表相同的编码方式来生成输出文件。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka消费者]]></title>
    <url>%2F2019%2F06%2F28%2F201906281609-kafka-consumer%2F</url>
    <content type="text"><![CDATA[Kafka消费者，生产者生产消息，并将消息发送到服务器中。而消费者则是根据topic订阅消息，从服务器上拉取消息进行处理。 消费者和消费者群组 消费者的概念很好理解，就是从Kafka服务器上拉取数据的进程。一个消费者可以消费一个Topic的所有分区。但是在实际生产环境下，存在多个消费者共同消费同一个Topic的消息，一般来说这多个消费者所做的任务是一样的，他们共同组成一个消费者群组。消费者群组的消费者分别处理一个Topic的所有分区，但是彼此消费的分区不重合。此外，随着消费者的增多，集群会出现分区再均衡，也就是分区重新分配给消费者。 消费者群组中的消费者的数量应该不大于一个Topic消息分区的数量，否则多余的消费者将不会得到消息。 多个消费者群组可以共享一个Topic消息分区，但是消费者群组之间的分区分配彼此是独立的。比如说现在一个Topic有四个分区，Group1有四个消费者，每个消费者获得一个分区；Group2有两个消费者，每个消费者消费两个分区的消息。两个群组之间消息的消费也是独立的，Group1与Group2的分区偏移量互不影响。 创建消费者消费者的创建于生产者的创建很相似，比如指定bootstrap.servers参数，让消费者知道服务器的位置；指定key.deserializer，value.deserializer，将Key和Value的字节数组反序列化成对象。此外还可以指定该消费者的消费者组群id，即group.id. 消费者要想获得消息，需要先订阅消息，也就是要知道Topic是什么，对此调用subscribe()函数。值得注意的是subscribe（）函数可以接受正则表达式，指定多个Topic。 消费者采用轮询的方法向服务器请求数据。轮询不仅是请求数据，还要想服务器发送心跳保持会话的持续性。长时间消费者不发送心跳，服务器会认为该消费者与分区的连接断开，出发分区再均衡事件。1234567891011121314151617181920212223242526272829303132333435363738394041public class SimpleKafkaConsumer &#123; public static void main(String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "localhost:9092"); //每个消费者分配独立的组号 props.put("group.id", "test"); //如果value合法，则自动提交偏移量 props.put("enable.auto.commit", "true"); //设置多久一次更新被消费消息的偏移量 props.put("auto.commit.interval.ms", "1000"); //设置会话响应的时间，超过这个时间kafka可以选择放弃消费或者消费下一条消息 props.put("session.timeout.ms", "30000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Collections.singletonList("test")); System.out.println("Subscribed to topic " + "test"); int i = 0; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) // print the offset,key and value for the consumer records. System.out.printf("offset = %d, key = %s, value = %s\n", record.offset(), record.key(), record.value()); &#125; &#125;&#125; 偏移量分区中每条消息都有一个偏移量，标记着消息在分区中得位置。消费者在消费消息的时候也会通过标记偏移量来定位目前已经消费的消息的位置，和下一次消费的消息应该从哪里开始。 偏移量的提交：消费者向名为_comsumer_offset的特殊主题发送消息，消息里包含了每个分区的偏移量。（按照我的理解，应该是每个消费者群组都会维护当前消费者的分区的偏移量） 偏移量的提交有五种方式： 自动提交 提交当前偏移量 异步提交 同步和异步组合提交 提交指定的偏移量 如何进行偏移量的事务存储将消息的处理和当前偏移量的提交当做一个原子操作，比如讲消息处理结果和偏移量存储到数据库里。若一个消费者要失去分区的所有权，在这之前，要把当前的事务提交，保证偏移量的准确性。 对于消费者想要获得当前分区的偏移量的问题，只要查询数据库或者其他文件系统存储的偏移量。 反序列化器这与生产者序列化器相对应，将生产者序列化为字节数组的数据反序列化为原始数据。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 生产者简介]]></title>
    <url>%2F2019%2F06%2F28%2F201906281603-kafka-producer%2F</url>
    <content type="text"><![CDATA[Kafka是分布式消息发布和订阅的消息系统。而生产者则对应的是消息的生产和发布。也就是说，生产者是消息的来源。生产者将消息发送搭kafka中。 ProducerRecordKafka生产者发送的消息封装成ProduceRecord，其中包含了消息的主题和要发送的内容。ProducerRecord对象有四个属性： Topic 消息的主题 Partition 消息也可以指定发送到的partition Key， 通过计算Key，可以定位该消息要发送到那个Partition上 Value， 也就是消息的内容了 创建生产者Kafka生产者需要指定三个必要的属性： bootstrap.servers, 指定broker的地址清单，格式为host:port。（一般建议指定两个broker，防止宕机）； key.serializer，在网络传输中，消息的键值和值需要序列化成字节数组。即使消息的key为空，也需要指定； value.serializer, 将消息的值序列化成字节数组。 消息发送方式生产者调用send()函数发送消息，有三种发送的方式，分别是： 1. 同步发送：顾名思义。生产者给broker发送玩消息之后，需要等待broker的响应，这才能知道消息是发送成功还是发生了错误。这种模式下，send()函数会返回一个Future对象，调用Future对象的get()等待broker的响应。 2. 异步发送：同步发送，生产者需要等到broker的响应，此时生产者是处于阻塞状态的。频繁发送消息的时候，这种等待会降低kafka的吞吐量。异步发送采用回调函数的形式，生产者发送完消息之后，不用同步阻塞，broker在响应的时候，调用回调函数向生产者发送消息。 3. 发送并忘记：生产者发送消息之后，不再等待broker的响应。也就是说在这种模式下，生产者不关心消息是否正常到达。 其他重要参数（非必须）前面已经讲了三个生产者的重要参数，还有几个参数值得我们注意。 acks：指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入成功。acks=0时，生产者在成功写入消息之前不会等待任何来自服务器的响应。acks-1时，只要集群的首领节点受到消息，生产者就会收到一个来自服务器的成功响应。acks=all时，只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。 buffer.memory：改参数制定了生产者内存缓冲的大小，生产者用它缓冲要发送到服务器的消息。当消息的发送速度小于应用程序产生消息的速度是，或导致生产者空间不足。此时send()函数要么被阻塞，要么抛出异常。 compression.type:指定消息放松给broker之前使用的压缩算法的类型。 retries: 当生产者的消息发送到服务器产生错误是临时性错误时，生产者将会重发消息。retries指定了消息重发的次数，当重发次数超过这个数时，生产者将会放弃重发，并返回错误。retry.backoff.ms参数指定了重发之间的时间间隔，默认是100ms，也可以更改这个设置。 batch.size：当有多个消息发送到同一个分区时，生产者会把他们放在同一个批次里。这个参数指定了同一个批次可以使用的内存的大小，按照字节数计算。 linger.ms:指定生产者发送批次之前等待更多消息加入批次的时间。KafkaProducer会在批次填满或者linger.ms达到上限是把批次发送出去。 client.id:该参数可以试试任意字符串，服务器会用它来识别消息的来源，还可以用在日志和配额指标里。 max.in.flight.requests.per.connection:该参数指定了生产者在收到服务器响应之前可以发送多少消息。把它设置为1可以保证消息是按照发送顺序写入服务器的，及时发生了重试。Kafka一个分区内的消息有序性依靠这个参数来设置，当第一次消息发送失败的重试之前，不会有其他消息发送给broker。 timeout.ms / request.timeout.ms / metadata.fetch.timeout.ms:request.timeout.ms指定了生产者发送数据时等待服务器返回响应的时间; metadata.fetch.timeout.ms指定了生产者在获取元数据（比如目标分区的首领是谁）是等待服务器返回响应的时间。如果服务器响应超时，那么生产者要么重试发送数据，要么返回一个错误（抛出异常使用回调)。timeout.ms指定了broker等待同步副本返回消息的确认时间，与asks的配置相匹配，如果在指定时间没有收到同步副本的确认，那么broker就会返回一个错误。 max.block.ms：指定了在调用send()方法或者使用partitionsFor()方法获取元数据时生产者的阻塞时间。当生产者发送缓冲区已满，或者没有可用的元数据的时候，这些方法就会被阻塞。在阻塞时间到达max.block.ms时，生产者就会抛出异常。 max.request.size:用于控制生产者发送的请求的发小，他可以指定发送的单个消息的最大值，也可以之单个请求所有消息总的大小。 receive.buffer.bytes和send.buffer.bytes：这两个参数分别指定了TCP socket接受和发送数据包的缓冲大小。若他们被指定为-1，则表示使用操作系统的默认值。 这篇文章记录了Kafka生产者的一些特点。生产者作为Kafka消息的生产者，在这个消息系统中的作用是极其巨大的，我们也应该掌握生产者的实现以及一些实际条件下的优化。 Kafka序列化器与分区生产者将消息序列化成字节数组，通过网络发送给服务器。在生产者的设置中存在两种，分别是key.serializer和value.serializer. 为什么不建议使用自定义序列化器为了避免耦合，序列化器和反序列化器之间的兼容问题值得注意。自定义序列化器一旦更改，会导致使用该序列化器的工作以及反序列化器都需要进行相应的改变。因此建议使用现有的序列化器和反序列化器，诸如JSON，Avro，Thrift，Protobuf等 Avro序列化器Avor是一种编程语言无关性的序列化格式，这一点是采用与语言无关的schema来定义的。Schema通过Json来描述，数据被序列化成二进制文件或者Json文件。Avro在读写文件的时候，会用到schema，虽然Schema一般会嵌入到数据文件里，但是在kafka中，假如每一条消息都嵌入了schema，这样会导致大量的数据冗余。因此Kafka采用了通用的结构模式并使用schema注册表来实现。也就是将schema写入到注册表中，然后再记录中应用schema标识符，负责读取数据的应用程序只要从注册表中拉去schema来反序列化记录。序列化器只要将schema注册到schema注册表。 分区kafka producerRecord对象包含了目标主题、键以及值。其中键的作用有： 作为消息的附加信息； 用来决定消息分配到主题的哪一个分区。当键值为空的时候，分区器将消息采用轮询的方式均衡地发送到各个分区上。对于键决定分区的分配要在主题的数量不变的情况下才有效，随着分区数量的改变，键值的散列也会不同。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka(1)]]></title>
    <url>%2F2019%2F06%2F16%2F201906162114-kafka-1%2F</url>
    <content type="text"><![CDATA[Kafka是一个消息队列。 为什么需要消息队列？在实际开发过程中出现的问题，也就是消息队列为什么要出现的原因。 常见的问题有： 应用解耦 异步通信 削峰/限流 以应用解耦这个为例，比如在开发一个网站，对于一个 引用 什么是消息队列 消息队列技术]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java实现SparkStreaming(用户商品浏览日志)]]></title>
    <url>%2F2019%2F06%2F11%2F201906112027-spark-java-2%2F</url>
    <content type="text"><![CDATA[本篇文章用spark实现了用户浏览购物网站的流处理。 一般在浏览淘宝之类的购物网站，对于每一个商品，都会记录你在浏览该商品时发生的行为，比如网页驻留时间、网页跳出、是否收藏该商品，以及是否购买商品。这些记录能够用来分析用户对这些商品的兴趣度，通过大数据分析，还可以分析当前购物网站最流行的商品等。 这篇文章主要是学习实验楼的一个项目《基于Spark实时计算商品关注度》 数据源使用Spark进行流处理，需要指定数据源，一般来说数据源可以是Socket流，文件流，还可以是Kafka流等。在这里采用Socket模拟实现用户浏览商品的行为数据。采用一个客户端socket向服务器socket持续发送数据。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.icesun.spark.streaming;import com.sun.xml.internal.ws.server.ServerSchemaValidationTube;import org.apache.orc.impl.OutStream;import java.io.IOException;import java.io.OutputStream;import java.io.PrintWriter;import java.net.ServerSocket;import java.net.Socket;import java.util.Random;public class SimulatorSocket &#123; public static void main(String[] args)&#123; new Thread(new SimulatorSocketLog()).start(); &#125;&#125;class SimulatorSocketLog implements Runnable&#123; //假设一共有200个商品 private int GOODSID = 200; //随机发送消息的条数 private int MSG_NUM = 20; //假设用户浏览商品的次数 private int BROWSE_NUM = 5; //假设用户浏览商品停留时间 private int STAY_TIME = 10; //用户是否收藏， 1为收藏，0为不收藏，-1为差评 int[] COLLECTION = new int[]&#123;-1, 0, 1&#125;; //模拟用户购买的商品件数，0比较多视为了增加没有买的概率 private int[] BUY_NUM = new int[]&#123;0,1,0,2,0,0,0,1,0&#125;; @Override public void run() &#123; Random r = new Random(); try &#123; //套接字服务器 端口为9999 ServerSocket server = new ServerSocket(9999); System.out.println("成功开启数据模拟模块，运行Streaming程序"); while (true)&#123; int msgNum = r.nextInt(MSG_NUM) +1; //套接字客户端与服务器端连接 Socket socket = server.accept(); //输出流 OutputStream os = socket.getOutputStream(); PrintWriter pw = new PrintWriter(os); for (int i = 0; i &lt; msgNum; i++)&#123; StringBuffer sb = new StringBuffer(); sb.append("goodsID-"+(r.nextInt(GOODSID)+1)); sb.append("::"); sb.append(r.nextInt(BROWSE_NUM)+1); sb.append("::"); sb.append(r.nextInt(STAY_TIME)+r.nextFloat()); sb.append("::"); sb.append(COLLECTION[r.nextInt(2)]); sb.append("::"); sb.append(BUY_NUM[r.nextInt(9)]); System.out.println(sb.toString()); //发送消息 pw.write(sb.toString()+"\n"); &#125; pw.flush(); pw.close(); Thread.sleep(1000); &#125; &#125; catch (IOException e) &#123; System.out.println("port used"); e.printStackTrace(); &#125; catch (InterruptedException e) &#123; System.out.println("Thread sleep failed"); e.printStackTrace(); &#125; &#125;&#125; SparkStreamingSpark Streaming能够实现实时数据的处理，但实际上其实微批处理。为什么是微批处理呢，这是因为spark将数据以时间连续的方式，将一段时间内的数据封装成RDD，流实际上就是一连串时间连续的RDD序列，这就是离散流。 用流处理分析一段时间内商品的热度，其实与Spark core离线分析数据差不多。http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html已经介绍了很多关于Spark Streaming处理的理论知识。 这里将上面提到的问题的代码贴出来。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104package com.icesun.spark.streaming;import org.apache.spark.HashPartitioner;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.Optional;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.api.java.function.VoidFunction;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import scala.Tuple2;import java.io.Serializable;import java.util.List;public class GoodsStreaming implements Serializable &#123; private static final long serialVersionUID = 1L; private static String checkPoint = "files/checkDir"; public static void main(String[] args) throws InterruptedException&#123; SparkConf conf = new SparkConf().setAppName("GoodsStreaming").setMaster("local[2]"); JavaStreamingContext ssc = new JavaStreamingContext(conf, Durations.seconds(1)); ssc.checkpoint(checkPoint); JavaReceiverInputDStream&lt;String&gt; dStream = ssc.socketTextStream("127.0.0.1", 9999); JavaDStream&lt;String&gt; mess = dStream.map(new Function&lt;String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public String call(String v1) throws Exception &#123; return v1; &#125; &#125;); mess.print(); //输出流文件以行为单位，实现map操作，将字符串编程k-v对，k为商品的名称，value为计算的得出的follow值 JavaPairDStream&lt;String,Double&gt; splitMess = dStream.mapToPair(new PairFunction&lt;String, String, Double&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Double&gt; call(String s) throws Exception &#123; String[] lineSplit = s.toString().split("::"); Double followValue = Double.parseDouble(lineSplit[1])* 0.8 + Double.parseDouble(lineSplit[2])*0.6 + Double.parseDouble(lineSplit[3])*1 + Double.parseDouble(lineSplit[4])*1; return new Tuple2&lt;String, Double&gt;(lineSplit[0], followValue); &#125; &#125;); //根据k值，对商品的follow值进行更新。 JavaPairDStream&lt;String, Double&gt; updateFollowValue = splitMess.updateStateByKey(new Function2&lt;List&lt;Double&gt;, Optional&lt;Double&gt;, Optional&lt;Double&gt;&gt;() &#123; @Override public Optional&lt;Double&gt; call(List&lt;Double&gt; newValue, Optional&lt;Double&gt; stateValue) throws Exception &#123; //加入之前的流中没有出现商品，那将他的初始状态设置为0 Double updateValue = stateValue.or(0.0); for(Double value : newValue)&#123; updateValue += value; &#125; return Optional.of(updateValue); &#125; &#125;, new HashPartitioner(ssc.sparkContext().defaultParallelism())); //根据商品的热度进行排序。但是在原来的updatFollowValue中，每个离散RDD其实是一个&lt;String, Double&gt; Tuple, //因此需要把Tuple中的内容进行调换形成&lt;Double,String&gt;，然后进程SortByKey操作 //最后将排序后的Tuple的内容还原成&lt;String,Double&gt; updateFollowValue.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String, Double&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public void call(JavaPairRDD&lt;String, Double&gt; stringDoubleJavaPairRDD) throws Exception &#123; JavaPairRDD&lt;Double, String&gt; followValueSort = stringDoubleJavaPairRDD.mapToPair(new PairFunction&lt;Tuple2&lt;String, Double&gt;, Double, String&gt;() &#123; @Override public Tuple2&lt;Double, String&gt; call(Tuple2&lt;String, Double&gt; stringDoubleTuple2) throws Exception &#123; return new Tuple2&lt;Double, String&gt;(stringDoubleTuple2._2, stringDoubleTuple2._1); &#125; &#125;).sortByKey(false); List&lt;Tuple2&lt;String,Double&gt;&gt; list = followValueSort.mapToPair(new PairFunction&lt;Tuple2&lt;Double,String&gt;,String, Double&gt;() &#123; public Tuple2&lt;String, Double&gt; call( Tuple2&lt;Double, String&gt; arg0) throws Exception &#123; // TODO Auto-generated method stub return new Tuple2&lt;String,Double&gt;(arg0._2,arg0._1); &#125; &#125;).take(10); for(Tuple2&lt;String,Double&gt; tu:list)&#123; System.out.println("商品ID: "+tu._1+" 关注度: "+tu._2); &#125; &#125; &#125;); ssc.start(); ssc.awaitTermination(); &#125;&#125; 未来，将对这个问题进行持续改进，比如使用Kafka作为数据源，还可以分析更多数据问题。这里只作为一个Spark Streaming处理的基石。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>网络编程</tag>
        <tag>SparkStreaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程（1）]]></title>
    <url>%2F2019%2F06%2F10%2F201906101705-multiThread-1%2F</url>
    <content type="text"><![CDATA[本文主要介绍多线程的相关概念。 创建多线程的方法https://blog.csdn.net/king_kgh/article/details/78213576中有详细的介绍。 继承Thread类； 实现Runnable接口； 实现Callable结构，与Runnable类似，但是增加了异常返回值； 123456789101112131415161718192021222324252627282930313233343536373839package com.kingh.thread.create;import java.util.concurrent.Callable;import java.util.concurrent.FutureTask;/** * 带返回值的方式 * * @author &lt;a href="https://blog.csdn.net/king_kgh&gt;Kingh&lt;/a&gt; * @version 1.0 * @date 2019/3/18 10:04 */public class CreateThreadDemo11_Callable &#123; public static void main(String[] args) throws Exception &#123; // 创建线程任务 Callable&lt;Integer&gt; call = () -&gt; &#123; System.out.println("线程任务开始执行了...."); Thread.sleep(2000); return 1; &#125;; // 将任务封装为FutureTask FutureTask&lt;Integer&gt; task = new FutureTask&lt;&gt;(call); // 开启线程，执行线程任务 new Thread(task).start(); // ==================== // 这里是在线程启动之后，线程结果返回之前 System.out.println("这里可以为所欲为...."); // ==================== // 为所欲为完毕之后，拿到线程的执行结果 Integer result = task.get(); System.out.println("主线程中拿到异步任务执行的结果为：" + result); &#125;&#125; 线程池 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.kingh.thread.create;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * 线程池 * * @author &lt;a href="https://blog.csdn.net/king_kgh&gt;Kingh&lt;/a&gt; * @version 1.0 * @date 2019/3/18 10:04 */public class CreateThreadDemo12_ThreadPool &#123; public static void main(String[] args) throws Exception &#123; // 创建固定大小的线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); while (true) &#123; // 提交多个线程任务，并执行 threadPool.execute(new Runnable() &#123; @Override public void run() &#123; printThreadInfo(); &#125; &#125;); &#125; &#125; /** * 输出当前线程的信息 */ private static void printThreadInfo() &#123; System.out.println("当前运行的线程名为： " + Thread.currentThread().getName()); try &#123; Thread.sleep(1000); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 线程之间的的调用并不是按照程序的顺序调用的,CPU以不确定的时间调用线程中的run方法。 start() 与run()线程有新建、就绪、执行、阻塞、结束等状态。new一个线程之后，线程就进入到新建状态，这时候线程还没有分配内存等资源。调用start()方法之后，线程就进入就绪状态，一旦分配到了CPU时间片，就会自动执行线程的run()方法，完成任务。 当线程直接调用run()，线程不会进入就绪状态，此时的状态就相当于main线程的普通方法，不会由原来的线程执行。 总结： 调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统——进程（1）]]></title>
    <url>%2F2019%2F06%2F10%2F201906100053-OS-Process-1%2F</url>
    <content type="text"><![CDATA[进程在操作系统中是一个非常重要的概念，这篇文章将要介绍一下进程。 在详细介绍之前，先考虑几个问题： 进程与线程的区别； 进程有哪几种状态，以及不同状态是怎么转换的； 进程与线程的通信； 进程同步的经典问题； 进程的定义程序是一个静态文件，本身并不能够执行。程序要想执行，计算机必须为它分配资源，包括内存、CPU等。在程序执行的过程中也需要维护他的状态。程序有顺序执行和并发执行两种状态。为了描述和控制程序的并发执行，引入进程的概念。 进程实体由程序段、相关数据段和进程控制块（PCB）组成。进程的实质就是进程实体的一次执行过程，是系统进行资源分配和调度的独立单位。 进程的状态 即将更新！！！]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java实现spark（1）]]></title>
    <url>%2F2019%2F06%2F09%2F201906092129-spark-java-1%2F</url>
    <content type="text"><![CDATA[用java实现简单的Spark例子。 数据很简单，是一个被预先处理的日志文件，包括时间、电话号，上行流量和下行流量。一行为一条记录，不同数据之间用制表符隔开。 样本类样本类是为了将日志文件的一条记录封装起来 123456789101112131415161718192021222324252627282930313233343536package com.icesun.java.accessLog;import java.io.Serializable;public class LogInfo implements Serializable &#123; private static final long serialVersionUID = 5749943279909593929L; private long timeStamp; private String phoneNo; private long down; private long up; LogInfo()&#123;&#125; LogInfo(long timeStamp, String phoneNo, long down, long up)&#123; this.timeStamp = timeStamp; this.phoneNo = phoneNo; this.down = down; this.up = up; &#125; public long getTimeStamp() &#123; return timeStamp; &#125; public String getPhoneNo() &#123; return phoneNo; &#125; public long getDown() &#123; return down; &#125; public long getUp() &#123; return up; &#125;&#125; Spark Core API123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.icesun.java.accessLog;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;public class LogSpark &#123; public static void main(String [] args)&#123; SparkConf conf = new SparkConf().setMaster("local").setAppName("AccessLog"); JavaSparkContext sc = new JavaSparkContext(conf); sc.setLogLevel("WARN"); String path = "files/access.log"; JavaRDD&lt;String&gt; lines = sc.textFile(path); JavaPairRDD&lt;String,LogInfo&gt; logPairRDD = RDD2RDDPair(lines); JavaPairRDD&lt;String,LogInfo&gt; reduceByKey = aggregateByDeviceID(logPairRDD); reduceByKey.foreach(x -&gt; System.out.println(x._2.getDown())); System.out.println(reduceByKey.count()); sc.close(); &#125; //实现strRDD到LogInfo RDD的转换 &lt;K,V&gt; 电话号为K，LogInfor为V private static JavaPairRDD&lt;String, LogInfo&gt; RDD2RDDPair(JavaRDD&lt;String&gt; accessLogRDD)&#123; return accessLogRDD.mapToPair((PairFunction&lt;String, String, LogInfo&gt;) line -&gt; &#123; String[] logInfo = line.split("\t"); long timeStamp = Long.valueOf(logInfo[0]); String phone = logInfo[1]; long down = Long.valueOf(logInfo[2]); long up = Long.valueOf(logInfo[2]); LogInfo log = new LogInfo(timeStamp, phone, down, up); return new Tuple2&lt;String, LogInfo&gt;(phone, log); &#125;); &#125; //实现reduceByKey 电话号为K，将上行流量和下行流量加和 private static JavaPairRDD&lt;String, LogInfo&gt; aggregateByDeviceID(JavaPairRDD&lt;String, LogInfo&gt; pairRDD)&#123; return pairRDD.reduceByKey((Function2&lt;LogInfo, LogInfo, LogInfo&gt;)(v1, v2) -&gt; &#123; //时间戳为最早的时间 long timeStamp = v1.getTimeStamp() &lt; v2.getTimeStamp() ? v1.getTimeStamp(): v2.getTimeStamp(); //上行流量和下行流量进行add long up = v1.getUp() + v2.getUp(); long down = v1.getDown() + v2.getDown(); String phone = v1.getPhoneNo(); return new LogInfo(timeStamp, phone, up, down); &#125; ); &#125;&#125; SparkSQL1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.icesun.java.accessLog;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.SQLContext;public class LogSparkSQL &#123; public static void main(String[] args)&#123; SparkConf conf = new SparkConf().setAppName("SparkSQL").setMaster("local"); JavaSparkContext sc = new JavaSparkContext(conf);// HiveConf hiveConf = new HiveConf(sc); SQLContext sqlContext = new SQLContext(sc); JavaRDD&lt;String&gt; lines = sc.textFile("files/access.log"); //将字符串转换成LogInfoRDD JavaRDD&lt;LogInfo&gt; logInfo = lines.map( line -&gt;&#123; String[] str = line.split("\t"); long timeStamp = Long.valueOf(str[0]); String phone = str[1]; long down = Long.valueOf(str[2]); long up = Long.valueOf(str[3]); LogInfo log = new LogInfo(timeStamp, phone, down, up); return log; &#125;); //将RDD转换成DataSet数据集 Dataset&lt;Row&gt; df = sqlContext.createDataFrame(logInfo, LogInfo.class); //在dataset数据集上进行查询操作 df.select("phoneNo", "down").where("up &gt; 50000").show(); //将df注册成临时视图，这样可以用SQL表达式进行查询操作 df.createOrReplaceTempView("log"); Dataset&lt;Row&gt; seleRs = sqlContext.sql("select * from log where up &gt; 50000 and down &lt; 10000"); seleRs.toJavaRDD().foreach(row -&gt; System.out.println(row.get(1))); &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——Spark(6)Spark RDD算子]]></title>
    <url>%2F2019%2F04%2F11%2F201904110935-spark-learning-6%2F</url>
    <content type="text"><![CDATA[这里，从源码的角度总结一下Spark RDD算子的用法。 单值型Transformation算子map1234567/** * Return a new RDD by applying a function to all elements of this RDD. */def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))&#125; 源码中有一个 sc.clean() 函数，它的所用是去除闭包中不能序列话的外部引用变量。Scala支持闭包，闭包会把它对外的引用(闭包里面引用了闭包外面的对像)保存到自己内部，这个闭包就可以被单独使用了，而不用担心它脱离了当前的作用域；但是在spark这种分布式环境里，这种作法会带来问题，如果对外部的引用是不可serializable的，它就不能正确被发送到worker节点上去了；还有一些引用，可能根本没有用到，这些没有使用到的引用是不需要被发到worker上的； 实际上sc.clean函数调用的是ClosureCleaner.clean()；ClosureCleaner.clean()通过递归遍历闭包里面的引用，检查不能serializable的, 去除unused的引用； map函数是一个粗粒度的操作，对于一个RDD来说，会使用迭代器对分区进行遍历，然后针对一个分区使用你想要执行的操作f, 然后返回一个新的RDD。其实可以理解为rdd的每一个元素都会执行同样的操作。12345678910111213141516171819scala&gt; val array = Array(1,2,3,4,5,6)array: Array[Int] = Array(1, 2, 3, 4, 5, 6)scala&gt; val rdd = sc.appappName applicationAttemptId applicationIdscala&gt; val rdd = sc.parallelize(array, 2)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:26scala&gt; val mapRdd = rdd.map(x =&gt; x * 2) mapRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at &lt;console&gt;:25scala&gt; mapRdd.collect().foreach(println)24681012 flatMapflatMap方法与map方法类似，但是允许一次map方法中输出多个对象，而不是map中的一个对象经过函数转换成另一个对象。12345678/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))&#125; 12345scala&gt; val a = sc.parallelize(1 to 10, 5)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24scala&gt; a.flatMap(num =&gt; 1 to num).collectres1: Array[Int] = Array(1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10) mapPartitionsmapPartitions是map的另一个实现，map的输入函数应用与RDD的每个元素，但是mapPartitions的输入函数作用于每个分区，也就是每个分区的内容作为整体。123456789101112131415/** * Return a new RDD by applying a function to each partition of this RDD. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */ def mapPartitions[U: ClassTag]( f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope &#123; val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(iter), preservesPartitioning) &#125; 123456789101112131415161718192021scala&gt; def myfunc[T](iter: Iterator[T]):Iterator[(T,T)]=&#123; | var res = List[(T,T)]() | var pre = iter.next | while(iter.hasNext)&#123; | var cur = iter.next | res .::= (pre, cur) | pre = cur | &#125; | res.iterator | &#125;myfunc: [T](iter: Iterator[T])Iterator[(T, T)]scala&gt; val a = sc.parallelize(1 to 9,3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; a.mapPartitionsmapPartitions mapPartitionsWithIndexscala&gt; a.mapPartitions(myfunc).collectres0: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8)) mapPartitionWithIndexmapPartitionWithIndex方法与mapPartitions方法类似，不同的是mapPartitionWithIndex会对原始分区的索引进行追踪，这样就可以知道分区所对应的元素，方法的参数为一个函数，函数的输入为整型索引和迭代器。12345678910111213141516/** * Return a new RDD by applying a function to each partition of this RDD, while tracking the index * of the original partition. * * `preservesPartitioning` indicates whether the input function preserves the partitioner, which * should be `false` unless this is a pair RDD and the input function doesn't modify the keys. */def mapPartitionsWithIndex[U: ClassTag]( f: (Int, Iterator[T]) =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = withScope &#123; val cleanedF = sc.clean(f) new MapPartitionsRDD( this, (context: TaskContext, index: Int, iter: Iterator[T]) =&gt; cleanedF(index, iter), preservesPartitioning)&#125; 12345678910111213scala&gt; val x = sc.parallelize(1 to 10, 3)x: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24scala&gt; def myFunc(index:Int, iter:Iterator[Int]):Iterator[String]=&#123; | iter.toList.map(x =&gt; index + "," + x).iterator | &#125;myFunc: (index: Int, iter: Iterator[Int])Iterator[String]scala&gt; x.mapPartitionsmapPartitions mapPartitionsWithIndexscala&gt; x.mapPartitionsWithIndex(myFunc).collectres1: Array[String] = Array(0,1, 0,2, 0,3, 1,4, 1,5, 1,6, 2,7, 2,8, 2,9, 2,10) foreachforeach主要对每一个输入的数据对象执行循环操作，可以用来执行对RDD元素的输出操作。1234567/** * Applies a function f to all elements of this RDD. */def foreach(f: T =&gt; Unit): Unit = withScope &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))&#125; 12345scala&gt; var x = sc.parallelize(List(1 to 9), 3)x: org.apache.spark.rdd.RDD[scala.collection.immutable.Range.Inclusive] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:24scala&gt; x.foreach(print)Range(1, 2, 3, 4, 5, 6, 7, 8, 9) foreachPartitionforeachPartition方法和mapPartition的作用一样，通过迭代器参数对RDD中每一个分区的数据对象应用函数，区别在于使用的参数是否有返回值。1234567/** * Applies a function f to each partition of this RDD. */def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withScope &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter))&#125; 1234567scala&gt; val b = sc.parallelize(List(1,2,3,4,5,6), 3)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24scala&gt; b.foreachPartition(x =&gt; println(x.reduce((a,b) =&gt; a +b)))7311 glomglom的作用与collec类似，collect是将RDD直接转化为数组的形式，而glom则是将RDD分区数据组装到数组类型的RDD中，每一个返回的数组包含一个分区的所有元素，按分区转化为数组，有几个分区就返回几个数组类型的RDD。1234567/** * Return an RDD created by coalescing all elements within each partition into an array. */ def glom(): RDD[Array[T]] = withScope &#123; new MapPartitionsRDD[Array[T], T](this, (context, pid, iter) =&gt; Iterator(iter.toArray))&#125; 下面的例子中，RDD a有三个分区，glom将a转化为由三个数组构成的RDD。12345678scala&gt; val a = sc.parallelize(1 to 9, 3)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at &lt;console&gt;:24scala&gt; a.glom.collectres5: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9))scala&gt; a.glomres6: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[4] at glom at &lt;console&gt;:26 unionunion方法与++方法是等价的，将两个RDD去并集，取并集的过程中不会去重。12345678910111213141516/** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */def union(other: RDD[T]): RDD[T] = withScope &#123; sc.union(this, other)&#125;/** * Return the union of this RDD and another one. Any identical elements will appear multiple * times (use `.distinct()` to eliminate them). */def ++(other: RDD[T]): RDD[T] = withScope &#123; this.union(other)&#125; 1234567891011scala&gt; val a = sc.parallelize(1 to 4,2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:24scala&gt; val b = sc.parallelize(2 to 5,1)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24scala&gt; a.ununion unpersistscala&gt; a.union(b).collectres7: Array[Int] = Array(1, 2, 3, 4, 2, 3, 4, 5) cartesian计算两个RDD中每个对象的笛卡尔积1234567/** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. */def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope &#123; new CartesianRDD(sc, this, other)&#125; 12345678cala&gt; val a = sc.parallelize(1 to 4,2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:24scala&gt; val b = sc.parallelize(2 to 5,1)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24scala&gt; a.cartesian(b).collectres8: Array[(Int, Int)] = Array((1,2), (1,3), (1,4), (1,5), (2,2), (2,3), (2,4), (2,5), (3,2), (3,3), (3,4), (3,5), (4,2), (4,3), (4,4), (4,5)) groupBygroupBy方法有三个重载方法，功能是讲元素通过map函数生成Key-Value格式，然后使用groupByKey方法对Key-Value进行聚合。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope &#123; groupBy[K](f, defaultPartitioner(this)) &#125; /** * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K]( f: T =&gt; K, numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope &#123; groupBy(f, new HashPartitioner(numPartitions)) &#125; /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. */ def groupBy[K](f: T =&gt; K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])] = withScope &#123; val cleanF = sc.clean(f) this.map(t =&gt; (cleanF(t), t)).groupByKey(p) &#125; 12345678910111213141516scala&gt; val a = sc.parallelize(1 to 9,2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at &lt;console&gt;:24scala&gt; a.groupBy(x =&gt; &#123;if(x % 2 == 0) "even" else "odd"&#125;).collectres9: Array[(String, Iterable[Int])] = Array((even,CompactBuffer(2, 4, 6, 8)), (odd,CompactBuffer(1, 3, 5, 7, 9)))scala&gt; def myfunc(a: Int):Int=&#123; | a % 2 | &#125;myfunc: (a: Int)Intscala&gt; a.groupBy(myfunc).collectres10: Array[(Int, Iterable[Int])] = Array((0,CompactBuffer(2, 4, 6, 8)), (1,CompactBuffer(1, 3, 5, 7, 9)))scala&gt; a.groupBy(myfunc(_), 1).collectres11: Array[(Int, Iterable[Int])] = Array((0,CompactBuffer(2, 4, 6, 8)), (1,CompactBuffer(1, 3, 5, 7, 9))) filterfilter方法对输入元素进行过滤，参数是一个返回值为boolean的函数，如果函数对元素的运算结果为true，则通过元素，否则就将该元素过滤，不进入结果集。12345678910/** * Return a new RDD containing only the elements that satisfy a predicate. */def filter(f: T =&gt; Boolean): RDD[T] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (context, pid, iter) =&gt; iter.filter(cleanF), preservesPartitioning = true)&#125; 1234567891011scala&gt; val a = sc.parallelize(List("we", "are", "from", "China", "not", "from", "America"))a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[16] at parallelize at &lt;console&gt;:24scala&gt; val b = a.filter(x =&gt; x.length &gt;= 4)b: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[17] at filter at &lt;console&gt;:25scala&gt; b.collect.foreach(println)fromChinafromAmerica distinctdistinct方法将RDD中重复的元素去掉，只留下唯一的RDD元素。123456/** * Return a new RDD containing the distinct elements in this RDD. */def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123; map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)&#125; 123456789101112131415scala&gt; val a = sc.parallelize(List("we", "are", "from", "China", "not", "from", "America"))a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[18] at parallelize at &lt;console&gt;:24scala&gt; val b = a.map(x =&gt; x.length)b: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[19] at map at &lt;console&gt;:25scala&gt; val c = b.distinctc: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[22] at distinct at &lt;console&gt;:25scala&gt; c.foreach(println)54237 subtractsubtract方法就是求集合A-B，即把集合A中包含集合B的元素都删除，返回剩下的元素。123456789101112131415161718192021222324252627282930313233343536373839/** * Return an RDD with the elements from `this` that are not in `other`. * * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting * RDD will be &amp;lt;= us. */ def subtract(other: RDD[T]): RDD[T] = withScope &#123; subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length))) &#125; /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope &#123; subtract(other, new HashPartitioner(numPartitions)) &#125; /** * Return an RDD with the elements from `this` that are not in `other`. */ def subtract( other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123; if (partitioner == Some(p)) &#123; // Our partitioner knows how to handle T (which, since we have a partitioner, is // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples val p2 = new Partitioner() &#123; override def numPartitions: Int = p.numPartitions override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1) &#125; // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies // anyway, and when calling .keys, will not have a partitioner set, even though // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be // partitioned by the right/real keys (e.g. p). this.map(x =&gt; (x, null)).subtractByKey(other.map((_, null)), p2).keys &#125; else &#123; this.map(x =&gt; (x, null)).subtractByKey(other.map((_, null)), p).keys &#125; &#125; 1234567891011scala&gt; val a = sc.parallelize(1 to 9, 2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24scala&gt; val b = sc.parallelize(2 to 5, 4)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[24] at parallelize at &lt;console&gt;:24scala&gt; val c = a.subtract(b)c: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[28] at subtract at &lt;console&gt;:27scala&gt; c.collectres14: Array[Int] = Array(6, 8, 1, 7, 9) persist与cachecache,缓存数据，把RDD缓存到内存中，以便下次计算式再次被调用。persist是把RDD根据不同的级别进行持久化，通过参数指定持久化级别，如果不带参数则为默认持久化级别，即只保存到内存中，与Cache等价。 samplesample方法的作用是随即对RDD中的元素进行采样，或得一个新的子RDD。根据参数制定是否放回采样，子集占总数的百分比和随机种子。1234567891011121314151617181920212223242526272829/** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD's size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be greater * than or equal to 0 * @param seed seed for the random number generator * * @note This is NOT guaranteed to provide exactly the fraction of the count * of the given [[RDD]]. */ def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = &#123; require(fraction &gt;= 0, s"Fraction must be nonnegative, but got $&#123;fraction&#125;") withScope &#123; require(fraction &gt;= 0.0, "Negative fraction value: " + fraction) if (withReplacement) &#123; new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) &#125; else &#123; new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) &#125; &#125; &#125; 123456789101112131415161718192021222324252627scala&gt; val a = sc.parallelize(1 to 100, 2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at &lt;console&gt;:24scala&gt; val b = a.sample(false, 0.2, 0)b: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[32] at sample at &lt;console&gt;:25scala&gt; b.foreach(println)519202627293057406145687350757981858999 键值对型transformation算子groupByKey类似于groupBy，将每一个相同的Key的Value聚集起来形成序列，可以使用默认的分区器和自定义的分区器。1234567891011121314151617181920212223242526272829303132333435363738394041/** * Group the values for each key in the RDD into a single sequence. Allows controlling the * partitioning of the resulting key-value pair RDD by passing a Partitioner. * The ordering of elements within each group is not guaranteed, and may even differ * each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. * * @note As currently implemented, groupByKey must be able to hold all the key-value pairs for any * key in memory. If a key has too many values, it can result in an `OutOfMemoryError`. */def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope &#123; // groupByKey shouldn't use map side combine because map side combine does not // reduce the amount of data shuffled and requires all map side data be inserted // into a hash table, leading to more objects in the old gen. val createCombiner = (v: V) =&gt; CompactBuffer(v) val mergeValue = (buf: CompactBuffer[V], v: V) =&gt; buf += v val mergeCombiners = (c1: CompactBuffer[V], c2: CompactBuffer[V]) =&gt; c1 ++= c2 val bufs = combineByKeyWithClassTag[CompactBuffer[V]]( createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine = false) bufs.asInstanceOf[RDD[(K, Iterable[V])]]&#125;/** * Group the values for each key in the RDD into a single sequence. Hash-partitions the * resulting RDD with into `numPartitions` partitions. The ordering of elements within * each group is not guaranteed, and may even differ each time the resulting RDD is evaluated. * * @note This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey` * or `PairRDDFunctions.reduceByKey` will provide much better performance. * * @note As currently implemented, groupByKey must be able to hold all the key-value pairs for any * key in memory. If a key has too many values, it can result in an `OutOfMemoryError`. */def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])] = self.withScope &#123; groupByKey(new HashPartitioner(numPartitions))&#125; 12345678scala&gt; val a = sc.parallelize(List("mk", "zq", "xwc", "fig", "dcp", "snn"), 2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24scala&gt; val b = a.keyBy(x =&gt; x.length)b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[34] at keyBy at &lt;console&gt;:25scala&gt; b.groupByKey.collectres17: Array[(Int, Iterable[String])] = Array((2,CompactBuffer(mk, zq)), (3,CompactBuffer(xwc, fig, dcp, snn))) combineByKeycomineByKey方法能够有效地讲键值对形式的RDD相同的Key的Value合并成序列形式，用户能自定义RDD的分区器和是否在Map端进行聚合操作。1234567891011121314151617181920212223242526272829303132/** * Generic function to combine the elements for each key using a custom set of aggregation * functions. This method is here for backward compatibility. It does not provide combiner * classtag information to the shuffle. * * @see `combineByKeyWithClassTag` */def combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)] = self.withScope &#123; combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners, partitioner, mapSideCombine, serializer)(null)&#125;/** * Simplified version of combineByKeyWithClassTag that hash-partitions the output RDD. * This method is here for backward compatibility. It does not provide combiner * classtag information to the shuffle. * * @see `combineByKeyWithClassTag` */def combineByKey[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, numPartitions: Int): RDD[(K, C)] = self.withScope &#123; combineByKeyWithClassTag(createCombiner, mergeValue, mergeCombiners, numPartitions)(null)&#125; 123456789101112131415scala&gt; val a = sc.parallelize(List("xwc", "fig","wc", "dcp", "zq", "znn", "mk", "zl", "hk", "lp"), 2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[36] at parallelize at &lt;console&gt;:24scala&gt; val b = sc.parallelize(List(1,2,2,3,2,1,2,2,2,3),2)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24scala&gt; val c = b.zip(a)c: org.apache.spark.rdd.RDD[(Int, String)] = ZippedPartitionsRDD2[38] at zip at &lt;console&gt;:27scala&gt; val d = c.combineByKey(List(_), (x:List[String], y:String)=&gt;y::x, (x:List[String], y:List[String])=&gt;x::: y)d: org.apache.spark.rdd.RDD[(Int, List[String])] = ShuffledRDD[39] at combineByKey at &lt;console&gt;:25scala&gt; d.collectres18: Array[(Int, List[String])] = Array((2,List(zq, wc, fig, hk, zl, mk)), (1,List(xwc, znn)), (3,List(dcp, lp))) 上面的例子使用三个参数重载的方法，该方法的第一个参数createCombiner把元素V转换成另一类元素C，该例子中使用的参数是List(_),表示将输入元素放在List集合中；第二个参数mergeValue的含义是吧元素V合并到元素C中，该例子中使用的是(x:List[String],y:String)=&gt;y::x,表示将y字符合并到x链表集合中；第三个参数的含义是讲两个C元素合并，该例子中使用的是（x:List[String], y:List[String]）=&gt;x:::y, 表示把x链表集合中的内容合并到y链表中。 reduceByKey使用一个reduce函数来实现对想要的Key的value的聚合操作，发送给reduce前会在map端本地merge操作，该方法的底层实现是调用combineByKey方法的一个重载方法。123456789101112131415161718192021222324252627/** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a "combiner" in MapReduce. */ def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123; combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner) &#125; /** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a "combiner" in MapReduce. Output will be hash-partitioned with numPartitions partitions. */ def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)] = self.withScope &#123; reduceByKey(new HashPartitioner(numPartitions), func) &#125; /** * Merge the values for each key using an associative and commutative reduce function. This will * also perform the merging locally on each mapper before sending results to a reducer, similarly * to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/ * parallelism level. */ def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123; reduceByKey(defaultPartitioner(self), func) &#125; 12345678scala&gt; val a = sc.parallelize(List("dcp","fjg","snn","wc", "za"), 2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; val b = a.map(x =&gt; (x.length,x))b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[2] at map at &lt;console&gt;:25scala&gt; b.reduceByKey((a, b) =&gt; a + b ).collectres1: Array[(Int, String)] = Array((2,wcza), (3,dcpfjgsnn)) sortByKey根据Key值对键值对进行排序，如果是字符，则按照字典顺序排序，如果是数组则按照数字大小排序，可通过参数指定升序还是降序。1234567891011scala&gt; val a = sc.parallelize(List("dcp","fjg","snn","wc", "za"), 2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at &lt;console&gt;:24scala&gt; val b = sc.parallelize(1 to a.count.toInt,2)b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at &lt;console&gt;:26scala&gt; val c = a.zip(b)c: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[6] at zip at &lt;console&gt;:27scala&gt; c.sortByKey(true).collectres2: Array[(String, Int)] = Array((dcp,1), (fjg,2), (snn,3), (wc,4), (za,5)) cogroup1234567891011121314151617181920212223242526scala&gt; val a = sc.parallelize(List(1,2,2,3,1,3),2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at &lt;console&gt;:24scala&gt; val b = a.map(x =&gt; (x, &quot;b&quot;))b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[11] at map at &lt;console&gt;:25scala&gt; val c = a.map(x =&gt; (x, &quot;c&quot;))c: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[12] at map at &lt;console&gt;:25scala&gt; b.cogroup(c).collectres3: Array[(Int, (Iterable[String], Iterable[String]))] = Array((2,(CompactBuffer(b, b),CompactBuffer(c, c))), (1,(CompactBuffer(b, b),CompactBuffer(c, c))), (3,(CompactBuffer(b, b),CompactBuffer(c, c))))scala&gt; val a = sc.parallelize(List(1,2,2,2,1,3),1)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[15] at parallelize at &lt;console&gt;:24scala&gt; val b = a.map(x =&gt; (x, &quot;b&quot;))b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[16] at map at &lt;console&gt;:25scala&gt; val c = a.map(x =&gt; (x, &quot;c&quot;))c: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[17] at map at &lt;console&gt;:25scala&gt; b.cogroup(c).collectres4: Array[(Int, (Iterable[String], Iterable[String]))] = Array((1,(CompactBuffer(b, b),CompactBuffer(c, c))), (3,(CompactBuffer(b),CompactBuffer(c))), (2,(CompactBuffer(b, b, b),CompactBuffer(c, c, c)))) join首先对RDD进行cogroup操作，然后对每个新的RDD下Key的值进行笛卡尔积操作，再返回结果使用flatmapValue方法。1234567891011121314scala&gt; val a= sc.parallelize(List(&quot;fjg&quot;,&quot;wc&quot;,&quot;xwc&quot;),2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[20] at parallelize at &lt;console&gt;:24 scala&gt; val c = sc.parallelize(List(&quot;fig&quot;, &quot;wc&quot;, &quot;sbb&quot;, &quot;zq&quot;,&quot;xwc&quot;,&quot;dcp&quot;), 2)c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[22] at parallelize at &lt;console&gt;:24scala&gt; val d = c.keyBy(_.length)d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[23] at keyBy at &lt;console&gt;:25scala&gt; val b = a.keyBy(_.length)b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[24] at keyBy at &lt;console&gt;:25scala&gt; b.join(d).collectres6: Array[(Int, (String, String))] = Array((2,(wc,wc)), (2,(wc,zq)), (3,(fjg,fig)), (3,(fjg,sbb)), (3,(fjg,xwc)), (3,(fjg,dcp)), (3,(xwc,fig)), (3,(xwc,sbb)), (3,(xwc,xwc)), (3,(xwc,dcp))) Action算子collect把RDD中的元素以数组的形式返回。 12345678910/** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */def collect(): Array[T] = withScope &#123; val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray) Array.concat(results: _*)&#125; 12345scala&gt; val a = sc.parallelize(List("a", "b", "c"),2)a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[28] at parallelize at &lt;console&gt;:24scala&gt; a.collectres7: Array[String] = Array(a, b, c) reduce使用一个带两个参数的函数把元素进行聚集，返回一个元素的结果。该函数中的二元操作应该满足交换律和结合律，这样才能在并行计算中得到正确的计算结果。1234567891011121314151617181920212223242526/** * Reduces the elements of this RDD using the specified commutative and * associative binary operator. */def reduce(f: (T, T) =&gt; T): T = withScope &#123; val cleanF = sc.clean(f) val reducePartition: Iterator[T] =&gt; Option[T] = iter =&gt; &#123; if (iter.hasNext) &#123; Some(iter.reduceLeft(cleanF)) &#125; else &#123; None &#125; &#125; var jobResult: Option[T] = None val mergeResult = (index: Int, taskResult: Option[T]) =&gt; &#123; if (taskResult.isDefined) &#123; jobResult = jobResult match &#123; case Some(value) =&gt; Some(f(value, taskResult.get)) case None =&gt; taskResult &#125; &#125; &#125; sc.runJob(this, reducePartition, mergeResult) // Get the final result out of our Option, or throw an exception if the RDD was empty jobResult.getOrElse(throw new UnsupportedOperationException("empty collection"))&#125; 12345scala&gt; val a = sc.parallelize(1 to 10, 2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[29] at parallelize at &lt;console&gt;:24scala&gt; a.reduce((a, b) =&gt; a + b)res8: Int = 55 taketake方法会从RDD中取出前n个元素。先扫描一个分区，之后从分区中得到结果，然后评估该分区的元素是否满足n，若果不满足则继续从其他分区中扫描获取。12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Take the first num elements of the RDD. It works by first scanning one partition, and use the * results from that partition to estimate the number of additional partitions needed to satisfy * the limit. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @note Due to complications in the internal implementation, this method will raise * an exception if called on an RDD of `Nothing` or `Null`. */ def take(num: Int): Array[T] = withScope &#123; val scaleUpFactor = Math.max(conf.getInt("spark.rdd.limit.scaleUpFactor", 4), 2) if (num == 0) &#123; new Array[T](0) &#125; else &#123; val buf = new ArrayBuffer[T] val totalParts = this.partitions.length var partsScanned = 0 while (buf.size &lt; num &amp;&amp; partsScanned &lt; totalParts) &#123; // The number of partitions to try in this iteration. It is ok for this number to be // greater than totalParts because we actually cap it at totalParts in runJob. var numPartsToTry = 1L val left = num - buf.size if (partsScanned &gt; 0) &#123; // If we didn't find any rows after the previous iteration, quadruple and retry. // Otherwise, interpolate the number of partitions we need to try, but overestimate // it by 50%. We also cap the estimation in the end. if (buf.isEmpty) &#123; numPartsToTry = partsScanned * scaleUpFactor &#125; else &#123; // As left &gt; 0, numPartsToTry is always &gt;= 1 numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor) &#125; &#125; val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt) val res = sc.runJob(this, (it: Iterator[T]) =&gt; it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += p.size &#125; buf.toArray &#125; &#125; 12345scala&gt; val a = sc.parallelize(1 to 10, 2)a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[30] at parallelize at &lt;console&gt;:24scala&gt; a.take(5)res9: Array[Int] = Array(1, 2, 3, 4, 5) toptop会采用隐式排序转换来获取最大的前n个元素。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Returns the top k (largest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of * [[takeOrdered]]. For example: * &#123;&#123;&#123; * sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1) * // returns Array(12) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2) * // returns Array(6, 5) * &#125;&#125;&#125; * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of top elements to return * @param ord the implicit ordering for T * @return an array of top elements */def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope &#123; takeOrdered(num)(ord.reverse)&#125;/** * Returns the first k (smallest) elements from this RDD as defined by the specified * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]]. * For example: * &#123;&#123;&#123; * sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1) * // returns Array(2) * * sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2) * // returns Array(2, 3) * &#125;&#125;&#125; * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. * * @param num k, the number of elements to return * @param ord the implicit ordering for T * @return an array of top elements */def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope &#123; if (num == 0) &#123; Array.empty &#125; else &#123; val mapRDDs = mapPartitions &#123; items =&gt; // Priority keeps the largest elements, so let's reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= collectionUtils.takeOrdered(items, num)(ord) Iterator.single(queue) &#125; if (mapRDDs.partitions.length == 0) &#123; Array.empty &#125; else &#123; mapRDDs.reduce &#123; (queue1, queue2) =&gt; queue1 ++= queue2 queue1 &#125;.toArray.sorted(ord) &#125; &#125;&#125; 12345scala&gt; val c = sc.parallelize(Array(1,2,3,5,3,8,7,97,32),2)c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at &lt;console&gt;:24scala&gt; c.top(3)res10: Array[Int] = Array(97, 32, 8) countcount方法计算返回RDD中元素的个数。1234/** * Return the number of elements in the RDD. */def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum 12345scala&gt; val c = sc.parallelize(Array(1,2,3,5,3,8,7,97,32),2)c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24scala&gt; c.countres11: Long = 9 takeSample返回一个固定大小的数组形式的采样子集，此外还会把返回元素的顺序随机打乱。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Return a fixed-size sampled subset of this RDD in an array * * @param withReplacement whether sampling is done with replacement * @param num size of the returned sample * @param seed seed for the random number generator * @return sample of specified size in an array * * @note this method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def takeSample( withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T] = withScope &#123; val numStDev = 10.0 require(num &gt;= 0, "Negative number of elements requested") require(num &lt;= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt), "Cannot support a sample size &gt; Int.MaxValue - " + s"$numStDev * math.sqrt(Int.MaxValue)") if (num == 0) &#123; new Array[T](0) &#125; else &#123; val initialCount = this.count() if (initialCount == 0) &#123; new Array[T](0) &#125; else &#123; val rand = new Random(seed) if (!withReplacement &amp;&amp; num &gt;= initialCount) &#123; Utils.randomizeInPlace(this.collect(), rand) &#125; else &#123; val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount, withReplacement) var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() // If the first sample didn't turn out large enough, keep trying to take samples; // this shouldn't happen often because we use a big multiplier for the initial size var numIters = 0 while (samples.length &lt; num) &#123; logWarning(s"Needed to re-sample due to insufficient sample size. Repeat #$numIters") samples = this.sample(withReplacement, fraction, rand.nextInt()).collect() numIters += 1 &#125; Utils.randomizeInPlace(samples, rand).take(num) &#125; &#125; &#125; &#125; 12345scala&gt; val c = sc.parallelize(Array(1,2,3,5,3,8,7,97,32),2)c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[34] at parallelize at &lt;console&gt;:24scala&gt; c.takeSample(true,3, 1)res14: Array[Int] = Array(1, 3, 7) saveAsTextFile将RDD存储为文本文件，一次存一行 countByKey类似count，但是countByKey会根据Key计算对应的Value个数，返回Map类型的结果。1234567891011/** * Count the number of elements for each key, collecting the results to a local Map. * * @note This method should only be used if the resulting map is expected to be small, as * the whole thing is loaded into the driver's memory. * To handle very large results, consider using rdd.mapValues(_ =&gt; 1L).reduceByKey(_ + _), which * returns an RDD[T, Long] instead of a map. */def countByKey(): Map[K, Long] = self.withScope &#123; self.mapValues(_ =&gt; 1L).reduceByKey(_ + _).collect().toMap&#125; 12345678scala&gt; val c = sc.parallelize(List("fig", "wc", "sbb", "zq","xwc","dcp"), 2)c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[36] at parallelize at &lt;console&gt;:24scala&gt; val d = c.keyBy(_.length)d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[37] at keyBy at &lt;console&gt;:25scala&gt; d.countByKeyres15: scala.collection.Map[Int,Long] = Map(2 -&gt; 2, 3 -&gt; 4) aggregate12345678910111213141516171819202122232425/** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral "zero value". This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. * * @param zeroValue the initial value for the accumulated result of each partition for the * `seqOp` operator, and also the initial value for the combine results from * different partitions for the `combOp` operator - this will typically be the * neutral element (e.g. `Nil` for list concatenation or `0` for summation) * @param seqOp an operator used to accumulate results within a partition * @param combOp an associative operator used to combine results from different partitions */def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U = withScope &#123; // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) =&gt; it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (index: Int, taskResult: U) =&gt; jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult&#125; fold1234567891011121314151617181920212223242526272829/** * Aggregate the elements of each partition, and then the results for all the partitions, using a * given associative function and a neutral "zero value". The function * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object * allocation; however, it should not modify t2. * * This behaves somewhat differently from fold operations implemented for non-distributed * collections in functional languages like Scala. This fold operation may be applied to * partitions individually, and then fold those results into the final result, rather than * apply the fold to each element sequentially in some defined ordering. For functions * that are not commutative, the result may differ from that of a fold applied to a * non-distributed collection. * * @param zeroValue the initial value for the accumulated result of each partition for the `op` * operator, and also the initial value for the combine results from different * partitions for the `op` operator - this will typically be the neutral * element (e.g. `Nil` for list concatenation or `0` for summation) * @param op an operator used to both accumulate results within a partition and combine results * from different partitions */ def fold(zeroValue: T)(op: (T, T) =&gt; T): T = withScope &#123; // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance()) val cleanOp = sc.clean(op) val foldPartition = (iter: Iterator[T]) =&gt; iter.fold(zeroValue)(cleanOp) val mergeResult = (index: Int, taskResult: T) =&gt; jobResult = op(jobResult, taskResult) sc.runJob(this, foldPartition, mergeResult) jobResult &#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——Spark(5) Spark 集群资源调度]]></title>
    <url>%2F2019%2F04%2F10%2F201904101128-spark-learning-5%2F</url>
    <content type="text"><![CDATA[Spark的集群调度其实蛮简单的，这里总结了一些集群资源调度的基本原理。 资源注册spark集群是怎么管理集群的计算资源和内存资源的呢？其实在这背后有一个资源注册的机制。简单来说，就是Worker节点讲自己的资源报告给master。 首先，在Worker节点启动的时候，Worker进程对应的worker类会实现一个WorkerArguments类从配置文件中读取Worker进程相关的配置信息，或者使用默认值，包括分配给Worker的核心数和内存容量。如果没有在配置文件配置这些资源，那么则采取默认的值，可有inferDefaultCores方法和inferDefaultMemory方法得到。如果配置文件中配置了，那么则采用SPARK_WORKER_CORES和SPARK_WORKER_MEMORY设置的值。如果再启动Worker的时候，显式地指定了参数–core(–c)或者–memory(–m)，则采用显式制定的值。 然后，Worker需要向Master进程注册，汇报Worker拥有的计算资源和内存容量。在实现的时候，调用Worker类中的tryRegisterAllMaster方法，通过Akka通信机制向Master节点发送注册消息RegisterWorker。包括Worker的节点编号，主机地址，端口，用户分配的内存以及核心数等信息。 最后，Master节点将Worker发送来的信息进行记录，存储在WorkerInfo对象中。 以上就是Worker资源注册的过程。 资源申请和分配在集群环境下，Driver程序通过实例化CoarseGrainedSchedulerBackend类实现Driver程序与集群之间的通信。不同的集群模式实例化不同的CoarseGrainedSchedulerBackend子类。对于Standalone来说，使用的是SparkDeploySchedulerBackend类，该类在启动的时候会实例化一个APPClient类，APPClient会想Master节点发送应用注册请求，注册请求含有应用需要的资源情况。 Master节点在接收到应用程序的注册请求之后，会把应用放在等待队列中，并调用Master.scheduler方法，这个方法对应的是Master进程的驱动程序/应用程序调度逻辑：首先对Driver进程调度，在YARN集群模式下，Driver程序可以运行在worker节点上，因此Master节点需要专门分配相应的集群资源来运行Driver进程，采用的方法就是随机把Driver分配到空闲的Worker上。 具体的应用程序的调度，多个应用程序之间采用FIFO调度方法，按照应用程序注册的先后顺序分配资源。对于单个应用程序之间，则有SpreadOut和非SpreadOut两种调度策略。在分配资源之前，Master需要查询当前集群的内存资源是否满足运行应用程序最低的需求，并且之前为这个应用程序分配过Executor的Worker不能参与资源调度，一个应用程序在一个Worker上只能有一个Executor。 SpreadOut策略，Master采用轮询的方式，让每个可用的Worker为应用程序分配一个核心，知道分配的核心满足应用程序的需求。非SpreadOut策略，会一次性把一个Worker上所有可分配的核心全部分配给应用程序。 被分配任务的Worker会调用addExecutor函数来添加相应的Executor，并调用launchExecutor方法启动Executor，改方法会记录Worker被消耗的资源，并向对应的Worker发送消息，通知其启动Executor。Worker接收到消息之后，会记录资源消耗量，并启动新的Executor进程。 上图是YARN集群模式下资源调度的流程图，根据这幅图，我们也可以了解资源调度的情况。提交一个Spark应用程序，首先通过Client向ResourceManager请求启动一个Application，同时检查是否有足够的资源满足Application的需求，如果资源条件满足，则准备ApplicationMaster的启动上下文，交给ResourceManager，并循环监控Application状态。 当提交的资源队列中有资源时，ResourceManager会在某个NodeManager上启动ApplicationMaster进程，ApplicationMaster会单独启动Driver后台线程，当Driver启动后，ApplicationMaster会通过本地的RPC连接Driver，并开始向ResourceManager申请Container资源运行Executor进程（一个Executor对应与一个Container），当ResourceManager返回Container资源，则在对应的Container上启动Executor。 Driver线程主要是初始化SparkContext对象，准备运行所需的上下文，然后一方面保持与ApplicationMaster的RPC连接，通过ApplicationMaster申请资源，另一方面根据用户业务逻辑开始调度任务，将任务下发到已有的空闲Executor上。 当ResourceManager向ApplicationMaster返回Container资源时，ApplicationMaster就尝试在对应的Container上启动Executor进程，Executor进程起来后，会向Driver注册，注册成功后保持与Driver的心跳，同时等待Driver分发任务，当分发的任务执行完毕后，将任务状态上报给Driver。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——Spark(4) Spark调度机制(1)]]></title>
    <url>%2F2019%2F04%2F09%2F201904091637-spark-learning-4%2F</url>
    <content type="text"><![CDATA[Spark的调度机制是Spark的核心基础，分成两个部分，一是集群资源调度，二是应用程序内部的作业调度。这篇文章简单介绍一下基本概念。 基本概念 应用程序(Application) 每次通过spark-submit命令提交的Jar包都可以看作是Spark应用程序。Spark应用程序是集群管理器调度的基本单位，一个应用程序对应着一个驱动程序。 驱动程序(Driver Program) 包含在main函数并且在函数内部实例化SparkContext对象的应用程序叫做驱动程序。Driver程序所运行的机器叫做客户端(Clien)，而客户端可以使Master节点，也可以是Worker节点，也有可能两个都不是。Spark中作业调度的相关工作，包括作业任务划分为阶段，阶段在进一步分割成多个任务等等都是在Driver程序中执行的。作业调度完毕之后，Driver程序会把实际的并行计算任务交付给Executor去执行。 Master运行Master守护进程的集群节点，管理这整个集群。一个集群中Master节点可以有多个，但是一次只有一个活跃状态的Master，其余的都是备用节点。 Master节点有两个用途：一是应用程序调度，Master接收用户提交的应用程序，并将应用程序交付执行，当有多个程序调试，会对这些应用程序执行的先后顺序进行调度；而是资源调度，由于集群中的资源实际上指的是Worker节点上的计算和内存资源，因此Master节点实际上是对Worker节点进行调度，注册和管理集群中所有的Worker节点。 Worker运行Worker守护线程的集群节点，是集群资源的贡献节点。一个Worker上有多个Executor。一个集群一般有多个Worker，统一受Master管理。每一个Worker节点再启动之后都会向master节点注册自己，向master汇报自己的核心总数和内存总量，并按照master节点的指示启动内部的Executor。并且worker节点会向Master发送心跳，让Master知道Worker是否宕机或者发生延迟。 任务执行器(Executor)实际执行任务的单元，拥有计算和内存资源。每一个Executor维持一个线程池，线程池中的每一个线程用于执行一个任务。每个Executor内部执行的任务属于同一个应用程序。 作业(Job)：前面也讲过，RDD计算具有惰性，只有遇到动作操作或者需要把RDD持久化的时候，计算才真正开始。Spark中将动作操作或者持久化操作作为Job的标识。一个Spark应用程序包含多个作业，由于这些作业之间相互独立，因此不需要对其进行调度，Spark的作业调度仅考虑一个作业内部的调度过程。 阶段(Stage): Spark讲依赖分为窄依赖和shuffle依赖，而shuffle依赖意味着分区之间的数据需要shuffle，因此Spark讲shuffle作为划分stage的标识。 任务集(Taskset):每一个阶段可能包含多个可并发执行的任务，这里把同一个阶段内部的所有任务汇总在一起，成为一个任务集。任务集是DAG调度器交付给任务调度器的基本单位。 通信框架Spark内部模块之间的通信采用了基于 Actor模型的消息传递机制，实际上使用了Actor模型的Scala语言实现的Akka库。Actor模型有Actor和Ref两类对象，分别有函数actorOf和actorFor创建。 通俗地讲，Actor对象相当与Web服务中的服务器端，用于接收来自客户端的请求，并根据请求消息的类型执行不同操作，返回结果。Ref对象可以类比为web服务中的客户端，用于向服务器端发送请求并接收处理结果。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark partitioner源码分析(转载)]]></title>
    <url>%2F2019%2F04%2F09%2F201904091552-spark-partitioner%2F</url>
    <content type="text"><![CDATA[偶然看到一篇关于spark Partitioner的源码分析的好文，转载过来看一下。不知道原作者是否同意文章转载，这里我们就不直接把文章贴出来了，访问链接即可阅读原文。 本来是想自己写一篇关于spark partitioner的文章，但是还是一直没有很理解其中的原理，看源码也是一知半解。这篇文章分别讲了spark1.X 和spark2.x范围分区器的实现原理。所以可以看看别的大佬的学习成果，节省一些时间。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>范围分区器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习资源链接]]></title>
    <url>%2F2019%2F04%2F09%2F201904090108-spark-resource-links%2F</url>
    <content type="text"><![CDATA[这列列举一些spark的学习资源和好的博客。 http://spark.apache.org 这个就不用讲啦，Spark的官方地址，提供了最新的源码以及文档 子雨大数据之Spark入门教程（Scala版） 基础教程，值得我仔细去看，还有实际操作 https://www.cnblogs.com/qingyunzong/category/1202252.html 老铁大佬，总结了很多关于大数据的博客 http://sharkdtu.com/categories/spark/ 大鲨鱼老哥对于Spark的原理分析的很透彻。 持续更新！！]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>学习资源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法汇总]]></title>
    <url>%2F2019%2F04%2F05%2F201904052000-algorithm-overview%2F</url>
    <content type="text"><![CDATA[在网上看到一个大佬整理的数据机构和算法系列，而且归纳的很好，附上链接，仅供参考。 http://www.cnblogs.com/skywang12345/p/3603935.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark scheduler 内部原理剖析(转载)]]></title>
    <url>%2F2019%2F04%2F02%2F201904021108-spark-scheduler%2F</url>
    <content type="text"><![CDATA[大鲨鱼的博客《Spark Scheduler内部原理剖析》中，对spark scheduler的内部原理的分析还是很到位的。现在把这篇文章转载，详细请点击链接。http://sharkdtu.com/posts/spark-scheduler.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——Spark(3)Spark RDD内部结构]]></title>
    <url>%2F2019%2F03%2F28%2F201903281452-spark-learning-3%2F</url>
    <content type="text"><![CDATA[Spark是一个基于分布式内存的大数据计算框架，RDD (Resilient Distributed Dataset)是Spark最重要的一个数据抽象。这篇文章记录了我对RDD的一些理解，有不足和错误的地方，请留言指正。 什么是RDDRDD (Resilient Distributed Dataset)，弹性分布式数据集，是数据集合的抽象。它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。 RDD的计算具有惰性，只有在RDD需要将计算结果提交到Driver或者需要把数据写到文件的时候，计算才真正开始。Spark为RDD的计算提供了两类接口，也就是转换(transformation)操作和动作（action）操作。其中转换操作，是将RDD转换成另一个RDD，这一步计算是具有惰性延迟的，不会立即执行计算。RDD不能改变，在转换的过程中只会生成一个新的RDD。动作操作，会激发计算。 RDD的转换操作和动作操作会在后续的博客中详细介绍。 RDD的接口在介绍RDD的接口之前，先带着几个思考： RDD的结构是什么样子的，怎么表示并行计算单元。 RDD是分布式数据集合的抽象，但是在分布式条件下，RDD怎么保证数据分布均匀？ RDD的计算具有惰性，那么它怎么保证计算的准确性，计算流程是什么样的？ 大数据计算，怎么容错？当Spark数据丢失之后，怎么保证数据的可靠性和完整性。 RDD通过实现一些接口来完成上述的一些问题。 分区(Partition)RDD内部如何去表示并行计算的一个单元呢？其实是采用分区，也就是RDD的内部数据集合在逻辑上被划分为多个分片，这样每一个分片叫做分区（Partition）。分区的个数将决定并行计算的粒度，而每一个分区的数据的计算都是在一个单独的任务中进行，因此任务的个数是由RDD的分区的个数决定的（准确来说应该是一个作业的最后一个RDD的分区数决定）。在下图中，每一个RDD里面的单元就是一个分区。 1234trait Partition extends Serializable&#123; def index:Int override def hashCode():Int=index&#125; 上面的代码可以看出，分区在源码级别的实现是Partition类，其实是分区的一个标识，index表示这个分区在RDD中的编号。我们通过RDD的编号和Partition编号就可定位到具体的分区，通过接口，可以层存储介质中提取分区对应的数据。 RDD的分区的原则就是尽可能让分区的个数等于集群核心的数目。当然用户也可以自指定分区的数据，也可以使用系统默认配置，也就是机器的CPU的核心总数。RDD可以通过创建操作和转换操作生成，在转换操作中，分区的个数根据转换操作中对应的多个RDD之间的以来关系得到。窄依赖的子RDD由父RDD分区个数决定，而宽依赖或shuffle依赖由子RDD的分区器（Partitioner）决定。 分区内部数据的分配原则是尽可能让不同分区内的记录数量保持一致，也就是保证数据分布均衡。正如上面所属，窄依赖的子RDD依赖于父RDD，转换之间不会发生shuffle操作，因此在父RDD分区的数据分配方式决定了窄依赖的分配方式。但是在宽依赖中，父RDD到子RDD之间会进行shuffle操作，因此分区数据的分配由子RDD的分区器决定。哈希分区器不能够保证数据被平均分配到各个分区上，但是范围分区器能够做到这一点。 依赖(Dependency)在上面也提及到了宽依赖和窄依赖。在Spark中，一个真正的计算标志着一个Job，而一个Job则是由多个转换操作构成。由于Spark的计算具有惰性，只有当遇到一个动作操作的时候，计算任务才被激活。因此经过不同变换之间的RDD形成了一个具有依赖关系的链，或者说是一个有向无环图（DAG）。在这个有向无环图中，节点表示RDD，子RDD与父RDD分区之间的边则表示数据的依赖关系。 窄依赖(Narrow Dependency)窄依赖如上图所示，父RDD中的一个分区最多只会被子RDD中的一个分区使用，也就是父RDD中一个分区内的数据不能够被再次分割，必须完整地交给子RDD的一个分区。窄依赖又可以分成1. 一对一依赖:一对一依赖很好理解，就是父RDD的分区编号与子RDD的分区编号完全一致，如上图map操作。2. 范围依赖: 范围依赖只被应用到UnionRDD与父RDD之间的依赖关系之中，如上图union操作。### 宽依赖（Shuffle Dependency）shuffle依赖中，父RDD中的一个分区可能会被子RDD中的多个分区使用。这时候父RDD的数据会被再次分割，发送给子RDD的一些分区，也就是Shuffle依赖意味着父RDD和子RDD之间存在这shuffle操作。 如上图，每一个子RDD的分区会接受来自多个父RDD分区的数据。Shuffle依赖也是后面会讲到的Stage的划分，Spark以是否发生shuffle操作，将一个Job划分为多个Stage进行调度。 抽象类Dependency源码链接1234@DeveloperApiabstract class Dependency[T] extends Serializable &#123; def rdd: RDD[T]&#125; 依赖在Spark源码中对应实现是Dependency，每个Dependency子类内部都会存储一个RDD对象，其实就是对应的父RDD。如果一次转换操作对应多个父RDD，就会产生多个Dependency对象，所有的Dependency对象存储在子RDD内部。只要遍历子RDD内部的Dependency对象，就能获取该RDD所有的依赖关系。窄依赖和shuffle依赖的源码参见上面的链接。 依赖与容错机制我们之前也提到了RDD之间会形成依赖关系，这些依赖关系会形成一个有向无环图。子RDD记录着它依赖着的父RDD。现在我们假设某一个分区的在计算的时候数据丢失了，我们只要遍历该分区的依赖关系，找到它的父RDD信息，然后沿着这个依赖关系往回追溯，就可以找到该分区数据的来源，然后再沿着依赖关系往后计算，就可以计算出这个分区丢失的数据是什么了。 计算(Computing)RDD的计算是惰性的，一系列的转化只有遇到动作操作的时候才回去计算数据，而分区是数据计算的基本单位。 RDD的计算会用到一个compute方法，RDD的抽象类要求所有的子类都要实现这个方法，该方法的参数之一是一个Partition对象，目的是计算该分区中的数据。看一下MappedRDD源码 12override def compute(split:Partition, context:TaskContext)= firstParent[T].iterator(split, context).map(f) MappedRDD类的compute方法调用当前RDD内的第一个父RDD的iterator方法，该方法拉取父RDD对应的分区内的数据。iterator方法会返回一个迭代对象，迭代器内的每个元素就是父RDD对应分区内的数据记录。RDD的粗粒度转换体现在调用iterator的map方法上，f函数是map转换操作的函数参数，RDD会对每一个分区（而不是一条一条数据记录）内的数据执行单个f操作，最终返回包含所有经过转换过的数据记录的新迭代器，也就是新的分区。换句话说，compute函数就是负责父RDD分区数据到子RDD分区数据的变换逻辑。 iterator在执行的时候会根据RDD的存储级别，如果存储级别不是None，则说明分区的数据可能存储在了文件系统，也可能是当前RDD执行过cache或者persist等持久化操作，因此执行getOrCompute方法。12345678910111213/** * Internal method to this RDD; will read from cache if applicable, or otherwise compute it. * This should ''not'' be called by users directly, but * is available for implementors of custom * subclasses of RDD. */final def iterator(split: Partition, context: TaskContext): Iterator[T] = &#123; if (storageLevel != StorageLevel.NONE) &#123; getOrCompute(split, context) &#125; else &#123; computeOrReadCheckpoint(split, context) &#125;&#125; getOrCompute方法会根据RDD的编号和分区编号计算得到当前分区在存储层对应的块编号，通过存储层提供的数据读取借口提取出块的数据。对于getOrCompute方法，会出现两种情况。第一种，数据之前存储在存储截介质中，可能是数据本身就在存储介质，也可能是RDD经过持久化操作并且经历过一次计算过程，这时候数据能够成功提取并返回。第二种情况，数据不存储在介质中，可能是丢失，或者RDD经过持久化操作，但是当前分区的数据是第一次被计算，因此会出现拉取到的数据为none的情况，这意味着需要计算分区数据，则会继续调用RDD类中的computeOrReadCheckpoint方法，并将计算得到的数据缓存到存储介质中，下次就不用再重新计算。 computeOrReadCheckpoint方法会检查当前RDD是否已经被标记为检查点，如果没有被标记为检查点，则执行自身的compute方法来计算分区的数据，否则直接拉去RDD分区内的数据。对于标记成检查点的情况，当前RDD的父RDD不再是原先转换操作中提供的父RDD，而是被Spark替换成一个Checkpoint对象，该对象中的数据存放在文件系统中，因此最终该对象会从文件系统中读取数据并返回给computeOrReadCheckpoint方法。 分区器(Partitioner)Spark内置了两类分区器，哈希分区器(Hash Partitioner)和范围分区器(Range Partitioner)分区器的主要作用有以下三点： 决定RDD的分区数量。 决定shuffle过程中reducer的个数（实际上也是子RDD的分区个数）以及map端的一条数据应该分配给哪一个分区。 决定依赖类型，如果父RDD和子RDD都有分区器并且分区器相同，则表示两个RDD之间是窄依赖，否则是Shuffle依赖。 哈希分区器1234567891011121314151617181920212223242526/** * A [[org.apache.spark.Partitioner]] that implements hash-based partitioning using * Java's `Object.hashCode`. * * Java arrays have hashCodes that are based on the arrays' identities rather than their contents, * so attempting to partition an RDD[Array[_]] or RDD[(Array[_], _)] using a HashPartitioner will * produce an unexpected or incorrect result. */class HashPartitioner(partitions: Int) extends Partitioner &#123; require(partitions &gt;= 0, s"Number of partitions ($partitions) cannot be negative.") def numPartitions: Int = partitions def getPartition(key: Any): Int = key match &#123; case null =&gt; 0 case _ =&gt; Utils.nonNegativeMod(key.hashCode, numPartitions) &#125; override def equals(other: Any): Boolean = other match &#123; case h: HashPartitioner =&gt; h.numPartitions == numPartitions case _ =&gt; false &#125; override def hashCode: Int = numPartitions&#125; 哈希分区器的实现在HashPartitioner中，其getPartition方法实现很简单，取键值的hashCoder，除上子RDD分区个数，取余即可。尽管哈希分区器实现很简单，运行速度很快，但是它不关心键值的分布情况，所以散列到不同发哪去的概率会因数据而异，所以会出现数据倾斜的现象，也就是一部分分区分配的数据很多，一部分很少。 范围分区器对于范围分区器，之后会单独写篇博客，这里附上一片博客给大家作为参考 分区器针对哈希分区器的缺点，范围分区器则在一定程度上避免了这个问题，范围分区器尽量使得所有分区的数据均匀，并且分区内的数据的上界是有序的。 持久化(Persistence)持久化方法有两个，分别是cache和persist。Cache等价于StorageLevel.Memory_ONLY的persist方法，而persist方法也仅仅是修改当前RDD的存储级别而已。SparkContext中维护了一张哈希表persistentRdds,用语等级所有被持久化的RDD，执行persist操作是，会将RDD的编号作为键值，把RDD记录到persistentRdds表中，unpersist函数会调用SparkContext对象的unpersistRDD方法，除了将RDD从哈希表中移除之外，该方法还会降RDD中的分区对应的所有块从存储介质中删除。 检查点(CheckPoint)检查点机制的实现与持久化是的实现有着较大的区别。检查点并非第一次计算就将计算结果进行存储，而是等到第一个作业结束之后再启动专门的一个作业去完成存储的过程。详见链接 [检查点] (https://www.cnblogs.com/tongxupeng/p/10439889.html) 引用1. 检查点2. 范围分区]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——Spark(2)Spark 集群搭建]]></title>
    <url>%2F2019%2F03%2F27%2F201903272048-spark-learning-2%2F</url>
    <content type="text"><![CDATA[本文讲介绍如何搭建spark集群。搭建spark集群需要进行一下几件事情： 集群配置ssh无秘登录 java jdk1.8 scala-2.11.12 spark-2.4.0-bin-hadoop2.7 hadoop-2.7.6 上述所有的文件都安装在/home/zhuyb/opt文件夹中。 服务器服务器是实验室的，选用了一台master和三台slave机器.IP和机器名在hosts文件中做了映射，因此可以通过hostname直接访问机器。 ip addr hostname 219.216.64.144 master 219.216.64.200 hadoop0 219.216.65.202 hadoop1 219.216.65.243 hadoop2 配置shh免密登录详情参考集群环境ssh免密码登录设置shh免密登录配置其实很容易，我们现在有四台机器，首先我们master上生成新的公钥和私钥文件。12ssh-keygen -t rsa #.ssh文件夹将出现id_rsa，id_rsa.pubcat id_rsa.pub &gt;&gt; authorized_keys #将公钥拷贝到authorized_keys文件中 然后按照同样的方式在三台slave机器上生成公钥和私钥，并将各自的authorized_keys文件拷贝到master上，并将文件中的公钥追加到master的authorized_keys文件中。1ssh-copy-id -i master #将公钥拷贝到master的authorized_keys中 最后讲master的authorized_keys文件拷贝到三台slave机器上。至此，四台机器即可实现ssh免密登录。 安装JDK和ScalaJDK版本是1.8, Scala版本是2.11。Scala 2.12与spark 2.4版本有些不兼容，在后续编程的时候会出现一些问题，之后应该会解决。讲jdk和scala文件解压之后，可在~/.bashrc 文件中配置环境变量。12345678export JAVA_HOME=/home/zhuyb/opt/jdk1.8.0_201export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib:$CLASSPATHexport JAVA_PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JRE_HOME&#125;/binexport PATH=$PATH:$&#123;JAVA_PATH&#125;export SCALA_HOME=/home/zhuyb/opt/scala-2.11.12export PATH=$&#123;SCALA_HOME&#125;/bin:$PATH 按照上述方式配置好之后，然后执行命令 source ~/.bashrc。所有的操作需要在每台机器上进行配置。 配置Hadoop 讲Hadoop文件解压到~/opt/文件夹下。 12tar -zxvf hadoop-2.7.3.tar.gzmv hadoop-2.7.3 ~/opt 在~/.bashrc文件中配置环境变量，并执行source ~/.bashrc生效 1234export HADOOP_HOME=/home/zhuyb/opt/hadoopp-2.7.6export PATH=.:$HADOOP_HOME/bin:$HADOOP_HOME/sbin/:$PATHexport CLASSPATH=.:$HADOOP_HOME/lib:$CLASSPATHexport HADOOP_PREFIX=/home/zhuyb/opt/hadoop-2.7.6 修改相应的配置文件 a. 修改$HADOOP_HOME/etc/hadoop/slaves，将原来的localhost删除，改成如下内容： 123hadoop0hadoop1hadoop2 b. 修改$HADOOP_HOME/etc/hadoop/core-site.xml 1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/zhuyb/opt/hadoop-2.7.6/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; c. 修改$HADOOP_HOME/etc/hadoop/hdfs-site.xml 1234567891011121314151617181920212223242526 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.address&lt;/name&gt; &lt;value&gt;0.0.0.0:50010&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- 备份数：默认为3--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/zhuyb/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;!-- datanode--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/zhuyb/tmp/dfs/data&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; d. 复制template，生成xml, cp mapred-site.xml.template mapred-site.xml,修改$HADOOP_HOME/etc/hadoop/mapred-site.xml 12345678910111213141516 &lt;configuration&gt;&lt;!-- mapreduce任务执行框架为yarn--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;!-- mapreduce任务记录访问地址--&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; e. 修改$HADOOP_HOME/etc/hadoop/yarn-site.xml 123456789101112 &lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; f. 修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh，修改JAVA_HOME 1export JAVA_HOME=~/opt/jdk1.8.0_121 将master Hadoop文件夹拷贝到Hadoop0，hadoop1，hadoop2三台机器上1scp -r ~/opt/hadoop-2.7.3 zhuyb@hadoop0:~/opt 配置Spark 将spark文件解压到~/opt下，然后在~/.bashrc配置环境变量，并执行source ~/.bashrc 12export SPARK_HOME=/home/zhuyb/opt/spark-2.4.0-bin-hadoop2.7export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME 复制spark-env.sh.template成spark-env.sh,cp spark-env.sh.template spark-env.sh修改$SPARK_HOME/conf/spark-env.sh，添加如下内容: 123export JAVA_HOME=/home/zhuyb/opt/jdk1.8.0_201export SPARK_MASTER_IP=masterexport SPARK_MASTER_PORT=7077 复制slaves.template成slaves,cp slaves.template slaves,修改$SPARK_HOME/conf/slaves，添加如下内容: 123hadoop0hadoop1hadoop2 修改hadoop0, hadoop1, hadoop2.修改hadoop1, hadoop2,hadoop0, $SPARK_HOME/conf/spark-env.sh，将export SPARK_LOCAL_IP改成hadoop1, hadoop2,hadoop0的IP 引用Hadoop2.7.3+Spark2.1.0完全分布式集群搭建过程]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>scala</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习之路——spark(1)Spark简介]]></title>
    <url>%2F2019%2F03%2F27%2F201903271913-spark-learning-1%2F</url>
    <content type="text"><![CDATA[之前看了一些Spark的相关内容，我觉得很有必要进行总结一下，不然都搭不起自己的知识框架。 Apache SparkApache Spark™是用于大规模数据处理的统一分析引擎，是一个实现快速通用的集群计算平台。它是由加州大学伯克利分校AMP实验室 开发的通用内存并行计算框架，用来构建大型的、低延迟的数据分析应用程序。它扩展了广泛使用的MapReduce计算模型。高效的支撑更多计算模式，包括交互式查询和流处理。spark的一个主要特点是能够在内存中进行计算，及时依赖磁盘进行复杂的运算，Spark依然比MapReduce更加高效。官网地址:http://spark.apache.org/ Spark的主要特点 提供Cache机制来支撑需要反复迭代计算或者多次数据共享，减少数据的I/O开销； 提供支持DAG图的分布式并行计算变成框架，减少多次计算之间中间结果写到HDFS的开销； 使用多线程池模型来减少Task启动开销，Shuffle操作中避免不必要的sort操作，并减少磁盘操作。Spark与Hadoop的比较 Hadoop的局限 Spark的改进 抽象层次低， 代码编写难易上手 通过使用RDD的统一抽象，实现数据处理逻辑的代码非常简洁。 只提供map和reduce两个操作，表达能力有限 通过RDD提供了很多transformation和action操作，实现了很多基本的操作，比如Sort，Join等。 一个job只有Map和Reduce两个阶段，复杂的程序需要大量的Job来完成，比起给Job之间的以来关系需要应用户自行管理 一个Job可以包含多个RDD转换操作，只需要在调度时生成多个Stage。一个Stage中可以包含多个Map操作，只需要Map操作使用的RDD分区保持不变。 处理逻辑隐藏在代码细节中，缺少整体逻辑视图 RDD的转换支持流式API，提供处理逻辑的整体视图 对迭代式的数据处理性能比较差，Reduce与下一步Map之间的中间结果只能保存在HDFS文件系统中 通过内存缓存数据，可大大提高迭代式计算的性能，内存不足是可以溢出到磁盘上。 ReduceTask需等待所有MapTask都完成后才开始执行 分区相同的转换操作可以在一个Task中以流水线的形式执行，只有分区不同的转换需要Shuffle操作。 时延高，只适用于批数据处理，对交互式数据处理和实时数据处理的支持不够 将流拆成小的Batch，提供Discretized Stream处理流数据 Spark主要组件 名称 功能 SparkCore 将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。 SparkSQL Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容。 SparkStreaming 是Spark提供的实时数据进行流式计算的组件。 MLlib 提供常用机器学习算法的实现库。 GraphX 提供一个分布式图计算框架，能高效进行图计算。 BlinkDB 用于在海量数据上进行交互式SQL的近似查询引擎。 Tachyon 以内存为中心高容错的的分布式文件系统。 Spark Cluster Managerscluster manager值集群上获取资源的外部服务器 Standalone: Spark原生的资源管理器，有manager负责资源的分配 Apache Messo: 与Hadoop MR兼容性良好的资源调度框架 Hadoop Yarn： 指yarn中的resourceManager]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯云作为图床]]></title>
    <url>%2F2019%2F03%2F27%2F201903271019-tencent-picdb%2F</url>
    <content type="text"><![CDATA[之前都是使用新浪微博图床，但是最近这个图床一直在维护，其他的图床使用都比较麻烦，因此在这里我打算使用腾讯云的对象存储作为图床。 怎么在腾讯云上创建对象存储，其实很简单。完全可以按照链接进行配置。 在这里我们测试一下防盗链设置。设置防盗链之后，即使别人能够获得图片的链接，但是也不能访问图片。]]></content>
      <categories>
        <category>blogs</category>
      </categories>
      <tags>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流浪——Hive(1)Hive的基本组件及简单的执行流程]]></title>
    <url>%2F2019%2F03%2F26%2F201903262337-hiv-introduction-1%2F</url>
    <content type="text"><![CDATA[本文简单介绍hive的基本组件和简单的执行流程，是笔记本上的照片。更为详细的内容参考文后的引用。 Hive组件 Hive执行流程 引用1 Hive学习之路(一)Hive初识2 Hive技术原理3 Hive学习系列(一)什么是Hive及Hive的架构]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git提交文件]]></title>
    <url>%2F2019%2F03%2F26%2F201903261608-git-upload-files%2F</url>
    <content type="text"><![CDATA[使用git命令上传文件到github中 git init用 git init 在目录中创建新的 Git 仓库。 你可以在任何时候、任何目录中这么做，完全是本地化的。 1git init git add12git add . #(.表示添加所有文件到缓存中)git add filename #(添加某一文件) git commit使用 git add 命令将想要快照的内容写入缓存区， 而执行 git commit 将缓存区内容添加到仓库中。使用 -m 选项以在命令行中提供提交注释。1git commit -m &apos;上传文件&apos; git remote add origin将本地仓库关联到GitHub上的仓库里去 1git remote add origin https://github.com/icesuns/icesuns.git.io.git git pull首次提交要git pull 一下12git pull origin mastergit pull --rebase origin master git push将文件提交到GitHub上1git push -u origin master]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark核心概念RDD(转载)]]></title>
    <url>%2F2019%2F03%2F23%2F201903272000-Spark%E2%80%94%E2%80%94RDD%2F</url>
    <content type="text"><![CDATA[本文是转载守护之鲨关于Spark RDD的文章。 RDD全称叫做弹性分布式数据集(Resilient Distributed Datasets)，它是一种分布式的内存抽象，表示一个只读的记录分区的集合，它只能通过其他RDD转换而创建，为此，RDD支持丰富的转换操作(如map, join, filter, groupBy等)，通过这种转换操作，新的RDD则包含了如何从其他RDDs衍生所必需的信息，所以说RDDs之间是有依赖关系的。基于RDDs之间的依赖，RDDs会形成一个有向无环图DAG，该DAG描述了整个流式计算的流程，实际执行的时候，RDD是通过血缘关系(Lineage)一气呵成的，即使出现数据分区丢失，也可以通过血缘关系重建分区，总结起来，基于RDD的流式计算任务可描述为：从稳定的物理存储(如分布式文件系统)中加载记录，记录被传入由一组确定性操作构成的DAG，然后写回稳定存储。另外RDD还可以将数据集缓存到内存中，使得在多个操作之间可以重用数据集，基于这个特点可以很方便地构建迭代型应用(图计算、机器学习等)或者交互式数据分析应用。可以说Spark最初也就是实现RDD的一个分布式系统，后面通过不断发展壮大成为现在较为完善的大数据生态系统，简单来讲，Spark-RDD的关系类似于Hadoop-MapReduce关系。 RDD特点RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。 分区如下图所示，RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。 只读如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。 RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。 依赖RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。 通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。 缓存如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。 checkpoint虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。 小结总结起来，给定一个RDD我们至少可以知道如下几点信息： 1、分区数以及分区方式； 2、由父RDDs衍生而来的相关依赖信息； 3、计算每个分区的数据，计算步骤为： 1）如果被缓存，则从缓存中取的分区的数据； 2）如果被checkpoint，则从checkpoint处恢复数据； 3）根据血缘关系计算分区的数据。 编程模型在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用actions触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。 要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，如下图所示。Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。 应用举例下面介绍一个简单的spark应用程序实例WordCount，统计一个数据集中每个单词出现的次数，首先将从hdfs中加载数据得到原始RDD-0，其中每条记录为数据中的一行句子，经过一个flatMap操作，将一行句子切分为多个独立的词，得到RDD-1，再通过map操作将每个词映射为key-value形式，其中key为词本身，value为初始计数值1，得到RDD-2，将RDD-2中的所有记录归并，统计每个词的计数，得到RDD-3，最后将其保存到hdfs。123456789101112131415161718import org.apache.spark._import SparkContext._object WordCount &#123; def main(args: Array[String]) &#123; if (args.length &lt; 2) &#123; System.err.println("Usage: WordCount &lt;inputfile&gt; &lt;outputfile&gt;"); System.exit(1); &#125; val conf = new SparkConf().setAppName("WordCount") val sc = new SparkContext(conf) val result = sc.textFile(args(0)) .flatMap(line =&gt; line.split(" ")) .map(word =&gt; (word, 1)) .reduceByKey(_ + _) result.saveAsTextFile(args(1)) &#125;&#125; 小结基于RDD实现的Spark相比于传统的Hadoop MapReduce有什么优势呢？总结起来应该至少有三点： 1）RDD提供了丰富的操作算子，不再是只有map和reduce两个操作了，对于描述应用程序来说更加方便； 2）通过RDDs之间的转换构建DAG，中间结果不用落地； 3）RDD支持缓存，可以在内存中快速完成计算。 引用1.守护之鲨——Spark核心概念RDD]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Processing vs Stream Processing 批处理与流处理]]></title>
    <url>%2F2019%2F03%2F22%2F201903272000-Batch-Processing-vs-Stream-Processing%2F</url>
    <content type="text"><![CDATA[最近在学习关于大数据处理方面的东西，因此想总结一下批处理和流处理之间的异同之处。 数字社会中数据到处都在产生，数据规模也在不断增长。处理较小的数据集可能跟简单，但是处理TB，PB级别的数据量将会是很困难的事情。大数据处理框架，比如MapReduce、Hadoop以及Spark等分布式处理框架出现，一定程度上解决了大数据处理的问题。 我们将讨论大数据出路中的一些的方法，批处理（Batch Processing）和流处理（Streaming Processing）。在实际的业务中，可根据自己的实际情况决定采用哪种处理模式。 批处理Batch processing is where the processing happens of blocks of data that have already been stored over a period of time. 批处理强调处理一个时间窗口的批数据，缺乏实时性。如下图，计算一天为单位产生的输入数据，当天产生的数据将会在第二天进行计算。也就是说我们对数据，有一个预先收集存储的过程。 在批处理方式中，数据首先被存储，随后被分析。MapReduce是非常重要的批处理模型。MapReduce的核心思想是，数据首先被分为若干小数据块chunks，随后这些数据块被并行处理并以分布的方式产生中间结果，最后这些中间结果被合并产生最终结果。MapReduce分配与数据存储位置距离较近的计算资源，以避免数据传输的通信开销。 流处理Stream Processing is the act of continuously incorporating new data to compute a result， and is a golden key if you want ananlytics results in real time。 而在流处理中，由于流数据的无边界性，无法确定数据何时产生、何时到达。而流处理强调的是计算的实时性。如下图，当天的数据计算当天需要被及时计算，数据一旦到达就要计算出结果。 流式处理假设数据的潜在价值是数据的新鲜度(freshness)，因此流式处理方式应尽可能快地处理数据并得到结果。在这种方式下，数据以流的方式到达。在数据连续到达的过程中，由于流携带了大量数据，只有小部分的流数据被保存在有限的内存中。流处理理论和技术已研究多年，代表性的开源系统包括Storm，S4和Kafka。流处理方式用于在线应用，通常工作在秒或毫秒级别。 1.大数据技术学习：大数据的两种处理方式的“流”和“批”2.Big Data Battle : Batch Processing vs Stream Processing]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>流处理</tag>
        <tag>批处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A survey of the Usages of Deep Learning in NLP]]></title>
    <url>%2F2019%2F01%2F13%2F201903272000-a-survey-of-the-usages-of-deep-learning-in-NLP%2F</url>
    <content type="text"><![CDATA[这是一篇关于深度学习在自然语言处理方向的综述. 点击下载原文 简介NLPNLP是一个交叉学科,涉及计算语言学,认知科学,统计学等等学科.自然语言处理又包含多方面的任务,包括认知,理解, 生成.自然语言认知和理解是让计算机把输入的语言变成meaningful的符号和关系,然后在根据任务目的进行在处理.自然语言生成是计算机数据转化成自然语言. NLP的工作可以分为两个子任务: core areas: Language modeling: morphological processing dealing with segmentation of meaningful component of words parts of speech of words syntatic processing or parsing semantic parsing applications: information extraction translation summarization question answering classification or clustering 接下来对以上每一个小点进行介绍. Neural Networks由于算力巨大提升和大量可用的NLP数据集的公开,神经网络(neural networks)在NLP的使用今年来发展迅速,比如机器翻的质量也愈来愈好.现在NLP任务中采用的神经网络有一下几种: Feedward Networks Convolutional Neural Networks (CNN) Recursive Neural Networks Recurrent Neural Networks (RNN) Long Short-Term Memory Networks (LSTM) Gated Recurrent Unit (GRU) 除此之外,还有一些其他的技术: Attention Mechanisms 注意力机制 Residual Connections 残差连接 Dropout Encoder-Decoder frameworks 对于这些神经网络的具体细节,这里不做介绍.想了解的,可以查找相关的技术资料,然后对这些知识点进行学习. Core AreasLanguage Modeling 语言模型,即是判断一句话是否为正常人的话.语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率$P(w_1, w_2, …, w_t)$ .$w_1$ 到$w_t$依次表示这句话中的各个词。有个很简单的推论是：$P(w_1, w_2, …, w_t) = P(w_1) \times P(w_2 | w_1) \times P(w_3 | w_1, w_2) \times … \times P(w_t | w_1, w_2, …, w_{t-1})$常用的语言模型都是在近似地求$P(w_t | w_1, w_2, …, w_{t-1})$。比如 n-gram 模型就是用$P(w_t | w_{t-n+1}, …, w_{t-1})$近似表示前者。 神经网络在处理语言模型的时候，有几点问题需要考虑分别是： 同义词（synonyms) 未登录词 （OOV) 词向量 (word embedding) 是语言模型的一个产物，能够获取词语之间的语义关系。在词语的表示有离散表示，如： one-hot形式，这种表示不能表达词语之间的联系，而且稀疏，当词语数量很大的时候，one-hot就更加稀疏了。word embedding是一种稠密的连续向量表示，在连续的向量空间里，含义相似的词语位置也比较近。甚至词语之间的组合关系，也能够通过向量的表示，比如：$W_(queen) = W_(king) + W_(woman)$ 深度学习在语言模型的应用，主要有以下几点： Attention Mechanism的应用 Residual Memory Networks Convolutional Neural Networks. 卷积神经网络应用到语言模型中，一般将pooling替换成full connection.一些特征会在pooling中丢失，而全连接能够在一定程度上保留这些特征。 Character Aware Neural Language Models. 之前的语言模型是以word为基本单元，这种方法是以character为基本的单元进行处理。 MorphologyMorphology 指的是词法，形态学。主要的研究内容是对词根(roots)、词干(stems)，前缀(preffixes)、后缀(suffixes)、中缀(infixes)。在NPL任务中，来自同一个词根的词语，会表示不同的含义，但是彼此语义仍然有联系。 ParsingParsing 研究的是不同的Word或者phrase如何在一个句子中组织起来，也就是研究语法。Parsing至少可以分成两中模式： Constituency Parsing, 以分层的方式从句子中提取除phrases，在确定短语的语法特征之后，反过来phrases又会组合成一个完整的句子。(In constituency parsing, phrasal constituents are extracted form a sentence in a hierarchical fashion. Phrasal are identified, which in turn form larger phrases, eventually culminating in conplete sentences.) Dependency Parsing, 仅关注pairs of individual words之间的关系。（Dependency parsing on the other hand looks solely at the relationships between pairs of individual words） 深度学习在dependency parsing中应用蛮广泛的，特别是基于Graph-based parsing，将句子解析成parsing trees，然后找出正确的parsing tree。 graph-based parsing用formal grammar去解析句子。最近的grahp-based方法用到了trainsition-based方法，这类方法只会生成一颗parsing tree，句子中word之间的联系用弧表示，弧的方向表示words之间的依赖关系。 SemanticsSemantics的关注点是如何理解words、phrases、sentences甚至是documents的meaning。Distributed Representation是表示Meaning的一种方式。Word Embeddings利用能够获得词语之间的语义联系。semantics的任务有： Semantic Comparison。两个实体的语义是否一直，其衡量指标是将模型和人主观判断的结果进行比较。 Sentence Modeling。用向量表示句子的语义。 ApplicationsInformation Extraction从文本中获取显式或者隐式的信息。 Named Entity Recognition命名实体识别，是指识别出文本中的专有名词，比如日期，时间，价格，产品IDs。一般常用的方法有支持向量机、条件随机场、LSTM网络等。 Event Extractionevent extraction涉及用来知道事件发生的words或者phrases，这些词表示事件的参与者(participant),比如agent、事件的对象(object)、事件的接受者(recipient)以及事件发生的时间等。也就是说挖掘文本中关于事件的相关信息，比如时间，地点，人物，事件等关于event的描述。 EventExtraction有四个子任务： identifying event mentions or phrases that describe events. 识别除描述事件的描述或者短语 identitying event triggers, which are the main words, usually verbs or gerunds，sometimes infinitives,that specify the occurence of the events.识别除事件的触发词,表示时间的发生，一般是动词或者动名词，有时候是动词不定时。 identitying arguments of events. identifying arguments’ roles in events Relationship ExtractionText ClassificationSummarizationQuestion AnsweringMachine TranslationImage and Vidoe Captioning 引用 [1] A Survey of the Usages of Deep Learning in Natural Language Processing 原文[2] 语言模型[3] 词向量 (word embedding)[4] Named Entity Recognition[5] Relationship Extraction[6] Text Classification[7] Summarization[8] Question Answering[9] Image and Vidoe Captioning]]></content>
      <categories>
        <category>Papers</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于RNN的英文姓名的性别预测]]></title>
    <url>%2F2019%2F01%2F12%2F201803272000-RNN-based-name-gender-prediction%2F</url>
    <content type="text"><![CDATA[根据人的名字判断任务的性别是一个很有趣的工作. 预测英文姓名的性别,对first name进行预测,有很多种方法,比如朴素贝叶斯法,SVM,神经网络等.预测的时候能够利用的特征也有多种:名字的最后一个字母,两个字母(2-gram),最后一个字母是否是元音或者辅音. 我们讲介绍如何利用RNN预测英文姓名的性别.点击dataset即可下载数据集 数据处理本项目采用的数据集是一个英文数据集.男性和女性的数据比为$ 3:5$,在一般情况下,数据比例较为合适. - female 5001条 - male 2943条 在处理数据的时候,对数据进行规范化处理: 所有字符小写化; 剔除所有的非字母数据; 字母数值化,每个字母对应在字符标的顺序; 模型的特征是姓名的单个字母; 测试数据和训练数据的划分按照$1:9$的比例进行划分. RNN模型模型有三层,分别是RNN层, 全连接层,softmax层.RNN层采用LSTM网络结构. 12345678910111213141516171819202122232425class Classifier(nn.Module): def __init__(self, input_size, hidden_size, embedding_size): super(Classifier, self).__init__() self.input_size = input_size self.hidden_size = hidden_size self.embedding_size = embedding_size self.embeddings = nn.Embedding(self.input_size, self.embedding_size) nn.init.xavier_normal(self.embeddings.weight.data) self.drop = nn.Dropout(p=0.1) self.rnn = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, batch_first=True) self.out = nn.Linear(self.hidden_size, 2) self.log_softmax = nn.LogSoftmax(dim=-1) def forward(self, input, length): input = self.embeddings(input) input = self.drop(input) input_packed = nn.utils.rnn.pack_padded_sequence(input, length, batch_first=True) # out_packed, (ht, ct) = self.rnn(input_packed, None) # out = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True) _, (ht, _) = self.rnn(input_packed, None) out = self.out(ht) out = self.log_softmax(out) return out RNN网络参数batch_first=True时,需要注意输入序列.用RNN（包括LSTM\GRU等）做NLP任务时，对于同一个batch内的短句子一般需要padding补齐，这些padding的字符一般不应该拿去算output、hidden state、loss. 在pytorch中,在处理NLP任务的时候,首先要讲数据的序列进行padding,让一个batch的数据长度一样.然后根据序列原有的长度进行逆序排序.在我们这个任务里,需要处理names序列,以及labels.这时候有两种方案: names序列按顺序排列之后,labels也按照names排列顺序进行调整. names序列按照长短顺序排列之后,进行出列,然后将结果序列调整为原来的顺序.在这里,采用第一种方法.1234567891011length = np.array([len(name) for name in names_list])sort_idx = np.argsort(-length)max_len = max(length)name_tensors = torch.zeros(len(names), max_len).to(torch.long)for i, idx in enumerate(sort_idx): for j, e in enumerate(names_list[idx]): name_tensors[i][j] = enames_lengths = torch.from_numpy(length[sort_idx]).to(torch.long)labels = labels[sort_idx].view(1, -1).squeeze(0) 训练1234567891011121314151617181920212223242526272829303132333435classifier = Classifier(len(chars), 128, 128)optimizer = optim.RMSprop(classifier.parameters(),lr=0.001)loss_func = nn.NLLLoss()total_loss = 0total_step = 0while True: for data in train_loader: total_step += 1 names = data[0] labels =data[1] name_tensors, labels, names_lengths = name_to_tensor(names, labels) out = classifier(name_tensors, names_lengths).view(-1, 2) # print(out) # print(labels) loss = loss_func(out, labels) total_loss += loss.item() loss.backward() optimizer.step() if total_step % 50 == 0: print('%dth step, avg_loss: %0.4f'%(total_step, total_loss/total_step)) with torch.no_grad(): for data in test_loader: names = data[0] labels = data[1] name_tensors, labels, names_lengths = name_to_tensor(names, labels) out = classifier(name_tensors, names_lengths).view(-1, 2) result = torch.argmax(out,dim=-1 ) result = (result == labels).to(torch.float) print(torch.mean(result)) break 完整代码和实验结果代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145# coding=utf-8import randomfrom torch.utils.data import DataLoaderfrom torch.utils.data import Datasetimport torch.nn as nnimport numpy as npimport torchimport torch.optim as optim################################load dataset ###################################################################################def load_data(path, label): names = [] with open(path) as f: lines = f.readlines() for l in lines: names.append((l.strip(&apos;\n&apos;), label)) return namesfemale_names = load_data(&apos;../datasets/names_gender/eng/female.txt&apos;, 0)male_names = load_data(&apos;../datasets/names_gender/eng/male.txt&apos;, 1)names = female_names + male_namesrandom.shuffle(names)# 将数据划分为训练集和测试集train_dataset = names[: int(len(names)*0.9)]test_dataset = names[int(len(names)*0.9):]# padding的字符为0,chars = [0] + [chr(i) for i in range(97,123)]# print(chars)class NameDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): # 这里可以对数据进行处理,比如讲字符数值化 data = self.data[index] name = data[0] label = data[1] return name, label def __len__(self): return len(self.data)train_dataset = NameDataset(train_dataset)test_dataset = NameDataset(test_dataset)train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)test_loader = DataLoader(test_dataset, batch_size=500, shuffle=True)# 序列按照长短顺序逆序排列def name_to_tensor(names, labels): names_list = [] for name in names: char_list = [] for ch in name.lower(): try: char_list.append(chars.index(ch)) except: char_list.append(0) # name_tensor = torch.from_numpy(np.array(char_list)) names_list.append(char_list) length = np.array([len(name) for name in names_list]) sort_idx = np.argsort(-length) max_len = max(length) name_tensors = torch.zeros(len(names), max_len).to(torch.long) for i, idx in enumerate(sort_idx): for j, e in enumerate(names_list[idx]): name_tensors[i][j] = e names_lengths = torch.from_numpy(length[sort_idx]).to(torch.long) labels = labels[sort_idx].view(1, -1).squeeze(0) return name_tensors, labels, names_lengths######################### model #################### #################################################################class Classifier(nn.Module): def __init__(self, input_size, hidden_size, embedding_size): super(Classifier, self).__init__() self.input_size = input_size self.hidden_size = hidden_size self.embedding_size = embedding_size self.embeddings = nn.Embedding(self.input_size, self.embedding_size) nn.init.xavier_normal(self.embeddings.weight.data) self.drop = nn.Dropout(p=0.1) self.rnn = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, batch_first=True) self.out = nn.Linear(self.hidden_size, 2) self.log_softmax = nn.LogSoftmax(dim=-1) def forward(self, input, length): input = self.embeddings(input) input = self.drop(input) input_packed = nn.utils.rnn.pack_padded_sequence(input, length, batch_first=True) # out_packed, (ht, ct) = self.rnn(input_packed, None) # out = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True) _, (ht, _) = self.rnn(input_packed, None) out = self.out(ht) out = self.log_softmax(out) return outclassifier = Classifier(len(chars), 128, 128)optimizer = optim.RMSprop(classifier.parameters(),lr=0.001)loss_func = nn.NLLLoss()total_loss = 0total_step = 0while True: for data in train_loader: total_step += 1 names = data[0] labels =data[1] name_tensors, labels, names_lengths = name_to_tensor(names, labels) out = classifier(name_tensors, names_lengths).view(-1, 2) # print(out) # print(labels) loss = loss_func(out, labels) total_loss += loss.item() loss.backward() optimizer.step() if total_step % 50 == 0: print(&apos;%dth step, avg_loss: %0.4f&apos;%(total_step, total_loss/total_step)) with torch.no_grad(): for data in test_loader: names = data[0] labels = data[1] name_tensors, labels, names_lengths = name_to_tensor(names, labels) out = classifier(name_tensors, names_lengths).view(-1, 2) result = torch.argmax(out,dim=-1 ) result = (result == labels).to(torch.float) print(torch.mean(result)) break 实验结果123456789101112131415161718192021222324252627282930313233343550th step, avg_loss: 0.4905100th step, avg_loss: 0.4662tensor(0.8200)150th step, avg_loss: 0.4535200th step, avg_loss: 0.4467tensor(0.8020)250th step, avg_loss: 0.4396300th step, avg_loss: 0.4320tensor(0.8260)350th step, avg_loss: 0.4270400th step, avg_loss: 0.4217tensor(0.8300)450th step, avg_loss: 0.4178500th step, avg_loss: 0.4117550th step, avg_loss: 0.4073tensor(0.8140)600th step, avg_loss: 0.4027650th step, avg_loss: 0.3994tensor(0.8260)700th step, avg_loss: 0.3971750th step, avg_loss: 0.3934tensor(0.8100)800th step, avg_loss: 0.3908850th step, avg_loss: 0.3884tensor(0.8160)900th step, avg_loss: 0.3861950th step, avg_loss: 0.38321000th step, avg_loss: 0.3822tensor(0.8140)1050th step, avg_loss: 0.38011100th step, avg_loss: 0.3789tensor(0.8060)1150th step, avg_loss: 0.37691200th step, avg_loss: 0.3757tensor(0.8420) 在训练的时候,验证的结果正确率最高能够达到86%.这个模型还比较粗糙,仅仅用名字的单个字母作为特征,模型还不能够完全分析出字母之间的联系.采用2-gram的方式,结果可能要好一点.这个待以后再去实现. [1] pytorch RNN处理变长的序列]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>deep learning</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pytorch实现数据数据读取]]></title>
    <url>%2F2019%2F01%2F11%2F201903272000-pytorch-dataLoder-datasets%2F</url>
    <content type="text"><![CDATA[在学习的过程中,遇到很多的问题.在训练模型的时候,需要对数据进行读取操作.本篇文章,介绍pytorch如何自定义数据dataset和dataloader.在pytorch中,提供了Dataset这个类,负责对数据进行抽象,一次调用只返回第一个同样本.而Dataloader提供了对一个对一个batch的数据操作,还有对数据进行shuffle等功能. DatasetDataset是一个抽象类,表示一个数据集.所有的数据集类都是Dataset的子类.在自定义数据集的时候,需要对函数__getitem()__, __len()__重写. 1234567891011121314151617class Dataset(object): """An abstract class representing a Dataset. All other datasets should subclass it. All subclasses should override ``__len__``, that provides the size of the dataset, and ``__getitem__``, supporting integer indexing in range from 0 to len(self) exclusive. """ #每调用一次,返回一个数据样本, index表示返回样本的索引位置 #在数据处理中，有时会出现某个样本无法读取等问题，比如某张图片损坏。这时在__getitem__函数中将出现异常，此时最好的解决方案即是将出错的样本剔除。然后在数据集中,随机加入一条数据 def __getitem__(self, index): raise NotImplementedError #__len__ 数据集的长度 def __len__(self): raise NotImplementedError #增加数据,对数据集进行拼接 def __add__(self, other): return ConcatDataset([self, other]) DataLoaderDataLoader是一个可迭代的对象,意味着我们可以像使用迭代器一样使用它或者 batch_datas, batch_labels in dataloader:123456789101112DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False)'''dataset：加载的数据集(Dataset对象) batch_size：batch size shuffle:：是否将数据打乱 sampler： 样本抽样，后续会详细介绍 num_workers：使用多进程加载的进程数，0代表不使用多进程 collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可 pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些 drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃''' 例子在本次的例子中,我们利用Dataset和DataLoader处理预测姓名的性别的原始数据.下面是完整的例子.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# coding=utf-8import randomfrom torch.utils.data import DataLoaderfrom torch.utils.data import Datasetimport torch.nn as nn################################load dataset ###################################################################################def load_data(path, label): names = [] with open(path) as f: lines = f.readlines() for l in lines: names.append((l.strip('\n'), label)) return namesfemale_names = load_data('../datasets/names_gender/eng/female.txt', 0)male_names = load_data('../datasets/names_gender/eng/male.txt', 1)names = female_names + male_namesrandom.shuffle(names)# 将数据划分为训练集和测试集train_dataset = names[: int(len(names)*0.9)]test_dataset = names[int(len(names)*0.9)]class NameDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): # 这里可以对数据进行处理,比如讲字符数值化 data = self.data[index] name = data[0] label = data[1] return name, label def __len__(self): return len(self.data)train_dataset = NameDataset(train_dataset)test_dataset = NameDataset(test_dataset)train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True)for data in train_loader: print(data[0]) break 123#结果(&apos;Danelle&apos;, &apos;Ransell&apos;, &apos;Tanya&apos;, &apos;Rutter&apos;, &apos;Herschel&apos;)tensor([0, 1, 0, 1, 1])]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
        <tag>数据读取</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning(2)——MDPs]]></title>
    <url>%2F2018%2F06%2F10%2F201806102000-reinforcement-learning-MDP%2F</url>
    <content type="text"><![CDATA[上一篇文章强化学习——简介简单介绍了一下强化学习的相关概念。这篇博客将引入 马尔科夫决策过程(Markov Decision Processes, MDPs)对强化学习进行建模。这篇文章，将对马尔科夫决策过程以及Q-leaning进行介绍。 马尔科夫过程 定义: 若随机过程 $ \left \lbrace X_n, n \in T \right \rbrace$对于任意非负正整数$n \in T$和任意的状态$i_0, i_1, …, i_n \in I$, 其概率满足$$P \left\lbrace X_{n+1}=i_{n+1} \mid X_0=i_0, X_1=i_1,…,X_n=i_n \right\rbrace =P \left\lbrace X_{n+1}=i_{n+1} \mid X_n=i_n \right\rbrace$$则称$ \left \lbrace X_n, n \in T \right \rbrace$是马尔科夫过程。 马尔科夫过程的一大特点是 无后效性 ,当一个随机过程在给定现在状态和过去状态情况下，其未来状态的条件概率仅依赖于当前状态。在给定现在状态是，随机过程与过去状态是条件独立的。也就是说，系统的下个状态只与当前状态有关，与更早之前的状态无关，一旦知道当前状态之后，过去的状态信息就可以抛弃。 马尔科夫过程还有一个重要的元素，那就是 转移概率矩阵。$P_{ij}(n) =P \left\lbrace X_{n+1}=j \mid X_n=i \right\rbrace$，指的时刻$n$，从状态$i$转移到状态$j$的一步转移概率。当$P_{ij}(n)$y与$n$无关是，则称马尔科夫链$ \left \lbrace X_n, n \in T \right \rbrace$是齐次的。 $P=\begin {bmatrix}{p_{11}}&amp;{p_{12}}&amp;{\cdots}&amp;{p_{1n}}\\{p_{21}}&amp;{p_{22}}&amp;{\cdots}&amp;{p_{2n}}\\{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\{p_{n1}}&amp;{p_{n2}}&amp;{\cdots}&amp;{p_{nn}}\end {bmatrix}$；其中，${p_{ij}\geq 0}$，$\sum_{j \in I} p_{ij} = 1$因此，定义一个马尔科夫过程，需要给出有限状态集合$\{ x_i \in X, i \in N \}$以及转移概率矩阵$P$。下图是一个马尔科夫链转移矩阵的例子。 马尔科夫决策过程上一篇文章介绍了强化学习是一个与不断进行环境交互学习只能agent的学习过程。而MDPs正是立足于agent与环境的直接交互，只考虑离散时间，假设agent与环境的交互可分解为一系列阶段，每个阶段由“感知——决策——行动”构成。如下图：MDP模型是一个四元组$(S,A,T,R)$ [1] Hexo编辑数学公式： http://bennychen.me/mathjax.html]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
        <tag>马尔科夫决策过程</tag>
        <tag>Q-learning</tag>
        <tag>值迭代</tag>
        <tag>策略迭代</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning(1)]]></title>
    <url>%2F2018%2F06%2F03%2F201803272000-reinforcement-learning-1%2F</url>
    <content type="text"><![CDATA[开始写强化学习这个系列的文章，主要是为了记录学习过程中学习到的知识点，分享自己的理解。 强化学习简介强化学习（英语：Reinforcement learning，简称RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。 强化学习涉及很多学科，其中包括计算机、神经科学、工程学科等，详细可见下图。机器学习大致可以分为强化学习、监督学习以及非监督学习这三种，这三者之间的关系如下图所示。强化学习属于机器学习中的一种，与我们熟悉的深度学习有一些区别。强化学习最大的特点就是能够与环境进行交互，通过交互学习到最优的方法解决问题。强化学习和标准的监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。 最直观的方式去区分这三种学习方法的特征就是数据的形式： 监督学习：数据带有明确的标签，这些标签可以看做是一个supervisor，指导学习数据之间的关系。常见应用有的分类、回归等 非监督学习：数据没有标签，算法学习数据特征之间的内部关系。常见有聚类。 强化学习： 数据没有标签，在强化学习系统中，agent感知环境变化，接受每一次行为产生的奖励(reward)，决策下一步的行为。 强化学习的特点： 正如上面所说的，强化学习没有supervisor，只有一个每次行为之后产生的奖励reward 环境对行为的反馈存在延迟，不是即时的 时间的影响很大，系统是一个连续的变化过程 agent的行为会影响下一次接受的数据。 强化学习系统上图为强化学习(RL)系统的原理图。用马尔科夫决策过程(Markov Decision Process, MDP)对RL问题进行建模。通常将MDP定义为一个四元组(S,A,ρ,f) 强化学习四个元素 策略（Policy）policy是agent的一系列行为，也是状态到行为的映射。Policy可以分为两类，一个是确定性策略（deterministic policy）a = π(s)即状态与策略之间的关系是确定的，当系统处于某种状态，必然有一个策略与之对应。另一列是随机策略（Stochastic policy）π(a∣s) = Ρ[At =a ∣St = s],这是在一个状态下，对应于不同策略的概率 值函数（value function）值函数是对未来奖励预测的函数，定义的是在状态s下，采取策略π的长期奖励 奖励信号（a reward signal）Reward就是一个标量值，是每个time step中环境根据agent的行为返回给agent的信号，reward定义了在该情景下执行该行为的好坏，agent可以根据reward来调整自己的policy。常用R来表示。 环境模型（a model of the environment），预测environment下一步会做出什么样的改变，从而预测agent接收到的状态或者reward是什么。 agent与environment上图是agent与environment之间的交互。现在来介绍一个关于agent和environment之间的一些概念。 历史（History） 与 状态（State） history是observations、actions以及rewards的序列。下一步要发生的事件要依赖于history。 state是决策下一步行动的信息，通常是一个history的函数。 environment state、 agent state、 information state enviroment state是环境的私有状态，通常对agent是不可见的 agent state是agent的内部表现状态，比如agent决策下一步行动所选用的信息等。是历史的一个函数。 information state包含来自于历史的所有有用信息。 fully observable environments 和 partially obervable environments full observability, agent能够直接观察到环境的变化状态，是一个马尔科夫决策过程 partial observability, agent只能简洁观察环境，这是一个部分马尔科夫决策过程（partially observable Markov decision process，POMDP）。代理必须根据其他的信息，建模自己的状态模型。 RL问题中agent的分类]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F03%2F201803272000-hello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>blogs</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客上线啦！]]></title>
    <url>%2F2018%2F04%2F03%2F201803272000-publish-first-blog%2F</url>
    <content type="text"><![CDATA[搞了一下午和一晚上，博客终于弄好了，现在来测试一下。献上一首很喜欢的诗《致橡树》 致橡树我如果爱你绝不像攀援的凌霄花，借你的高枝炫耀自己；我如果爱你绝不学痴情的鸟儿，为绿荫重复单调的歌曲；也不止像泉源，常年送来清凉的慰藉；也不止像险峰，增加你的高度，衬托你的威仪。甚至日光。甚至春雨。不，这些都还不够！我必须是你近旁的一株木棉，做为树的形象和你站在一起。根，紧握在地下，叶，相触在云里。每一阵风过，我们都互相致意，但没有人，听懂我们的言语。你有你的铜枝铁干，像刀，像剑，也像戟，我有我的红硕花朵，像沉重的叹息，又像英勇的火炬，我们分担寒潮、风雷、霹雳；我们共享雾霭、流岚、虹霓，仿佛永远分离，却又终身相依，这才是伟大的爱情，坚贞就在这里：爱不仅爱你伟岸的身躯，也爱你坚持的位置，脚下的土地。 ——舒婷《致橡树》]]></content>
      <categories>
        <category>blogs</category>
      </categories>
      <tags>
        <tag>诗</tag>
      </tags>
  </entry>
</search>
